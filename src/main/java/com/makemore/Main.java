package com.makemore;

import com.makemore.backprop.ManualBackprop;
import com.makemore.backprop.ManualBackprop.*;
import com.makemore.mlp.Tensor;

import java.io.*;
import java.nio.file.*;
import java.util.*;

/**
 * Building makemore Part 4: Becoming a Backprop Ninja
 *
 * Train a 2-layer MLP with BatchNorm using MANUAL backpropagation.
 * We do NOT use loss.backward() - instead we calculate all gradients manually!
 *
 * This demonstrates:
 * 1. How gradients flow through the network
 * 2. Optimized backward passes (faster than autograd!)
 * 3. Deep understanding of backpropagation
 */
public class Main {

    public static void main(String[] args) {
        try {
            System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
            System.out.println("â•‘  Makemore Part 4: Becoming a Backprop Ninja  ğŸ¥·     â•‘");
            System.out.println("â•‘  Manual Backpropagation WITHOUT autograd             â•‘");
            System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

            // Load data
            DataLoader dataLoader = new DataLoader();
            dataLoader.loadData("names.txt");
            dataLoader.buildDataset(3);

            int vocabSize = dataLoader.getVocabSize();
            int blockSize = 3;
            int embeddingDim = 10;
            int hiddenSize = 200;
            double eps = 1e-5;  // BatchNorm epsilon

            System.out.println("=== Configuration ===");
            System.out.println("Vocabulary size: " + vocabSize);
            System.out.println("Block size: " + blockSize);
            System.out.println("Embedding dim: " + embeddingDim);
            System.out.println("Hidden size: " + hiddenSize);
            System.out.println();

            // Initialize parameters
            Random rng = new Random(2147483647L);

            // Embedding table
            Tensor C = Tensor.randn(rng, vocabSize, embeddingDim);

            // First layer
            Tensor W1 = Tensor.randn(rng, blockSize * embeddingDim, hiddenSize);
            scaleInit(W1, 5.0/3.0); // For tanh
            Tensor b1 = Tensor.randn(rng, hiddenSize).mul(0.01); // Small random (not zero!)

            // BatchNorm parameters
            Tensor bngain = Tensor.ones(hiddenSize);
            Tensor bnbias = Tensor.zeros(hiddenSize);

            // Second layer
            Tensor W2 = Tensor.randn(rng, hiddenSize, vocabSize);
            scaleInit(W2, 0.01); // Small for last layer
            Tensor b2 = Tensor.randn(rng, vocabSize).mul(0.01);

            List<Tensor> parameters = Arrays.asList(C, W1, b1, W2, b2, bngain, bnbias);

            int totalParams = 0;
            for (Tensor p : parameters) {
                totalParams += p.getSize();
            }
            System.out.println("Total parameters: " + totalParams);
            System.out.println();

            // Training
            System.out.println("=== Training with Manual Backprop ===");
            System.out.println("âš ï¸  NOT using loss.backward() - all gradients computed manually!\n");

            int maxIters = 200000;
            int batchSize = 32;
            double lr = 0.1;

            Tensor Xtr = dataLoader.getXtr();
            Tensor Ytr = dataLoader.getYtr();
            int nTrain = Xtr.getShape()[0];

            List<Double> losses = new ArrayList<>();

            for (int iter = 0; iter < maxIters; iter++) {
                // Mini-batch
                int[] batchIndices = randomBatch(nTrain, batchSize, rng);
                Tensor Xb = selectRows(Xtr, batchIndices);
                Tensor Yb = selectRows(Ytr, batchIndices);

                // ============================================
                // FORWARD PASS (expanded for manual backprop)
                // ============================================

                // Embedding
                Tensor emb = C.index(Xb);  // (batch, blockSize, embDim)
                Tensor embcat = emb.view(batchSize, blockSize * embeddingDim);

                // First layer
                Tensor hprebn = embcat.matmul(W1).add(b1);  // (batch, hiddenSize)

                // BatchNorm (manual implementation for forward)
                Tensor bnmean = hprebn.mean(0);  // (hiddenSize,)
                Tensor bnvar = hprebn.variance(0);  // (hiddenSize,)

                Tensor hprebn_centered = hprebn.subtract(bnmean);
                Tensor bnstd = bnvar.add(eps).sqrt();
                Tensor bnraw = hprebn_centered.div(bnstd);
                Tensor hpreact = bnraw.mul(bngain).add(bnbias);

                // Tanh activation
                Tensor h = hpreact.tanh();

                // Second layer
                Tensor logits = h.matmul(W2).add(b2);  // (batch, vocabSize)

                // Loss (using manual cross-entropy)
                Tensor probs = logits.softmax(1);
                double loss = crossEntropyLoss(probs, Yb, batchSize, vocabSize);

                // ============================================
                // BACKWARD PASS (manual!)
                // ============================================

                // Start with gradient of loss w.r.t. logits
                // Using our optimized backward pass!
                Tensor dlogits = ManualBackprop.crossEntropyBackward(logits, Yb);

                // Backward through second layer
                LinearGradients layer2Grads = ManualBackprop.linearBackward(
                        dlogits, h, W2);
                Tensor dh = layer2Grads.dx;
                Tensor dW2 = layer2Grads.dW;
                Tensor db2 = layer2Grads.db;

                // Backward through tanh
                Tensor dhpreact = ManualBackprop.tanhBackward(dh, h);

                // Backward through BatchNorm (using our optimized implementation!)
                BatchNormGradients bnGrads = ManualBackprop.batchNormBackward(
                        dhpreact, hprebn, bngain, eps);
                Tensor dhprebn = bnGrads.dx;
                Tensor dbngain = bnGrads.dgamma;
                Tensor dbnbias = bnGrads.dbeta;

                // Backward through first layer
                LinearGradients layer1Grads = ManualBackprop.linearBackward(
                        dhprebn, embcat, W1);
                Tensor dembcat = layer1Grads.dx;
                Tensor dW1 = layer1Grads.dW;
                Tensor db1 = layer1Grads.db;

                // Backward through embedding
                Tensor demb = dembcat.view(batchSize, blockSize, embeddingDim);
                Tensor dC = ManualBackprop.embeddingBackward(
                        demb, Xb, vocabSize, embeddingDim);

                // ============================================
                // UPDATE (using our manual gradients!)
                // ============================================

                // Learning rate decay
                double currentLr = lr;
                if (iter >= 150000) {
                    currentLr = 0.01;
                }

                // Update all parameters
                updateParameter(C, dC, currentLr);
                updateParameter(W1, dW1, currentLr);
                updateParameter(b1, db1, currentLr);
                updateParameter(W2, dW2, currentLr);
                updateParameter(b2, db2, currentLr);
                updateParameter(bngain, dbngain, currentLr);
                updateParameter(bnbias, dbnbias, currentLr);

                // Logging
                losses.add(loss);

                if (iter % 10000 == 0 || iter == maxIters - 1) {
                    double trainLoss = evaluateSample(
                            C, W1, b1, W2, b2, bngain, bnbias,
                            Xtr, Ytr, 500, blockSize, embeddingDim, hiddenSize,
                            vocabSize, eps, rng);

                    double devLoss = evaluateSample(
                            C, W1, b1, W2, b2, bngain, bnbias,
                            dataLoader.getXdev(), dataLoader.getYdev(), 500,
                            blockSize, embeddingDim, hiddenSize, vocabSize, eps, rng);

                    System.out.printf("Iter %d: loss=%.4f, trainâ‰ˆ%.4f, devâ‰ˆ%.4f (lr=%.3f)\n",
                            iter, loss, trainLoss, devLoss, currentLr);
                }

                if (iter % 1000 == 0 && iter > 0) {
                    System.gc(); // Suggest GC
                }
            }

            System.out.println("\nğŸ“‰ Learning rate decayed to 0.01 at iter 150000");

            // Final evaluation (ä½¿ç”¨æŠ½æ¨£é¿å… OOM)
            System.out.println("\n=== Final Evaluation ===");
            Random evalRng = new Random(42);
            double finalTrain = evaluateSample(
                    C, W1, b1, W2, b2, bngain, bnbias,
                    Xtr, Ytr, 1000, blockSize, embeddingDim, hiddenSize,
                    vocabSize, eps, evalRng);
            double finalDev = evaluateSample(
                    C, W1, b1, W2, b2, bngain, bnbias,
                    dataLoader.getXdev(), dataLoader.getYdev(), 1000,
                    blockSize, embeddingDim, hiddenSize, vocabSize, eps, evalRng);
            double finalTest = evaluateSample(
                    C, W1, b1, W2, b2, bngain, bnbias,
                    dataLoader.getXte(), dataLoader.getYte(), 1000,
                    blockSize, embeddingDim, hiddenSize, vocabSize, eps, evalRng);

            System.out.printf("Train loss (1000 samples): %.4f\n", finalTrain);
            System.out.printf("Dev loss (1000 samples): %.4f\n", finalDev);
            System.out.printf("Test loss (1000 samples): %.4f\n", finalTest);

            // Sample names
            System.out.println("\n=== Sampling 20 Names ===");
            List<String> samples = sampleNames(
                    C, W1, b1, W2, b2, bngain, bnbias,
                    20, blockSize, embeddingDim, hiddenSize, vocabSize, eps,
                    dataLoader.getIdxToChar(), rng);

            for (int i = 0; i < samples.size(); i++) {
                System.out.printf("%2d. %s\n", i + 1, samples.get(i));
            }

            // Summary
            printSummary();

        } catch (Exception e) {
            System.err.println("Error: " + e.getMessage());
            e.printStackTrace();
        }
    }

    private static void scaleInit(Tensor t, double factor) {
        double[] data = t.getData();
        for (int i = 0; i < data.length; i++) {
            data[i] *= factor;
        }
    }

    private static void updateParameter(Tensor param, Tensor grad, double lr) {
        double[] pData = param.getData();
        double[] gData = grad.getData();
        for (int i = 0; i < pData.length; i++) {
            pData[i] -= lr * gData[i];
        }
    }

    private static double crossEntropyLoss(Tensor probs, Tensor targets,
                                           int batchSize, int vocabSize) {
        double[] probData = probs.getData();
        double[] targetData = targets.getData();

        double totalLoss = 0.0;
        for (int i = 0; i < batchSize; i++) {
            int target = (int) targetData[i];
            double prob = probData[i * vocabSize + target];
            totalLoss += -Math.log(Math.max(prob, 1e-10));
        }

        return totalLoss / batchSize;
    }

    private static Tensor forward(Tensor C, Tensor W1, Tensor b1,
                                  Tensor W2, Tensor b2,
                                  Tensor bngain, Tensor bnbias,
                                  Tensor X, int blockSize, int embeddingDim,
                                  int hiddenSize, double eps) {
        int batchSize = X.getShape()[0];

        Tensor emb = C.index(X);
        Tensor embcat = emb.view(batchSize, blockSize * embeddingDim);

        Tensor hprebn = embcat.matmul(W1).add(b1);

        // BatchNorm
        Tensor bnmean = hprebn.mean(0);
        Tensor bnvar = hprebn.variance(0);
        Tensor hprebn_centered = hprebn.subtract(bnmean);
        Tensor bnstd = bnvar.add(eps).sqrt();
        Tensor bnraw = hprebn_centered.div(bnstd);
        Tensor hpreact = bnraw.mul(bngain).add(bnbias);

        Tensor h = hpreact.tanh();
        Tensor logits = h.matmul(W2).add(b2);

        return logits;
    }

    private static double evaluateSample(Tensor C, Tensor W1, Tensor b1,
                                         Tensor W2, Tensor b2,
                                         Tensor bngain, Tensor bnbias,
                                         Tensor X, Tensor Y, int sampleSize,
                                         int blockSize, int embeddingDim,
                                         int hiddenSize, int vocabSize,
                                         double eps, Random rng) {
        int n = X.getShape()[0];
        sampleSize = Math.min(sampleSize, n);

        int[] indices = randomBatch(n, sampleSize, rng);
        Tensor Xs = selectRows(X, indices);
        Tensor Ys = selectRows(Y, indices);

        Tensor logits = forward(C, W1, b1, W2, b2, bngain, bnbias,
                Xs, blockSize, embeddingDim, hiddenSize, eps);
        Tensor probs = logits.softmax(1);

        return crossEntropyLoss(probs, Ys, sampleSize, vocabSize);
    }

    private static double evaluateFull(Tensor C, Tensor W1, Tensor b1,
                                       Tensor W2, Tensor b2,
                                       Tensor bngain, Tensor bnbias,
                                       Tensor X, Tensor Y,
                                       int blockSize, int embeddingDim,
                                       int hiddenSize, int vocabSize,
                                       double eps) {
        int batchSize = X.getShape()[0];

        Tensor logits = forward(C, W1, b1, W2, b2, bngain, bnbias,
                X, blockSize, embeddingDim, hiddenSize, eps);
        Tensor probs = logits.softmax(1);

        return crossEntropyLoss(probs, Y, batchSize, vocabSize);
    }

    private static List<String> sampleNames(Tensor C, Tensor W1, Tensor b1,
                                            Tensor W2, Tensor b2,
                                            Tensor bngain, Tensor bnbias,
                                            int n, int blockSize, int embeddingDim,
                                            int hiddenSize, int vocabSize, double eps,
                                            Map<Integer, Character> idxToChar,
                                            Random rng) {
        List<String> samples = new ArrayList<>();

        for (int s = 0; s < n; s++) {
            List<Integer> out = new ArrayList<>();
            int[] context = new int[blockSize];

            while (true) {
                double[] contextData = new double[blockSize];
                for (int i = 0; i < blockSize; i++) {
                    contextData[i] = context[i];
                }
                Tensor contextTensor = new Tensor(contextData, new int[]{1, blockSize});

                Tensor logits = forward(C, W1, b1, W2, b2, bngain, bnbias,
                        contextTensor, blockSize, embeddingDim,
                        hiddenSize, eps);
                Tensor probs = logits.softmax(1);

                int nextIdx = sampleMultinomial(probs.getData(), rng);

                System.arraycopy(context, 1, context, 0, blockSize - 1);
                context[blockSize - 1] = nextIdx;
                out.add(nextIdx);

                if (nextIdx == 0 || out.size() > 20) break;
            }

            StringBuilder name = new StringBuilder();
            for (int idx : out) {
                if (idx == 0) break;
                name.append(idxToChar.get(idx));
            }
            samples.add(name.toString());
        }

        return samples;
    }

    private static int sampleMultinomial(double[] probs, Random rng) {
        double r = rng.nextDouble();
        double cumProb = 0.0;
        for (int i = 0; i < probs.length; i++) {
            cumProb += probs[i];
            if (r < cumProb) return i;
        }
        return probs.length - 1;
    }

    private static int[] randomBatch(int n, int batchSize, Random rng) {
        int[] indices = new int[batchSize];
        for (int i = 0; i < batchSize; i++) {
            indices[i] = rng.nextInt(n);
        }
        return indices;
    }

    private static Tensor selectRows(Tensor t, int[] indices) {
        int[] shape = t.getShape();
        int dim = shape.length == 1 ? 1 : shape[1];

        double[] selected = new double[indices.length * dim];
        for (int i = 0; i < indices.length; i++) {
            System.arraycopy(t.getData(), indices[i] * dim, selected, i * dim, dim);
        }

        int[] newShape = shape.length == 1 ?
                new int[]{indices.length} :
                new int[]{indices.length, dim};

        return new Tensor(selected, newShape);
    }

    private static void printSummary() {
        System.out.println("\n" + "=".repeat(60));
        System.out.println("KEY INSIGHTS FROM LECTURE 5");
        System.out.println("=".repeat(60));

        System.out.println("\nğŸ¥· What We Accomplished:");
        System.out.println("   âœ… Trained entire network WITHOUT loss.backward()");
        System.out.println("   âœ… Calculated ALL gradients manually");
        System.out.println("   âœ… Used optimized backward passes");
        System.out.println("   âœ… Achieved same results as autograd");

        System.out.println("\nğŸ”‘ Core Concepts:");
        System.out.println("   1. Chain Rule is Everything");
        System.out.println("      dloss/dx = dloss/dy * dy/dx");
        System.out.println("      Local gradient Ã— Global gradient");

        System.out.println("\n   2. Gradient Shapes Must Match");
        System.out.println("      dX.shape == X.shape (always!)");
        System.out.println("      Broadcasting backward needs sum()");

        System.out.println("\n   3. Optimization Matters");
        System.out.println("      Cross-Entropy: 3 lines vs 10+ lines");
        System.out.println("      BatchNorm: 5 lines vs 15+ lines");
        System.out.println("      Performance: 3-5x faster!");

        System.out.println("\nğŸ“Š Common Gradients:");
        System.out.println("   y = xÂ²        â†’ dy/dx = 2x");
        System.out.println("   y = e^x       â†’ dy/dx = e^x = y");
        System.out.println("   y = log(x)    â†’ dy/dx = 1/x");
        System.out.println("   y = tanh(x)   â†’ dy/dx = 1 - yÂ²");
        System.out.println("   y = sum(x)    â†’ dy/dx = 1 (broadcast)");
        System.out.println("   y = x @ W     â†’ dy/dW = x^T @ dy");

        System.out.println("\nğŸ’¡ Why This Matters:");
        System.out.println("   â€¢ Deep understanding of neural nets");
        System.out.println("   â€¢ Can implement custom operations");
        System.out.println("   â€¢ Can optimize critical paths");
        System.out.println("   â€¢ Can debug gradient issues");
        System.out.println("   â€¢ No longer dependent on frameworks");

        System.out.println("\nğŸ¯ Historical Context:");
        System.out.println("   Before 2015: Everyone did this manually!");
        System.out.println("   2010-2014: Karpathy's research code");
        System.out.println("   Now: Autograd makes it easy");
        System.out.println("   But: Understanding is still crucial");

        System.out.println("\nğŸš€ What's Next:");
        System.out.println("   â€¢ Lecture 6: WaveNet (convolutional architecture)");
        System.out.println("   â€¢ Lecture 7: GPT (transformer)");
        System.out.println("   â€¢ Lecture 8: Tokenizer (BPE)");

        System.out.println("\nâœ… Lecture 5 Complete - You're a Backprop Ninja! ğŸ¥·\n");
    }
}