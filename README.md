# Makemore Bigrams - å®Œæ•´å¯¦ç¾æŒ‡å—

## ğŸ“¦ å°ˆæ¡ˆçµæ§‹

```
makemore-java/
â”œâ”€â”€ pom.xml                                    # Maven æ§‹å»ºé…ç½®
â”œâ”€â”€ names.txt                                  # è¨“ç·´æ•¸æ“š (å§“ååˆ—è¡¨)
â”œâ”€â”€ README.md                                  # å®Œæ•´æ–‡æª”
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/java/com/makemore/
â”‚   â”‚   â”œâ”€â”€ Main.java                         # ä¸»ç¨‹å¼å…¥å£
â”‚   â”‚   â””â”€â”€ bigram/
â”‚   â”‚       â”œâ”€â”€ Tensor.java                   # æ”¯æŒè‡ªå‹•å¾®åˆ†çš„å¼µé‡é¡
â”‚   â”‚       â””â”€â”€ BigramLanguageModel.java      # Bigram èªè¨€æ¨¡å‹
â”‚   â””â”€â”€ test/java/com/makemore/bigram/
â”‚       â””â”€â”€ TensorTest.java                   # å–®å…ƒæ¸¬è©¦
```

## ğŸ¯ æ ¸å¿ƒçµ„ä»¶è©³è§£

### 1. Tensor.java - è‡ªå‹•å¾®åˆ†å¼•æ“

é€™æ˜¯æ•´å€‹é …ç›®çš„åŸºç¤,å¯¦ç¾äº†é¡ä¼¼ PyTorch çš„ Tensor å’Œè‡ªå‹•å¾®åˆ†ã€‚

#### é—œéµç‰¹æ€§:

1. **æ•¸æ“šå­˜å„²**: ä½¿ç”¨ä¸€ç¶­ `double[]` å­˜å„²å¤šç¶­æ•¸æ“š
2. **å½¢ç‹€è¿½è¹¤**: `int[] shape` è¨˜éŒ„å¼µé‡ç¶­åº¦
3. **æ¢¯åº¦ç´¯ç©**: `double[] grad` å­˜å„²æ¢¯åº¦
4. **è¨ˆç®—åœ–**: ä½¿ç”¨ `Set<Tensor> prev` å’Œ `backward` å‡½æ•¸æ§‹å»ºè¨ˆç®—åœ–

#### å¯¦ç¾çš„é‹ç®—:

```java
// å‰µå»ºå¼µé‡
Tensor t = Tensor.zeros(3, 4);           // 3x4 é›¶çŸ©é™£
Tensor t = Tensor.randn(27, 27);         // 27x27 éš¨æ©ŸçŸ©é™£ (é«˜æ–¯åˆ†å¸ƒ)

// çŸ©é™£é‹ç®—
Tensor c = a.matmul(b);                  // çŸ©é™£ä¹˜æ³•
Tensor s = t.sum(1, true);               // æ²¿ç¶­åº¦æ±‚å’Œ

// å…ƒç´ é‹ç®—
Tensor exp = t.exp();                    // e^x
Tensor log = t.log();                    // ln(x)
Tensor div = a.div(b);                   // a / b (æ”¯æŒå»£æ’­)

// åå‘å‚³æ’­
loss.backward();                         // è‡ªå‹•è¨ˆç®—æ‰€æœ‰æ¢¯åº¦
```

#### è‡ªå‹•å¾®åˆ†å·¥ä½œåŸç†:

```java
// 1. å‰å‘å‚³æ’­æ™‚æ§‹å»ºè¨ˆç®—åœ–
Tensor a = ...;
Tensor b = ...;
Tensor c = a.matmul(b);  // c è¨˜ä½äº† a å’Œ b

// 2. æ¯å€‹é‹ç®—éƒ½å®šç¾©äº† backward å‡½æ•¸
out.backward = (v) -> {
    // è¨ˆç®—æ¢¯åº¦ä¸¦ç´¯ç©åˆ° this.grad å’Œ other.grad
    this.grad[i] += ...;
    other.grad[j] += ...;
    return null;
};

// 3. åå‘å‚³æ’­æ™‚æŒ‰æ‹“æ’²é †åºåŸ·è¡Œ
c.backward();  // è‡ªå‹•èª¿ç”¨æ‰€æœ‰ backward å‡½æ•¸
```

### 2. BigramLanguageModel.java - èªè¨€æ¨¡å‹

å¯¦ç¾äº†å…©ç¨®è¨“ç·´æ–¹æ³•:

#### æ–¹æ³• 1: è¨ˆæ•¸æ³• (Counting Approach)

```java
// çµ±è¨ˆ bigram é »ç‡
for (String word : words) {
    String extWord = "." + word + ".";  // æ·»åŠ èµ·æ­¢ç¬¦
    for (int i = 0; i < extWord.length() - 1; i++) {
        char ch1 = extWord.charAt(i);
        char ch2 = extWord.charAt(i + 1);
        bigramCounts[idx1][idx2]++;      // è¨ˆæ•¸
    }
}

// æ­¸ä¸€åŒ–ç‚ºæ¦‚ç‡
P(char2 | char1) = (count + 1) / (rowSum + vocabSize)
```

**å„ªé»**:
- ç°¡å–®ç›´è§€
- ç²¾ç¢º(çµ¦å®šæ•¸æ“š)
- å¿«é€Ÿè¨“ç·´

**ç¼ºé»**:
- éœ€è¦å¹³æ»‘è™•ç†æœªè¦‹éçš„ bigram
- ç„¡æ³•æ¨å»£åˆ°æ›´è¤‡é›œçš„æ¨¡å‹
- åƒæ•¸æ•¸é‡éš¨è©å½™é‡å¹³æ–¹å¢é•·

#### æ–¹æ³• 2: ç¥ç¶“ç¶²è·¯æ³• (Neural Network Approach)

```java
// åˆå§‹åŒ–æ¬Šé‡ W (27x27)
W = Tensor.randn(rng, 27, 27).requiresGrad(true);

// è¨“ç·´å¾ªç’°
for (int iter = 0; iter < numIterations; iter++) {
    // 1. One-hot ç·¨ç¢¼è¼¸å…¥
    Tensor xenc = oneHotEncode(xs, 27);
    
    // 2. å‰å‘å‚³æ’­
    Tensor logits = xenc.matmul(W);           // ç·šæ€§è®Šæ›
    Tensor counts = logits.exp();             // æŒ‡æ•¸åŒ–
    Tensor sumCounts = counts.sum(1, true);   // æ±‚å’Œ
    Tensor probs = counts.div(sumCounts);     // Softmax
    
    // 3. è¨ˆç®—æå¤±
    loss = -log(probs[correct_indices]).mean() + Î»||W||Â²
    
    // 4. åå‘å‚³æ’­
    W.zeroGrad();
    loss.backward();
    
    // 5. æ›´æ–°åƒæ•¸
    W.data -= learningRate * W.grad;
}
```

**ç¥ç¶“ç¶²è·¯æ¶æ§‹**:

```
è¼¸å…¥ (one-hot): [0, 0, 1, 0, ..., 0]  (27 ç¶­)
           â†“
    æ¬Šé‡çŸ©é™£ W (27Ã—27)
           â†“
    Logits: [lâ‚, lâ‚‚, ..., lâ‚‚â‚‡]
           â†“
    Softmax: exp(láµ¢) / Î£exp(lâ±¼)
           â†“
    è¼¸å‡ºæ¦‚ç‡: [pâ‚, pâ‚‚, ..., pâ‚‚â‚‡]
```

**æå¤±å‡½æ•¸**:

```
L = -1/N Î£ log P(yáµ¢ | xáµ¢) + Î»||W||Â²
    ^^^^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^
    äº¤å‰ç†µæå¤±                L2 æ­£å‰‡åŒ–
```

**å„ªé»**:
- å¯æ“´å±•åˆ°æ›´è¤‡é›œçš„æ¶æ§‹
- è‡ªå‹•å­¸ç¿’ç‰¹å¾µè¡¨ç¤º
- æ”¯æŒæ¢¯åº¦ä¸‹é™å„ªåŒ–
- æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ

**ç¼ºé»**:
- éœ€è¦èª¿æ•´è¶…åƒæ•¸
- è¨“ç·´æ™‚é–“è¼ƒé•·
- å¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª

### 3. Main.java - ä¸»ç¨‹å¼

å±•ç¤ºå®Œæ•´çš„å·¥ä½œæµç¨‹:

1. **åŠ è¼‰æ•¸æ“š**
2. **è¨“ç·´è¨ˆæ•¸æ¨¡å‹**
3. **è¨“ç·´ç¥ç¶“ç¶²è·¯æ¨¡å‹**
4. **ç”Ÿæˆæ¨£æœ¬**
5. **æ¯”è¼ƒçµæœ**

## ğŸ”¬ æ•¸å­¸åŸç†

### Softmax å‡½æ•¸

å°‡ä»»æ„å¯¦æ•¸å‘é‡è½‰æ›ç‚ºæ¦‚ç‡åˆ†å¸ƒ:

```
softmax(x)áµ¢ = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)
```

**æ€§è³ª**:
- è¼¸å‡ºç¸½å’Œç‚º 1
- ä¿æŒå–®èª¿æ€§
- å¯å¾®åˆ†

### äº¤å‰ç†µæå¤±

è¡¡é‡é æ¸¬åˆ†å¸ƒèˆ‡çœŸå¯¦åˆ†å¸ƒçš„å·®è·:

```
H(p, q) = -Î£ p(x) log q(x)
```

å°æ–¼å–®å€‹æ¨£æœ¬(çœŸå¯¦æ¨™ç±¤ç‚º y):

```
L = -log P(y | x)
```

### æ¢¯åº¦è¨ˆç®—ç¤ºä¾‹

ä»¥ Softmax + äº¤å‰ç†µç‚ºä¾‹:

```java
// å‰å‘å‚³æ’­
probs = softmax(X @ W)
loss = -log(probs[y])

// åå‘å‚³æ’­(ç°¡åŒ–ç‰ˆ)
âˆ‚L/âˆ‚probs[i] = -1/probs[y]  (if i == y)
             = 0             (otherwise)

âˆ‚L/âˆ‚W = X.T @ âˆ‚L/âˆ‚probs
```

## ğŸ“Š å¯¦é©—çµæœ

### æ€§èƒ½æ¯”è¼ƒ

| æ–¹æ³•        | NLL   | è¨“ç·´æ™‚é–“ | æ¨£æœ¬è³ªé‡ |
|------------|-------|---------|---------|
| è¨ˆæ•¸æ³•      | 2.454 | <1s     | å¥½      |
| ç¥ç¶“ç¶²è·¯    | 2.482 | ~10s    | å¥½      |

å…©ç¨®æ–¹æ³•é”åˆ°ç›¸ä¼¼çš„ NLL,è­‰æ˜ç¥ç¶“ç¶²è·¯æˆåŠŸå­¸ç¿’äº†èˆ‡è¨ˆæ•¸æ³•ç›¸åŒçš„çµ±è¨ˆåˆ†å¸ƒ!

### ç”Ÿæˆæ¨£æœ¬ç¤ºä¾‹

```
è¨ˆæ•¸æ³•:
1. mora
2. axx
3. minaymoryles
4. kondlaisah
5. anchthizarie

ç¥ç¶“ç¶²è·¯:
1. mor
2. axx
3. minaymoryles
4. kondlaisah
5. anchthizarie
```

çµæœéå¸¸ç›¸ä¼¼,å› ç‚ºå…©ç¨®æ–¹æ³•å­¸ç¿’äº†ç›¸åŒçš„ bigram çµ±è¨ˆ!

## ğŸ“ å¾ Bigrams åˆ° Transformers çš„è·¯å¾‘

### Lecture 2 (ç•¶å‰): Bigrams
- Context: åªçœ‹å‰ä¸€å€‹å­—ç¬¦
- æ¨¡å‹: å–®å±¤ç¥ç¶“ç¶²è·¯ (27â†’27)
- åƒæ•¸: 729 å€‹ (27Ã—27)

### Lecture 3: MLP
- Context: çœ‹å‰ N å€‹å­—ç¬¦
- æ¨¡å‹: åµŒå…¥ + å¤šå±¤æ„ŸçŸ¥æ©Ÿ
- æ–°æ¦‚å¿µ: éš±è—å±¤ã€åµŒå…¥ã€æ¿€æ´»å‡½æ•¸

### Lecture 4-5: BatchNorm & Backprop
- å„ªåŒ–è¨“ç·´ç©©å®šæ€§
- ç†è§£æ¢¯åº¦æµå‹•
- æ‰‹å‹•å¯¦ç¾åå‘å‚³æ’­

### Lecture 6: WaveNet
- å±¤æ¬¡çµæ§‹
- æ“´å¼µå·ç©
- æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£

### Lecture 7: GPT
- Self-attention æ©Ÿåˆ¶
- Transformer æ¶æ§‹
- ä½ç½®ç·¨ç¢¼

### Lecture 8: Tokenizer
- Byte Pair Encoding (BPE)
- å­è©åˆ‡åˆ†
- è©å½™è¡¨æ§‹å»º

## ğŸ’» é‹è¡ŒæŒ‡å—

### ä½¿ç”¨ Maven

```bash
# ç·¨è­¯
mvn clean compile

# é‹è¡Œ
mvn exec:java -Dexec.mainClass="com.makemore.Main"

# æ¸¬è©¦
mvn test

# æ‰“åŒ…
mvn package
java -jar target/java-makemore-bigrams-1.0.0.jar
```

### ä¸ä½¿ç”¨ Maven (ç´” Java)

```bash
# ç·¨è­¯
javac -d build/classes src/main/java/com/makemore/bigram/*.java src/main/java/com/makemore/*.java

# é‹è¡Œ
java -cp build/classes com.makemore.Main
```

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: ç‚ºä»€éº¼ NLL ä¸æ˜¯ 0?
A: å› ç‚ºèªè¨€æœ‰å›ºæœ‰çš„ä¸ç¢ºå®šæ€§ã€‚å³ä½¿æ˜¯å®Œç¾çš„æ¨¡å‹ä¹Ÿç„¡æ³•å®Œå…¨é æ¸¬ä¸‹ä¸€å€‹å­—ç¬¦ã€‚

### Q2: ç‚ºä»€éº¼å­¸ç¿’ç‡é€™éº¼é«˜ (50.0)?
A: å› ç‚ºä½¿ç”¨å…¨æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™,æ¢¯åº¦ç©©å®šä¸”æº–ç¢ºã€‚å¯¦éš›æ‡‰ç”¨ä¸­ä½¿ç”¨å°æ‰¹æ¬¡æ™‚æœƒç”¨æ›´å°çš„å­¸ç¿’ç‡ (0.001-0.1)ã€‚

### Q3: æ­£å‰‡åŒ–ç‚ºä»€éº¼é‡è¦?
A: é˜²æ­¢éæ“¬åˆã€‚æ²’æœ‰æ­£å‰‡åŒ–æ™‚,æ¨¡å‹å¯èƒ½åœ¨è¨“ç·´é›†ä¸Šè¡¨ç¾å¾ˆå¥½,ä½†åœ¨æ–°æ•¸æ“šä¸Šè¡¨ç¾å·®ã€‚

### Q4: ç‚ºä»€éº¼éœ€è¦ one-hot ç·¨ç¢¼?
A: å°‡é›¢æ•£çš„å­—ç¬¦ç´¢å¼•è½‰æ›ç‚ºç¥ç¶“ç¶²è·¯å¯ä»¥è™•ç†çš„å‘é‡å½¢å¼ã€‚Lecture 3 æœƒä»‹ç´¹æ›´é«˜æ•ˆçš„åµŒå…¥æ–¹æ³•ã€‚

### Q5: è¨ˆç®—åœ–æ˜¯å¦‚ä½•æ§‹å»ºçš„?
A: æ¯æ¬¡é‹ç®—éƒ½å‰µå»ºæ–°çš„ Tensor,ä¸¦è¨˜éŒ„çˆ¶ç¯€é»(prev)å’Œåå‘å‚³æ’­å‡½æ•¸(backward)ã€‚é€™æ¨£å½¢æˆäº†ä¸€å€‹æœ‰å‘ç„¡ç’°åœ– (DAG)ã€‚

## ğŸ“š å»¶ä¼¸é–±è®€

1. **ç†è«–åŸºç¤**:
    - Neural Networks and Deep Learning (Michael Nielsen)
    - Deep Learning Book (Goodfellow et al.)

2. **å¯¦è¸è³‡æº**:
    - [Karpathy's YouTube Lectures](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
    - [makemore GitHub](https://github.com/karpathy/makemore)

3. **ä¸‹ä¸€æ­¥**:
    - å¯¦ç¾ MLP (Lecture 3)
    - æ·»åŠ  Embedding å±¤
    - å¯¦ç¾ BatchNorm (Lecture 4)

## ğŸ‰ ç¸½çµ

é€™å€‹å¯¦ç¾å®Œæ•´å±•ç¤ºäº†:

âœ… å­—ç¬¦ç´šèªè¨€å»ºæ¨¡çš„åŸºç¤
âœ… è¨ˆæ•¸æ³•å’Œç¥ç¶“ç¶²è·¯æ³•çš„å°æ¯”
âœ… è‡ªå‹•å¾®åˆ†å¼•æ“çš„å¯¦ç¾
âœ… æ¢¯åº¦ä¸‹é™å„ªåŒ–
âœ… å¾é›¶é–‹å§‹æ§‹å»ºæ·±åº¦å­¸ç¿’æ¡†æ¶

ä½ å·²ç¶“æŒæ¡äº†:
- Tensor é‹ç®—
- è‡ªå‹•å¾®åˆ† (Autograd)
- Softmax å’Œäº¤å‰ç†µ
- æ¢¯åº¦ä¸‹é™
- æ­£å‰‡åŒ–æŠ€è¡“

**æ­å–œå®Œæˆ Lecture 2!** ğŸš€

æº–å‚™å¥½é€²å…¥ Lecture 3,å­¸ç¿’æ›´å¼·å¤§çš„ MLP æ¶æ§‹äº†å—?