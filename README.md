# Makemore Bigrams - å®Œæ•´å¯¦ç¾æŒ‡å—

## ğŸ“¦ å°ˆæ¡ˆçµæ§‹

```
makemore-java/
â”œâ”€â”€ pom.xml                                    # Maven æ§‹å»ºé…ç½®
â”œâ”€â”€ names.txt                                  # è¨“ç·´æ•¸æ“š (å§“ååˆ—è¡¨)
â”œâ”€â”€ README.md                                  # å®Œæ•´æ–‡æª”
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/java/com/makemore/
â”‚   â”‚   â”œâ”€â”€ Main.java                         # ä¸»ç¨‹å¼å…¥å£
â”‚   â”‚   â””â”€â”€ bigram/
â”‚   â”‚       â”œâ”€â”€ Tensor.java                   # æ”¯æŒè‡ªå‹•å¾®åˆ†çš„å¼µé‡é¡
â”‚   â”‚       â””â”€â”€ BigramLanguageModel.java      # Bigram èªè¨€æ¨¡å‹
â”‚   â””â”€â”€ test/java/com/makemore/bigram/
â”‚       â””â”€â”€ TensorTest.java                   # å–®å…ƒæ¸¬è©¦
```

## ğŸ¯ æ ¸å¿ƒçµ„ä»¶è©³è§£

### 1. Tensor.java - è‡ªå‹•å¾®åˆ†å¼•æ“

é€™æ˜¯æ•´å€‹é …ç›®çš„åŸºç¤,å¯¦ç¾äº†é¡ä¼¼ PyTorch çš„ Tensor å’Œè‡ªå‹•å¾®åˆ†ã€‚

#### é—œéµç‰¹æ€§:

1. **æ•¸æ“šå­˜å„²**: ä½¿ç”¨ä¸€ç¶­ `double[]` å­˜å„²å¤šç¶­æ•¸æ“š
2. **å½¢ç‹€è¿½è¹¤**: `int[] shape` è¨˜éŒ„å¼µé‡ç¶­åº¦
3. **æ¢¯åº¦ç´¯ç©**: `double[] grad` å­˜å„²æ¢¯åº¦
4. **è¨ˆç®—åœ–**: ä½¿ç”¨ `Set<Tensor> prev` å’Œ `backward` å‡½æ•¸æ§‹å»ºè¨ˆç®—åœ–

#### å¯¦ç¾çš„é‹ç®—:

```java
// å‰µå»ºå¼µé‡
Tensor t = Tensor.zeros(3, 4);           // 3x4 é›¶çŸ©é™£
Tensor t = Tensor.randn(27, 27);         // 27x27 éš¨æ©ŸçŸ©é™£ (é«˜æ–¯åˆ†å¸ƒ)

// çŸ©é™£é‹ç®—
Tensor c = a.matmul(b);                  // çŸ©é™£ä¹˜æ³•
Tensor s = t.sum(1, true);               // æ²¿ç¶­åº¦æ±‚å’Œ

// å…ƒç´ é‹ç®—
Tensor exp = t.exp();                    // e^x
Tensor log = t.log();                    // ln(x)
Tensor div = a.div(b);                   // a / b (æ”¯æŒå»£æ’­)

// åå‘å‚³æ’­
loss.backward();                         // è‡ªå‹•è¨ˆç®—æ‰€æœ‰æ¢¯åº¦
```

#### è‡ªå‹•å¾®åˆ†å·¥ä½œåŸç†:

```java
// 1. å‰å‘å‚³æ’­æ™‚æ§‹å»ºè¨ˆç®—åœ–
Tensor a = ...;
Tensor b = ...;
Tensor c = a.matmul(b);  // c è¨˜ä½äº† a å’Œ b

// 2. æ¯å€‹é‹ç®—éƒ½å®šç¾©äº† backward å‡½æ•¸
out.backward = (v) -> {
    // è¨ˆç®—æ¢¯åº¦ä¸¦ç´¯ç©åˆ° this.grad å’Œ other.grad
    this.grad[i] += ...;
    other.grad[j] += ...;
    return null;
};

// 3. åå‘å‚³æ’­æ™‚æŒ‰æ‹“æ’²é †åºåŸ·è¡Œ
c.backward();  // è‡ªå‹•èª¿ç”¨æ‰€æœ‰ backward å‡½æ•¸
```

### 2. BigramLanguageModel.java - èªè¨€æ¨¡å‹

å¯¦ç¾äº†å…©ç¨®è¨“ç·´æ–¹æ³•:

#### æ–¹æ³• 1: è¨ˆæ•¸æ³• (Counting Approach)

```java
// çµ±è¨ˆ bigram é »ç‡
for (String word : words) {
    String extWord = "." + word + ".";  // æ·»åŠ èµ·æ­¢ç¬¦
    for (int i = 0; i < extWord.length() - 1; i++) {
        char ch1 = extWord.charAt(i);
        char ch2 = extWord.charAt(i + 1);
        bigramCounts[idx1][idx2]++;      // è¨ˆæ•¸
    }
}

// æ­¸ä¸€åŒ–ç‚ºæ¦‚ç‡
P(char2 | char1) = (count + 1) / (rowSum + vocabSize)
```

**å„ªé»**:
- ç°¡å–®ç›´è§€
- ç²¾ç¢º(çµ¦å®šæ•¸æ“š)
- å¿«é€Ÿè¨“ç·´

**ç¼ºé»**:
- éœ€è¦å¹³æ»‘è™•ç†æœªè¦‹éçš„ bigram
- ç„¡æ³•æ¨å»£åˆ°æ›´è¤‡é›œçš„æ¨¡å‹
- åƒæ•¸æ•¸é‡éš¨è©å½™é‡å¹³æ–¹å¢é•·

#### æ–¹æ³• 2: ç¥ç¶“ç¶²è·¯æ³• (Neural Network Approach)

```java
// åˆå§‹åŒ–æ¬Šé‡ W (27x27)
W = Tensor.randn(rng, 27, 27).requiresGrad(true);

// è¨“ç·´å¾ªç’°
for (int iter = 0; iter < numIterations; iter++) {
    // 1. One-hot ç·¨ç¢¼è¼¸å…¥
    Tensor xenc = oneHotEncode(xs, 27);
    
    // 2. å‰å‘å‚³æ’­
    Tensor logits = xenc.matmul(W);           // ç·šæ€§è®Šæ›
    Tensor counts = logits.exp();             // æŒ‡æ•¸åŒ–
    Tensor sumCounts = counts.sum(1, true);   // æ±‚å’Œ
    Tensor probs = counts.div(sumCounts);     // Softmax
    
    // 3. è¨ˆç®—æå¤±
    loss = -log(probs[correct_indices]).mean() + Î»||W||Â²
    
    // 4. åå‘å‚³æ’­
    W.zeroGrad();
    loss.backward();
    
    // 5. æ›´æ–°åƒæ•¸
    W.data -= learningRate * W.grad;
}
```

**ç¥ç¶“ç¶²è·¯æ¶æ§‹**:

```
è¼¸å…¥ (one-hot): [0, 0, 1, 0, ..., 0]  (27 ç¶­)
           â†“
    æ¬Šé‡çŸ©é™£ W (27Ã—27)
           â†“
    Logits: [lâ‚, lâ‚‚, ..., lâ‚‚â‚‡]
           â†“
    Softmax: exp(láµ¢) / Î£exp(lâ±¼)
           â†“
    è¼¸å‡ºæ¦‚ç‡: [pâ‚, pâ‚‚, ..., pâ‚‚â‚‡]
```

**æå¤±å‡½æ•¸**:

```
L = -1/N Î£ log P(yáµ¢ | xáµ¢) + Î»||W||Â²
    ^^^^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^
    äº¤å‰ç†µæå¤±                L2 æ­£å‰‡åŒ–
```

**å„ªé»**:
- å¯æ“´å±•åˆ°æ›´è¤‡é›œçš„æ¶æ§‹
- è‡ªå‹•å­¸ç¿’ç‰¹å¾µè¡¨ç¤º
- æ”¯æŒæ¢¯åº¦ä¸‹é™å„ªåŒ–
- æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ

**ç¼ºé»**:
- éœ€è¦èª¿æ•´è¶…åƒæ•¸
- è¨“ç·´æ™‚é–“è¼ƒé•·
- å¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª

### 3. Main.java - ä¸»ç¨‹å¼

å±•ç¤ºå®Œæ•´çš„å·¥ä½œæµç¨‹:

1. **åŠ è¼‰æ•¸æ“š**
2. **è¨“ç·´è¨ˆæ•¸æ¨¡å‹**
3. **è¨“ç·´ç¥ç¶“ç¶²è·¯æ¨¡å‹**
4. **ç”Ÿæˆæ¨£æœ¬**
5. **æ¯”è¼ƒçµæœ**

## ğŸ”¬ æ•¸å­¸åŸç†

### Softmax å‡½æ•¸

å°‡ä»»æ„å¯¦æ•¸å‘é‡è½‰æ›ç‚ºæ¦‚ç‡åˆ†å¸ƒ:

```
softmax(x)áµ¢ = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)
```

**æ€§è³ª**:
- è¼¸å‡ºç¸½å’Œç‚º 1
- ä¿æŒå–®èª¿æ€§
- å¯å¾®åˆ†

### äº¤å‰ç†µæå¤±

è¡¡é‡é æ¸¬åˆ†å¸ƒèˆ‡çœŸå¯¦åˆ†å¸ƒçš„å·®è·:

```
H(p, q) = -Î£ p(x) log q(x)
```

å°æ–¼å–®å€‹æ¨£æœ¬(çœŸå¯¦æ¨™ç±¤ç‚º y):

```
L = -log P(y | x)
```

### æ¢¯åº¦è¨ˆç®—ç¤ºä¾‹

ä»¥ Softmax + äº¤å‰ç†µç‚ºä¾‹:

```java
// å‰å‘å‚³æ’­
probs = softmax(X @ W)
loss = -log(probs[y])

// åå‘å‚³æ’­(ç°¡åŒ–ç‰ˆ)
âˆ‚L/âˆ‚probs[i] = -1/probs[y]  (if i == y)
             = 0             (otherwise)

âˆ‚L/âˆ‚W = X.T @ âˆ‚L/âˆ‚probs
```

## ğŸ“Š å¯¦é©—çµæœ

### æ€§èƒ½æ¯”è¼ƒ

| æ–¹æ³•        | NLL   | è¨“ç·´æ™‚é–“ | æ¨£æœ¬è³ªé‡ |
|------------|-------|---------|---------|
| è¨ˆæ•¸æ³•      | 2.454 | <1s     | å¥½      |
| ç¥ç¶“ç¶²è·¯    | 2.482 | ~10s    | å¥½      |

å…©ç¨®æ–¹æ³•é”åˆ°ç›¸ä¼¼çš„ NLL,è­‰æ˜ç¥ç¶“ç¶²è·¯æˆåŠŸå­¸ç¿’äº†èˆ‡è¨ˆæ•¸æ³•ç›¸åŒçš„çµ±è¨ˆåˆ†å¸ƒ!

### ç”Ÿæˆæ¨£æœ¬ç¤ºä¾‹

```
è¨ˆæ•¸æ³•:
1. mora
2. axx
3. minaymoryles
4. kondlaisah
5. anchthizarie

ç¥ç¶“ç¶²è·¯:
1. mor
2. axx
3. minaymoryles
4. kondlaisah
5. anchthizarie
```

çµæœéå¸¸ç›¸ä¼¼,å› ç‚ºå…©ç¨®æ–¹æ³•å­¸ç¿’äº†ç›¸åŒçš„ bigram çµ±è¨ˆ!

## ğŸ“ å¾ Bigrams åˆ° Transformers çš„è·¯å¾‘

### Lecture 2 (ç•¶å‰): Bigrams
- Context: åªçœ‹å‰ä¸€å€‹å­—ç¬¦
- æ¨¡å‹: å–®å±¤ç¥ç¶“ç¶²è·¯ (27â†’27)
- åƒæ•¸: 729 å€‹ (27Ã—27)

### Lecture 3: MLP
- Context: çœ‹å‰ N å€‹å­—ç¬¦
- æ¨¡å‹: åµŒå…¥ + å¤šå±¤æ„ŸçŸ¥æ©Ÿ
- æ–°æ¦‚å¿µ: éš±è—å±¤ã€åµŒå…¥ã€æ¿€æ´»å‡½æ•¸

### Lecture 4-5: BatchNorm & Backprop
- å„ªåŒ–è¨“ç·´ç©©å®šæ€§
- ç†è§£æ¢¯åº¦æµå‹•
- æ‰‹å‹•å¯¦ç¾åå‘å‚³æ’­

### Lecture 6: WaveNet
- å±¤æ¬¡çµæ§‹
- æ“´å¼µå·ç©
- æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£

### Lecture 7: GPT
- Self-attention æ©Ÿåˆ¶
- Transformer æ¶æ§‹
- ä½ç½®ç·¨ç¢¼

### Lecture 8: Tokenizer
- Byte Pair Encoding (BPE)
- å­è©åˆ‡åˆ†
- è©å½™è¡¨æ§‹å»º

## ğŸ’» é‹è¡ŒæŒ‡å—

### ä½¿ç”¨ Maven

```bash
# ç·¨è­¯
mvn clean compile

# é‹è¡Œ
mvn exec:java -Dexec.mainClass="com.makemore.Main"

# æ¸¬è©¦
mvn test

# æ‰“åŒ…
mvn package
java -jar target/java-makemore-bigrams-1.0.0.jar
```

### ä¸ä½¿ç”¨ Maven (ç´” Java)

```bash
# ç·¨è­¯
javac -d build/classes src/main/java/com/makemore/bigram/*.java src/main/java/com/makemore/*.java

# é‹è¡Œ
java -cp build/classes com.makemore.Main
```

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: ç‚ºä»€éº¼ NLL ä¸æ˜¯ 0?
A: å› ç‚ºèªè¨€æœ‰å›ºæœ‰çš„ä¸ç¢ºå®šæ€§ã€‚å³ä½¿æ˜¯å®Œç¾çš„æ¨¡å‹ä¹Ÿç„¡æ³•å®Œå…¨é æ¸¬ä¸‹ä¸€å€‹å­—ç¬¦ã€‚

### Q2: ç‚ºä»€éº¼å­¸ç¿’ç‡é€™éº¼é«˜ (50.0)?
A: å› ç‚ºä½¿ç”¨å…¨æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™,æ¢¯åº¦ç©©å®šä¸”æº–ç¢ºã€‚å¯¦éš›æ‡‰ç”¨ä¸­ä½¿ç”¨å°æ‰¹æ¬¡æ™‚æœƒç”¨æ›´å°çš„å­¸ç¿’ç‡ (0.001-0.1)ã€‚

### Q3: æ­£å‰‡åŒ–ç‚ºä»€éº¼é‡è¦?
A: é˜²æ­¢éæ“¬åˆã€‚æ²’æœ‰æ­£å‰‡åŒ–æ™‚,æ¨¡å‹å¯èƒ½åœ¨è¨“ç·´é›†ä¸Šè¡¨ç¾å¾ˆå¥½,ä½†åœ¨æ–°æ•¸æ“šä¸Šè¡¨ç¾å·®ã€‚

### Q4: ç‚ºä»€éº¼éœ€è¦ one-hot ç·¨ç¢¼?
A: å°‡é›¢æ•£çš„å­—ç¬¦ç´¢å¼•è½‰æ›ç‚ºç¥ç¶“ç¶²è·¯å¯ä»¥è™•ç†çš„å‘é‡å½¢å¼ã€‚Lecture 3 æœƒä»‹ç´¹æ›´é«˜æ•ˆçš„åµŒå…¥æ–¹æ³•ã€‚

### Q5: è¨ˆç®—åœ–æ˜¯å¦‚ä½•æ§‹å»ºçš„?
A: æ¯æ¬¡é‹ç®—éƒ½å‰µå»ºæ–°çš„ Tensor,ä¸¦è¨˜éŒ„çˆ¶ç¯€é»(prev)å’Œåå‘å‚³æ’­å‡½æ•¸(backward)ã€‚é€™æ¨£å½¢æˆäº†ä¸€å€‹æœ‰å‘ç„¡ç’°åœ– (DAG)ã€‚


# Building makemore Part 2: MLP

**å®Œæ•´ Java å¯¦ç¾** - Andrej Karpathy's makemore Lecture 3

## ğŸ¯ å°ˆæ¡ˆæ¦‚è¿°

å¯¦ç¾å­—ç¬¦ç´šå¤šå±¤æ„ŸçŸ¥æ©Ÿ (MLP) èªè¨€æ¨¡å‹,å¾å‰ 3 å€‹å­—ç¬¦é æ¸¬ä¸‹ä¸€å€‹å­—ç¬¦ã€‚

### èˆ‡ Lecture 2 (Bigrams) çš„å°æ¯”

| ç‰¹æ€§ | Bigrams | MLP (æœ¬å°ˆæ¡ˆ) |
|------|---------|-------------|
| ä¸Šä¸‹æ–‡ | 1 å€‹å­—ç¬¦ | **3 å€‹å­—ç¬¦** |
| è¼¸å…¥è¡¨ç¤º | One-hot (27ç¶­) | **å­¸ç¿’çš„åµŒå…¥ (10ç¶­)** |
| æ¨¡å‹ | ç·šæ€§ (å–®å±¤) | **éç·šæ€§ (é›™å±¤)** |
| åƒæ•¸é‡ | 729 | **11,897** |
| Loss (NLL) | ~2.48 | **~2.13** â¬‡ï¸ |
| æ¨£æœ¬è³ªé‡ | å¯è¾¨èª | **æ›´åƒçœŸå** âœ¨ |

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ§‹

```
è¼¸å…¥: [châ‚, châ‚‚, châ‚ƒ]  (å‰3å€‹å­—ç¬¦çš„ç´¢å¼•)
         â†“
    Embedding Lookup
    C[châ‚], C[châ‚‚], C[châ‚ƒ]  (3å€‹ 10ç¶­å‘é‡)
         â†“
    Flatten æˆ 30ç¶­
         â†“
    Linear + Tanh
    h = tanh(W1 @ x + b1)  (30 â†’ 200)
         â†“
    Linear
    logits = W2 @ h + b2   (200 â†’ 27)
         â†“
    Softmax
    P(next_char | context)
```

### åƒæ•¸è©³æƒ…

```
C  : 27 Ã— 10   = 270     (å­—ç¬¦åµŒå…¥è¡¨)
W1 : 30 Ã— 200  = 6,000   (ç¬¬ä¸€å±¤æ¬Šé‡)
b1 : 200       = 200     (ç¬¬ä¸€å±¤åç½®)
W2 : 200 Ã— 27  = 5,400   (ç¬¬äºŒå±¤æ¬Šé‡)
b2 : 27        = 27      (ç¬¬äºŒå±¤åç½®)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total         = 11,897 åƒæ•¸
```

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1ï¸âƒ£ å­—ç¬¦åµŒå…¥ (Character Embeddings)

**ç‚ºä»€éº¼éœ€è¦?**
- One-hot å¤ªç¨€ç– (27ç¶­åªæœ‰1å€‹éé›¶)
- ç„¡æ³•è¡¨é”å­—ç¬¦ç›¸ä¼¼æ€§
- åµŒå…¥å­¸ç¿’**å¯†é›†è¡¨ç¤º**

**å¦‚ä½•å·¥ä½œ:**
```java
// è¼¸å…¥ç´¢å¼•: [5, 13, 13] â†’ 'e', 'm', 'm'
Tensor emb = C.index(X);  // æŸ¥è¡¨
// è¼¸å‡º: (3, 10) - ä¸‰å€‹10ç¶­å‘é‡
```

**å­¸åˆ°ä»€éº¼?**
- å…ƒéŸ³ (a,e,i,o,u) èšåœ¨ä¸€èµ·
- å¸¸è¦‹çµ„åˆçš„å­—ç¬¦è·é›¢è¿‘
- æ¯å€‹ç¶­åº¦æ•ç²ä¸åŒç‰¹å¾µ

---

### 2ï¸âƒ£ ä¸Šä¸‹æ–‡çª—å£ (Context Window)

```
Word: "emma"
åŠ ä¸Šèµ·æ­¢ç¬¦: "...emma."

è¨“ç·´æ¨£æœ¬:
[., ., .] â†’ e
[., ., e] â†’ m
[., e, m] â†’ m
[e, m, m] â†’ a
[m, m, a] â†’ .
```

**Block Size = 3** è¡¨ç¤º:
- ç”¨å‰ 3 å€‹å­—ç¬¦é æ¸¬ä¸‹ä¸€å€‹
- æ¯” bigram çš„ 1 å€‹å­—ç¬¦æœ‰**æ›´å¤šä¿¡æ¯**
- å¯å­¸ç¿’æ›´è¤‡é›œçš„æ¨¡å¼ (å¦‚ "qu" å¾Œå¸¸æ¥ "a")

---

### 3ï¸âƒ£ éç·šæ€§æ¿€æ´» (Tanh)

```java
h = tanh(W1 @ x + b1)
```

**ç‚ºä»€éº¼é‡è¦?**
- æ²’æœ‰æ¿€æ´» â†’ å¤šå±¤åªæ˜¯ç·šæ€§è®Šæ›çš„çµ„åˆ = é‚„æ˜¯ç·šæ€§
- **Tanh å¼•å…¥éç·šæ€§** â†’ å¯å­¸è¤‡é›œæ±ºç­–é‚Šç•Œ
- è¼¸å‡ºç¯„åœ: [-1, 1]

**æ¢¯åº¦:**
```
âˆ‚tanh/âˆ‚x = 1 - tanhÂ²(x)
```

---

### 4ï¸âƒ£ Train/Dev/Test åˆ†å‰²

```
228,146 å€‹è¨“ç·´æ¨£æœ¬
â”œâ”€ Train (80%): 182,516  â† å­¸ç¿’åƒæ•¸
â”œâ”€ Dev   (10%):  22,815  â† èª¿è¶…åƒæ•¸
â””â”€ Test  (10%):  22,815  â† æœ€çµ‚è©•ä¼°
```

**ç‚ºä»€éº¼éœ€è¦?**
- **Overfitting**: æ¨¡å‹è¨˜ä½è¨“ç·´æ•¸æ“šä½†ä¸æ³›åŒ–
- **Dev set**: æª¢æ¸¬éæ“¬åˆ,èª¿å­¸ç¿’ç‡
- **Test set**: å ±å‘Šæœ€çµ‚æ€§èƒ½ (çµ•ä¸ç”¨æ–¼èª¿åƒ!)

---

### 5ï¸âƒ£ Mini-Batch æ¢¯åº¦ä¸‹é™

```java
for iter in 1..200000:
    // 1. éš¨æ©ŸæŠ½32å€‹æ¨£æœ¬
    batch = randomSample(data, 32)
    
    // 2. å‰å‘å‚³æ’­
    logits = forward(batch)
    loss = crossEntropy(logits, targets)
    
    // 3. åå‘å‚³æ’­
    loss.backward()
    
    // 4. æ›´æ–°åƒæ•¸
    params -= learningRate * grads
```

**ç‚ºä»€éº¼ç”¨ Mini-Batch?**
- âœ… æ¯”å…¨æ‰¹æ¬¡**å¿«** (ä¸ç”¨è™•ç†æ‰€æœ‰æ•¸æ“š)
- âœ… æ¯”å–®æ¨£æœ¬**ç©©å®š** (æ¢¯åº¦æ–¹å‘å°)
- âœ… **æ­£å‰‡åŒ–æ•ˆæœ** (å™ªéŸ³å¹«åŠ©æ³›åŒ–)

---

## ğŸ”¬ é—œéµå¯¦ç¾ç´°ç¯€

### Gather æ“ä½œ (Cross-Entropy çš„é—œéµ!)

**å•é¡Œ**: å¾ softmax æ¦‚ç‡ä¸­é¸å‡ºç›®æ¨™é¡åˆ¥çš„æ¦‚ç‡,åŒæ™‚**ä¿æŒæ¢¯åº¦é€£æ¥**

```java
// probs: (batch=32, vocab=27)
// targets: (batch=32) å¦‚ [5, 13, 13, 1, 0, ...]

// âŒ éŒ¯èª¤åšæ³• - åˆ‡æ–·æ¢¯åº¦!
double[] selected = new double[32];
for (int i = 0; i < 32; i++) {
    selected[i] = probs.data[i * 27 + targets[i]];
}
Tensor result = new Tensor(selected, ...);  // æ²’é€£åˆ° probs!

// âœ… æ­£ç¢ºåšæ³• - gather æ“ä½œ
Tensor selected = probs.gather(targets);  // ä¿æŒè¨ˆç®—åœ–é€£æ¥!
```

**Gather å¯¦ç¾:**
```java
public Tensor gather(Tensor indices) {
    // Forward: result[i] = this[i, indices[i]]
    for (int i = 0; i < batchSize; i++) {
        result[i] = this.data[i * numClasses + indices[i]];
    }
    
    // Backward: this.grad[i, indices[i]] += out.grad[i]
    out.backward = (v) -> {
        for (int i = 0; i < batchSize; i++) {
            int classIdx = (int) indicesData[i];
            this.grad[i * numClasses + classIdx] += out.grad[i];
        }
    };
}
```

---

### Cross-Entropy Loss

```java
public Tensor crossEntropyLoss(Tensor logits, Tensor targets) {
    // 1. Softmax
    Tensor exp = logits.exp();
    Tensor sumExp = exp.sum(1, true);
    Tensor probs = exp.div(sumExp);
    
    // 2. Gather ç›®æ¨™æ¦‚ç‡ (ä¿æŒæ¢¯åº¦!)
    Tensor selected = probs.gather(targets);
    
    // 3. -log(p).mean()
    Tensor loss = selected.log().neg().mean();
    
    return loss;  // è‡ªå‹•è¿½è¹¤æ•´å€‹è¨ˆç®—åœ–!
}
```

**ç‚ºä»€éº¼é€™æ¨£å¯«?**
- âœ… æ¯ä¸€æ­¥éƒ½æ˜¯ Tensor æ“ä½œ
- âœ… è‡ªå‹•æ§‹å»ºè¨ˆç®—åœ–
- âœ… `loss.backward()` è‡ªå‹•å‚³æ’­æ¢¯åº¦

---

## ğŸ“Š è¨“ç·´éç¨‹

### è¶…åƒæ•¸

```java
blockSize = 3         // ä¸Šä¸‹æ–‡é•·åº¦
embeddingDim = 10     // åµŒå…¥ç¶­åº¦
hiddenSize = 200      // éš±è—å±¤å¤§å°
learningRate = 0.1    // å­¸ç¿’ç‡ (100k å¾Œé™åˆ° 0.01)
batchSize = 32        // æ‰¹æ¬¡å¤§å°
iterations = 200000   // è¨“ç·´æ­¥æ•¸
```

### é æœŸè¨“ç·´æ›²ç·š

```
Iter 0:      loss=3.69, train=3.69, dev=3.69  â† éš¨æ©Ÿåˆå§‹åŒ–
Iter 10000:  loss=2.45, train=2.43, dev=2.48  â† é–‹å§‹å­¸ç¿’
Iter 50000:  loss=2.21, train=2.19, dev=2.24
Iter 100000: loss=2.15, train=2.13, dev=2.17  â† å­¸ç¿’ç‡é™ä½
Iter 200000: loss=2.13, train=2.11, dev=2.15  â† æ”¶æ–‚

Final Test Loss: 2.12
```

### å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥

```java
if (iter == 100000) {
    learningRate = 0.01;  // å¾ 0.1 é™åˆ° 0.01
}
```

**ç‚ºä»€éº¼?**
- å‰æœŸ: å¤§æ­¥å‰é€² (lr=0.1)
- å¾ŒæœŸ: ç²¾ç´°èª¿æ•´ (lr=0.01)

---

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### ç·¨è­¯é‹è¡Œ

```bash
# ä½¿ç”¨ Maven
mvn clean compile exec:java

# æˆ–ç›´æ¥ç”¨ Java
javac src/main/java/com/makemore/**/*.java
java com.makemore.Main
```

### é æœŸè¼¸å‡º

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Makemore Part 2: MLP Language Model     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

=== Loading Data ===
Loaded 32033 words
Vocabulary size: 27

=== Building Dataset ===
Total examples: 228146
Training: 182516, Dev: 22815, Test: 22815

=== Initializing Parameters ===
Total parameters: 11897

=== Training ===
Initial gradient norm: 4.523456  âœ…

Iter 0: loss=3.6892, train=3.6892, dev=3.6889
Iter 10000: loss=2.4521, train=2.4312, dev=2.4798
...
Iter 199999: loss=2.0765, train=2.0532, dev=2.0867

=== Final Evaluation ===
Train: 2.0532, Dev: 2.0867, Test: 2.0891

=== Sampling 20 Names ===
 1. carmahela     â† çœ‹èµ·ä¾†åƒçœŸåå­—!
 2. jhovi
 3. kimrin
 4. halanna
 5. jazhien
 6. amerynci
 7. aqui
 8. nellara
 9. chaiiv
10. kaleigh
```

---

## ğŸ› å¸¸è¦‹å•é¡Œ & Debug

### Q1: Loss ä¸ä¸‹é™ (åœåœ¨ ~24)

**ç—‡ç‹€:**
```
Iter 0: loss=24.96
Iter 10000: loss=24.96  â† å®Œå…¨ä¸è®Š!
```

**åŸå› **: æ¢¯åº¦æ²’æœ‰æµå‹•!

**æª¢æŸ¥:**
```java
// åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å¾Œæ‰“å°
double gradNorm = 0;
for (Tensor p : parameters) {
    for (double g : p.getGrad()) {
        gradNorm += g * g;
    }
}
System.out.println("Grad norm: " + Math.sqrt(gradNorm));
```

å¦‚æœ = 0 â†’ **è¨ˆç®—åœ–æ–·äº†!**

**å¯èƒ½åŸå› :**
- âŒ Cross-entropy æ²’ç”¨ `gather()`
- âŒ æŸå€‹æ“ä½œæ²’è¨­ç½® `backward` å‡½æ•¸
- âŒ æ²’èª¿ç”¨ `requiresGrad(true)`

---

### Q2: ç”Ÿæˆçš„éƒ½æ˜¯ "qqqxbx" ä¹‹é¡çš„åƒåœ¾

**åŸå› **: æ¨¡å‹å®Œå…¨æ²’å­¸åˆ°æ±è¥¿

**è§£æ±º**: è¦‹ Q1

---

### Q3: Loss çˆ†ç‚¸ (è®Šæˆ NaN)

**ç—‡ç‹€:**
```
Iter 100: loss=2.5
Iter 200: loss=15.8
Iter 300: loss=NaN  â† çˆ†äº†!
```

**åŸå› **: å­¸ç¿’ç‡å¤ªå¤§

**è§£æ±º:**
```java
learningRate = 0.01;  // é™ä½å­¸ç¿’ç‡
```

---

### Q4: ç‚ºä»€éº¼ç”¨ Tanh è€Œä¸æ˜¯ ReLU?

**ç­”**:
- Karpathy çš„åŸå§‹å¯¦ç¾ç”¨ tanh
- Tanh è¼¸å‡ºæœ‰ç•Œ [-1, 1] â†’ è¨“ç·´æ›´ç©©å®š
- ReLU ä¹Ÿå¯ä»¥,ä½†éœ€è¦èª¿æ•´åˆå§‹åŒ–

---

### Q5: å¯ä»¥å¢åŠ  hidden size æå‡æ€§èƒ½å—?

**ç­”**: å¯ä»¥!

```java
hiddenSize = 300;  // å¾ 200 å¢åŠ åˆ° 300
```

**é æœŸæ•ˆæœ:**
- âœ… Loss å¯èƒ½é™åˆ° ~2.05
- âš ï¸ è¨“ç·´è®Šæ…¢
- âš ï¸ éæ“¬åˆé¢¨éšªå¢åŠ  (train/dev gap è®Šå¤§)

**å»ºè­°**: å…ˆç”¨ 200 è·‘é€š,å†å˜—è©¦èª¿åƒ

---

## ğŸ“ å­¸åˆ°çš„é—œéµé»

### 1. åµŒå…¥çš„å¨åŠ›

**One-hot (Bigram):**
```
'a' â†’ [1,0,0,...,0]  (27ç¶­,ç¨€ç–)
'e' â†’ [0,0,0,0,1,...]
```
â†’ 'a' å’Œ 'e' æ²’æœ‰ç›¸ä¼¼æ€§

**Embedding (MLP):**
```
'a' â†’ [0.3, -0.1, 0.5, ...]  (10ç¶­,å¯†é›†)
'e' â†’ [0.2, -0.2, 0.4, ...]
```
â†’ ç›¸ä¼¼å­—ç¬¦æœ‰ç›¸ä¼¼å‘é‡!

---

### 2. è¨ˆç®—åœ–çš„å®Œæ•´æ€§

**éŒ¯èª¤çš„æµç¨‹:**
```
logits â†’ probs â†’ [æ‰‹å‹•æå–] â†’ loss
                    â†‘
                  æ¢¯åº¦æ–·äº†!
```

**æ­£ç¢ºçš„æµç¨‹:**
```
logits â†’ probs â†’ gather â†’ log â†’ neg â†’ mean
  â†“       â†“        â†“      â†“     â†“      â†“
æ‰€æœ‰æ“ä½œéƒ½ä¿æŒæ¢¯åº¦é€£æ¥ âœ…
```

---

### 3. ç‚ºä»€éº¼ Loss ç„¡æ³•é™åˆ° 0?

**ç†è«–æœ€å°å€¼:**
```
L = -log(1) = 0  (å®Œç¾é æ¸¬æ¯å€‹å­—ç¬¦)
```

**å¯¦éš›:**
```
L â‰ˆ 2.1
```

**åŸå› :**
1. **å›ºæœ‰ä¸ç¢ºå®šæ€§**: å³ä½¿äººé¡ä¹Ÿç„¡æ³• 100% é æ¸¬
2. **æ¨¡å‹å®¹é‡**: 11k åƒæ•¸å¯èƒ½ä¸å¤ 
3. **ä¸Šä¸‹æ–‡é™åˆ¶**: åªçœ‹ 3 å€‹å­—ç¬¦,å¯èƒ½ä¸å¤ 

**æ”¹é€²æ–¹å‘:**
- å¢å¤§ embedding/hidden size
- å¢åŠ ä¸Šä¸‹æ–‡é•·åº¦ (block_size = 5)
- åŠ æ·±ç¶²çµ¡ (2 å±¤éš±è—å±¤)

---

## ğŸ¯ èˆ‡ Lecture 2 å°æ¯”ç¸½çµ

### Bigram (Lecture 2)
```python
P(next | prev1)
- ç°¡å–®ç›´æ¥
- 729 åƒæ•¸
- NLL â‰ˆ 2.48
```

### MLP (Lecture 3)
```python
P(next | prev3, prev2, prev1)
- éœ€è¦è¨“ç·´
- 11,897 åƒæ•¸
- NLL â‰ˆ 2.13 âœ¨
```

**æ”¹é€²ä¾†è‡ª:**
1. âœ… **æ›´é•·ä¸Šä¸‹æ–‡** (3 vs 1)
2. âœ… **å­¸ç¿’çš„åµŒå…¥** (æ•ç²ç›¸ä¼¼æ€§)
3. âœ… **éç·šæ€§** (Tanh éš±è—å±¤)
4. âœ… **åˆ†ä½ˆå¼è¡¨ç¤º** (200 å€‹éš±è—å–®å…ƒ)

---

# Building makemore Part 3: Activations & Gradients, BatchNorm

**å®Œæ•´ Java å¯¦ç¾** - Andrej Karpathy's makemore Lecture 4

## ğŸ¯ å°ˆæ¡ˆæ¦‚è¿°

å¯¦ç¾**æ·±å±¤ MLP** (5å€‹éš±è—å±¤) ä¸¦å¼•å…¥ **Batch Normalization**,è§£æ±ºæ·±åº¦ç¶²è·¯è¨“ç·´å›°é›£çš„æ ¸å¿ƒå•é¡Œã€‚

é€™æ˜¯ makemore ç³»åˆ—ä¸­æœ€å…·æŒ‘æˆ°æ€§çš„ä¸€èª²,å› ç‚ºå®ƒæ·±å…¥æ¢è¨äº†**ç‚ºä»€éº¼æ·±åº¦å­¸ç¿’éœ€è¦æ­¸ä¸€åŒ–æŠ€è¡“**ã€‚

## ğŸ“Š èˆ‡ Lecture 3 çš„é—œéµå°æ¯”

| ç‰¹æ€§ | Lecture 3 (MLP) | Lecture 4 (BatchNorm) |
|------|-----------------|----------------------|
| éš±è—å±¤æ•¸ | **1 å±¤** | **5 å±¤** (æ·±å±¤ç¶²è·¯) |
| å±¤çµæ§‹ | Linear + Tanh | **Linear + BatchNorm + Tanh** |
| åƒæ•¸é‡ | ~12k | **~47k** |
| è¨“ç·´é›£åº¦ | ç°¡å–®ç›´æ¥ | **éœ€è¦ BatchNorm æ‰èƒ½è¨“ç·´** |
| Train Loss | ~2.11 | **~2.05** (æ›´æ·±æ›´å¥½) |
| Dev Loss | ~2.15 | **~2.39** |
| ä¸»è¦æŒ‘æˆ° | ç†è§£åµŒå…¥å’Œ MLP | **è¨ºæ–·æ¿€æ´»å€¼å’Œæ¢¯åº¦** |

## ğŸ”¬ æ ¸å¿ƒå•é¡Œ: ç‚ºä»€éº¼æ·±å±¤ç¶²è·¯é›£è¨“ç·´?

### æ²’æœ‰ BatchNorm çš„ç½é›£

```
è¨“ç·´çµæœ (å¯¦éš›é‹è¡Œ):
Iter 0:   loss=18.24  â† è¶…ç´šé«˜!
Iter 999: loss=4.75   â† å®Œå…¨æ²’å­¸åˆ°æ±è¥¿

æ¿€æ´»å€¼çµ±è¨ˆ:
Layer 1: mean=-0.02, std=1.00, saturated=98.69%  â† å¹¾ä¹å®Œå…¨é£½å’Œ!
Layer 3: mean=+0.01, std=0.98, saturated=91.81%
Layer 5: mean=+0.03, std=0.98, saturated=89.60%
Layer 7: mean=+0.08, std=0.98, saturated=90.04%
Layer 9: mean=+0.06, std=0.99, saturated=95.60%

ç”Ÿæˆçµæœ: (åƒåœ¾)
```

**å•é¡Œæ ¹æº:**

1. **æ¿€æ´»å€¼é£½å’Œ** - Tanh è¼¸å‡ºå¹¾ä¹éƒ½æ˜¯ Â±1
2. **æ¢¯åº¦æ¶ˆå¤±** - é£½å’Œå€åŸŸæ¢¯åº¦ â‰ˆ 0
3. **è¨“ç·´åœæ»¯** - åƒæ•¸ç„¡æ³•æ›´æ–°

---

### æœ‰ BatchNorm çš„æˆåŠŸ

```
è¨“ç·´çµæœ (å¯¦éš›é‹è¡Œ):
Iter 0:      loss=3.28   â† æ­£å¸¸åˆå§‹å€¼
Iter 10000:  trainâ‰ˆ2.23, devâ‰ˆ2.56
Iter 100000: trainâ‰ˆ2.16, devâ‰ˆ2.39
Iter 199999: trainâ‰ˆ2.03, devâ‰ˆ2.39  â† æˆåŠŸæ”¶æ–‚!

æ¿€æ´»å€¼çµ±è¨ˆ:
Layer 2:  mean=+0.01, std=0.63, saturated=2.93%  âœ… å¥åº·!
Layer 5:  mean=-0.01, std=0.65, saturated=3.27%  âœ…
Layer 8:  mean=-0.02, std=0.67, saturated=2.59%  âœ…
Layer 11: mean=-0.01, std=0.68, saturated=2.18%  âœ…
Layer 14: mean=-0.00, std=0.71, saturated=4.12%  âœ…

ç”Ÿæˆçµæœ:
1. elrio        â† çœ‹èµ·ä¾†åƒçœŸåå­—!
2. anna
3. janni
4. tyla
5. kamiyah
```

**æˆåŠŸåŸå› :** BatchNorm å¼·åˆ¶æ¯å±¤æ¿€æ´»å€¼ä¿æŒå¥åº·åˆ†ä½ˆ!

---

## ğŸ—ï¸ ç¶²è·¯æ¶æ§‹

```
Input: [châ‚, châ‚‚, châ‚ƒ]  (3å€‹å­—ç¬¦ç´¢å¼•)
         â†“
    Embedding (27 â†’ 10)
         â†“
    Flatten (30)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æœ‰ BatchNorm:               â”‚
â”‚                             â”‚
â”‚  Linear(30 â†’ 100, no bias)  â”‚
â”‚  BatchNorm1d(100)           â”‚ â† é—œéµå±¤!
â”‚  Tanh                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é‡è¤‡ 4 æ¬¡:                  â”‚
â”‚  Linear(100 â†’ 100, no bias) â”‚
â”‚  BatchNorm1d(100)           â”‚
â”‚  Tanh                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Linear(100 â†’ 27, no bias)  â”‚
â”‚  BatchNorm1d(27)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Softmax
```

**é—œéµè¨­è¨ˆæ±ºç­–:**

1. **Linear å±¤ä¸ç”¨ bias** - BatchNorm çš„ beta å·²æä¾›åç§»
2. **æ¯å±¤å¾Œéƒ½æ¥ BatchNorm** - ä¿æŒæ¿€æ´»å€¼ç©©å®š
3. **æœ€å¾Œä¸€å±¤ä¹Ÿç”¨ BatchNorm** - è®“åˆå§‹é æ¸¬ä¸é‚£éº¼è‡ªä¿¡

---

## ğŸ”‘ Batch Normalization åŸç†

### æ ¸å¿ƒæ€æƒ³

**åœ¨æ¯ä¸€å±¤å¾Œå¼·åˆ¶æ¨™æº–åŒ–æ¿€æ´»å€¼,è®“è¨“ç·´æ›´ç©©å®šã€‚**

### æ•¸å­¸å…¬å¼

```python
# Training mode (ä½¿ç”¨ batch çµ±è¨ˆ)
mean = x.mean(0)                    # è¨ˆç®— batch å‡å€¼
var = x.var(0)                      # è¨ˆç®— batch æ–¹å·®
x_norm = (x - mean) / sqrt(var + Îµ) # æ¨™æº–åŒ–
out = gamma * x_norm + beta         # å¯å­¸ç¿’çš„ç¸®æ”¾å’Œåç§»

# æ›´æ–° running çµ±è¨ˆ (ç”¨æ–¼æ¨ç†)
running_mean = 0.9 * running_mean + 0.1 * mean
running_var = 0.9 * running_var + 0.1 * var

# Inference mode (ä½¿ç”¨ running çµ±è¨ˆ)
x_norm = (x - running_mean) / sqrt(running_var + Îµ)
out = gamma * x_norm + beta
```

### åƒæ•¸èªªæ˜

- **gamma (scale)**: å¯å­¸ç¿’,åˆå§‹åŒ–ç‚º 1
- **beta (shift)**: å¯å­¸ç¿’,åˆå§‹åŒ–ç‚º 0
- **running_mean**: ä¸å¯å­¸ç¿’,æŒ‡æ•¸ç§»å‹•å¹³å‡
- **running_var**: ä¸å¯å­¸ç¿’,æŒ‡æ•¸ç§»å‹•å¹³å‡
- **epsilon (Îµ)**: æ•¸å€¼ç©©å®šæ€§,é€šå¸¸ 1e-5
- **momentum**: æ›´æ–°é€Ÿåº¦,é€šå¸¸ 0.1

### ç‚ºä»€éº¼æœ‰æ•ˆ?

1. **ç©©å®šæ¿€æ´»åˆ†ä½ˆ** - æ¯å±¤è¼¸å…¥éƒ½æ˜¯ mean=0, std=1
2. **æ¸›å°‘å…§éƒ¨å”è®Šé‡åç§»** - å±¤é–“åˆ†ä½ˆä¸å†æ¼‚ç§»
3. **å…è¨±æ›´å¤§å­¸ç¿’ç‡** - è¨“ç·´æ›´å¿«
4. **æ­£å‰‡åŒ–æ•ˆæœ** - batch çµ±è¨ˆå¼•å…¥å™ªéŸ³

---

## ğŸ’» æ ¸å¿ƒå¯¦ç¾

### 1. BatchNorm1d å±¤

```java
public class BatchNorm1d implements Layer {
    private Tensor gamma;  // Scale (learnable)
    private Tensor beta;   // Shift (learnable)
    private double[] runningMean;  // Buffer
    private double[] runningVar;   // Buffer
    
    public Tensor forward(Tensor x) {
        if (training) {
            // ä½¿ç”¨ batch çµ±è¨ˆ
            Tensor mean = x.mean(0);
            Tensor variance = x.variance(0);
            Tensor xNorm = (x - mean) / sqrt(variance + eps);
            
            // æ›´æ–° running çµ±è¨ˆ
            runningMean = 0.9 * runningMean + 0.1 * mean;
            runningVar = 0.9 * runningVar + 0.1 * variance;
            
            return gamma * xNorm + beta;
        } else {
            // ä½¿ç”¨ running çµ±è¨ˆ
            Tensor xNorm = (x - runningMean) / sqrt(runningVar + eps);
            return gamma * xNorm + beta;
        }
    }
}
```

### 2. Tensor æ–°å¢æ“ä½œ

BatchNorm éœ€è¦é€™äº›æ–°çš„å¼µé‡æ“ä½œ:

```java
// æ²¿ batch ç¶­åº¦æ±‚å‡å€¼
Tensor mean(int dim)         // (batch, features) â†’ (features,)

// æ²¿ batch ç¶­åº¦æ±‚æ–¹å·®
Tensor variance(int dim)     // (batch, features) â†’ (features,)

// æ¸›æ³• (æ”¯æŒå»£æ’­)
Tensor subtract(Tensor)      // (batch, features) - (features,)

// å¹³æ–¹æ ¹
Tensor sqrt()                // element-wise sqrt

// ä¹˜æ³• (æ”¯æŒå»£æ’­)
Tensor mul(Tensor)           // (batch, features) * (features,)

// é™¤æ³• (æ”¯æŒå»£æ’­) - é—œéµä¿®å¾©!
Tensor div(Tensor)           // (batch, features) / (features,)
```

**é‡è¦:** `div()` æ–¹æ³•éœ€è¦æ”¯æŒ `(batch, features) / (features,)` çš„å»£æ’­,é€™æ˜¯ BatchNorm çš„æ ¸å¿ƒéœ€æ±‚!

### 3. Layer ä»‹é¢ (PyTorch é¢¨æ ¼)

```java
public interface Layer {
    Tensor forward(Tensor x);
    List parameters();
    void setTraining(boolean training);  // åˆ‡æ›è¨“ç·´/æ¨ç†æ¨¡å¼
    Tensor getOutput();                  // ç”¨æ–¼è¨ºæ–·
}
```

### 4. æ·±å±¤ MLP æ§‹å»º

```java
List layers = Arrays.asList(
    new Linear(30, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 27, false, rng), new BatchNorm1d(27)
);

// åˆå§‹åŒ–æœ€å¾Œä¸€å±¤ (è®“åˆå§‹é æ¸¬ä¸é‚£éº¼è‡ªä¿¡)
lastBatchNorm.getGamma() *= 0.1;
```

---

## ğŸ“ˆ è¨“ç·´éç¨‹èˆ‡çµæœ

### å¯¦é©—è¨­è¨ˆ

æˆ‘å€‘é€²è¡Œäº†**å…©å€‹å°ç…§å¯¦é©—**:

1. **å¯¦é©— 1: æ²’æœ‰ BatchNorm** (é æœŸå¤±æ•—)
2. **å¯¦é©— 2: æœ‰ BatchNorm** (é æœŸæˆåŠŸ)

### è¶…åƒæ•¸

```java
vocabSize = 27
blockSize = 3
embeddingDim = 10
hiddenSize = 100
numHiddenLayers = 5

batchSize = 32
learningRate = 0.1 (å‰ 150k æ¬¡)
              0.01 (å¾Œ 50k æ¬¡)
maxIterations = 200000
```

### å¯¦é©— 1 çµæœ: WITHOUT BatchNorm âŒ

```
è¨“ç·´æ›²ç·š:
Iter 0:   loss=18.24, trainâ‰ˆ17.07, devâ‰ˆ17.15  â† éš¨æ©ŸçŒœæ¸¬
Iter 999: loss=4.75,  trainâ‰ˆ3.96,  devâ‰ˆ4.07   â† æ²’æœ‰æ”¹å–„!

æ¿€æ´»å€¼è¨ºæ–·:
Layer 1: saturated=98.69%  â† ç½é›£!
Layer 3: saturated=91.81%
Layer 5: saturated=89.60%
Layer 7: saturated=90.04%
Layer 9: saturated=95.60%

çµè«–: æ·±å±¤ç¶²è·¯å®Œå…¨ç„¡æ³•è¨“ç·´! âŒ
```

**ç‚ºä»€éº¼å¤±æ•—?**

- æ¿€æ´»å€¼å¹¾ä¹å…¨éƒ¨é£½å’Œ (>90%)
- Tanh åœ¨é£½å’Œå€åŸŸæ¢¯åº¦ â‰ˆ 0
- æ¢¯åº¦ç„¡æ³•åå‘å‚³æ’­
- åƒæ•¸ç„¡æ³•æ›´æ–°

---

### å¯¦é©— 2 çµæœ: WITH BatchNorm âœ…

```
è¨“ç·´æ›²ç·š:
Iter 0:      loss=3.28,  trainâ‰ˆ3.36,  devâ‰ˆ3.34   â† æ­£å¸¸åˆå§‹åŒ–
Iter 10000:  loss=2.37,  trainâ‰ˆ2.23,  devâ‰ˆ2.56   â† å¿«é€Ÿä¸‹é™
Iter 100000: loss=2.24,  trainâ‰ˆ2.16,  devâ‰ˆ2.39   â† æŒçºŒæ”¹å–„
Iter 150000: å­¸ç¿’ç‡é™è‡³ 0.01                      â† LR decay
Iter 199999: loss=2.13,  trainâ‰ˆ2.03,  devâ‰ˆ2.39   â† æ”¶æ–‚!

æ¿€æ´»å€¼è¨ºæ–·:
Layer 2:  mean=+0.01, std=0.63, saturated=2.93%  âœ… å¥åº·!
Layer 5:  mean=-0.01, std=0.65, saturated=3.27%  âœ…
Layer 8:  mean=-0.02, std=0.67, saturated=2.59%  âœ…
Layer 11: mean=-0.01, std=0.68, saturated=2.18%  âœ…
Layer 14: mean=-0.00, std=0.71, saturated=4.12%  âœ…

æœ€çµ‚è©•ä¼°:
Train loss: 2.05  â† æ¯” Lecture 3 (2.11) æ›´å¥½!
Dev loss:   2.39
Test loss:  2.46

ç”Ÿæˆæ¨£æœ¬ (è³ªé‡å¾ˆå¥½!):
1. elrio          11. lytka
2. davdanamaria   12. paileah
3. janni          13. caiya
4. raley          14. tyla
5. anna           15. keadiaup
6. ridsing        16. mykentleigh
7. man            17. graycensley
8. dedi           18. amarelde
9. jeelee         19. kamiyah
10. janiella      20. suthenishia

çµè«–: BatchNorm è®“æ·±å±¤ç¶²è·¯æˆåŠŸè¨“ç·´! âœ…
```

**æˆåŠŸé—œéµ:**

- æ¿€æ´»å€¼é£½å’Œç‡ < 5% (å¥åº·ç¯„åœ)
- æ¯å±¤ mean â‰ˆ 0, std â‰ˆ 0.6-0.7
- æ¢¯åº¦é †åˆ©åå‘å‚³æ’­
- Loss ç©©å®šä¸‹é™

---

## ğŸ“Š æ€§èƒ½å°æ¯”ç¸½çµ

| Model | æ·±åº¦ | Loss (Train) | Loss (Dev) | æ¿€æ´»é£½å’Œç‡ | è¨“ç·´ç‹€æ…‹ |
|-------|------|-------------|-----------|-----------|---------|
| Lecture 3 (1å±¤) | æ·º | 2.11 | 2.15 | ~5% | âœ… æˆåŠŸ |
| Lecture 4 ç„¡BN (5å±¤) | æ·± | 3.96 | 4.07 | **95%** | âŒ å¤±æ•— |
| Lecture 4 æœ‰BN (5å±¤) | æ·± | **2.05** | 2.39 | **3%** | âœ… æˆåŠŸ |

**é—œéµç™¼ç¾:**

1. æ·±å±¤ç¶²è·¯ **æ²’æœ‰ BatchNorm** â†’ è¨“ç·´å¤±æ•—
2. æ·±å±¤ç¶²è·¯ **æœ‰ BatchNorm** â†’ è¨“ç·´æˆåŠŸ,æ€§èƒ½æ›´å¥½
3. BatchNorm æ˜¯è¨“ç·´æ·±åº¦ç¶²è·¯çš„**é—œéµæŠ€è¡“**

---

## ğŸ” è¨ºæ–·å·¥å…·

### æ¿€æ´»å€¼çµ±è¨ˆåˆ†æ

```java
DiagnosticTools.analyzeActivations(layers, "After training");

// è¼¸å‡º:
// Layer 2 (TanhLayer): mean=+0.01, std=0.63, saturated=2.93%
```

**å¥åº·æ¨™æº–:**

- âœ… mean â‰ˆ 0 (ä¸­å¿ƒåŒ–)
- âœ… std â‰ˆ 0.6-0.7 (é©ä¸­æ–¹å·®)
- âœ… saturation < 5% (å¾ˆå°‘é£½å’Œ)

**ä¸å¥åº·æ¨™æº–:**

- âŒ mean é›¢ 0 å¾ˆé  (åç§»)
- âŒ std å¤ªå°æˆ–å¤ªå¤§ (æ–¹å·®ç•°å¸¸)
- âŒ saturation > 90% (å¹¾ä¹å…¨é£½å’Œ)

### é£½å’Œåº¦è¨ˆç®—

```java
// Tanh é£½å’Œå®šç¾©: |tanh(x)| > 0.99
saturated = count(|activation| > 0.99) / total
```

---

## ğŸ’¡ é—œéµæ´å¯Ÿ

### 1. ç‚ºä»€éº¼ Linear å±¤ä¸ç”¨ bias?

```java
// æ²’æœ‰ BatchNorm:
Linear(x) = W @ x + b  â† éœ€è¦ bias

// æœ‰ BatchNorm:
Linear(x) = W @ x             â† ä¸éœ€è¦ bias!
BN(x) = gamma * normalize(x) + beta  â† beta æä¾›åç§»

// çµè«–: BatchNorm çš„ beta å·²ç¶“æä¾›äº†åç§»åŠŸèƒ½
// æ·»åŠ  bias æ˜¯å¤šé¤˜çš„
```

### 2. è¨“ç·´ vs æ¨ç†çš„å·®ç•°

```
è¨“ç·´æ¨¡å¼ (training=True):
  - ä½¿ç”¨ç•¶å‰ batch çµ±è¨ˆ (mean, var)
  - æ›´æ–° running çµ±è¨ˆ
  - å¼•å…¥æ‰¹æ¬¡é–“çš„å™ªéŸ³ (æ­£å‰‡åŒ–æ•ˆæœ)

æ¨ç†æ¨¡å¼ (training=False):
  - ä½¿ç”¨ running çµ±è¨ˆ
  - æ¯å€‹æ¨£æœ¬ç¨ç«‹è™•ç†
  - çµæœç©©å®šå¯é‡ç¾
```

**ç‚ºä»€éº¼éœ€è¦ running çµ±è¨ˆ?**

- æ¨ç†æ™‚å¯èƒ½åªæœ‰ 1 å€‹æ¨£æœ¬,ç„¡æ³•è¨ˆç®— batch çµ±è¨ˆ
- Running çµ±è¨ˆä»£è¡¨æ•´å€‹è¨“ç·´é›†çš„åˆ†ä½ˆ
- é€šéæŒ‡æ•¸ç§»å‹•å¹³å‡å¹³æ»‘æ›´æ–°

### 3. BatchNorm çš„å‰¯ä½œç”¨

**å„ªé»:**

- âœ… ç©©å®šè¨“ç·´æ·±å±¤ç¶²è·¯
- âœ… å…è¨±æ›´å¤§å­¸ç¿’ç‡
- âœ… æ¸›å°‘å°åˆå§‹åŒ–çš„ä¾è³´
- âœ… éš±å«çš„æ­£å‰‡åŒ–æ•ˆæœ

**ç¼ºé»:**

- âŒ è¨“ç·´/æ¨ç†ä¸ä¸€è‡´
- âŒ å° batch size æ•æ„Ÿ (å° batch ä¸ç©©å®š)
- âŒ åœ¨ RNN ä¸­é›£ä»¥æ‡‰ç”¨
- âŒ ä»£ç¢¼è¤‡é›œ,å®¹æ˜“å‡º bug
- âŒ è€¦åˆ batch ä¸­çš„æ¨£æœ¬ (ç ´å£ç¨ç«‹æ€§)

### 4. å…¶ä»–æ­¸ä¸€åŒ–æ–¹æ³•

```
BatchNorm   - æ²¿ batch ç¶­åº¦æ­¸ä¸€åŒ– (æœ¬èª²é‡é»)
LayerNorm   - æ²¿ feature ç¶­åº¦æ­¸ä¸€åŒ– (Transformer ç”¨)
GroupNorm   - åˆ†çµ„æ­¸ä¸€åŒ– (å° batch å‹å¥½)
InstanceNorm - å–®æ¨£æœ¬æ­¸ä¸€åŒ– (é¢¨æ ¼é·ç§»)
```

---

## ğŸ› å¸¸è¦‹å•é¡Œèˆ‡èª¿è©¦

### Q1: `div()` æ–¹æ³•å ±éŒ¯ "Incompatible shapes"

**ç—‡ç‹€:**
```
Error: Incompatible shapes for division
BatchNorm forward å¤±æ•—
```

**åŸå› :** ä½ çš„ `Tensor.div()` ä¸æ”¯æŒ `(batch, features) / (features,)` çš„å»£æ’­

**è§£æ±º:**
```java
// éœ€è¦åœ¨ div() ä¸­æ·»åŠ é€™å€‹ case:
if (shape.length == 2 && other.shape.length == 1 && shape[1] == other.shape[0]) {
    // Broadcasting: (batch, features) / (features,)
    // å¯¦ç¾å»£æ’­é™¤æ³•...
}
```

åƒè€ƒ `TENSOR_div_FIXED.java` ä¸­çš„å®Œæ•´å¯¦ç¾ã€‚

---

### Q2: OutOfMemoryError (Java heap space)

**ç—‡ç‹€:**
```
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
```

**åŸå› :**

1. æ¯æ¬¡è¿­ä»£è©•ä¼°æ•´å€‹è¨“ç·´é›† (182k æ¨£æœ¬)
2. è¨ˆç®—åœ–ä¸æ–·ç´¯ç©
3. ä¸­é–“å¼µé‡æ²’é‡‹æ”¾

**è§£æ±º:**

1. **å¢åŠ å †è¨˜æ†¶é«”:**
   ```
   VM options: -Xms1g -Xmx2g
   ```

2. **ä½¿ç”¨æŠ½æ¨£è©•ä¼°:**
   ```java
   // ä¸è¦è©•ä¼°æ•´å€‹é›†åˆ
   double loss = evaluate(model, allData, allLabels);  // âŒ
   
   // åªè©•ä¼° 500 å€‹æ¨£æœ¬
   double loss = evaluateSample(model, data, labels, 500, rng);  // âœ…
   ```

3. **æ¸›å°‘è©•ä¼°é »ç‡:**
   ```java
   // æ¯ 10000 æ¬¡è¿­ä»£æ‰è©•ä¼°
   if (iter % 10000 == 0) {
       evaluate(...);
   }
   ```

4. **å®šæœŸ GC:**
   ```java
   if (iter % 1000 == 0) {
       System.gc();
   }
   ```

---

### Q3: Loss ä¸ä¸‹é™ (WITH BatchNorm ä¹Ÿä¸è¡Œ)

**å¯èƒ½åŸå› :**

1. **å­¸ç¿’ç‡å¤ªå¤§æˆ–å¤ªå°**
   ```java
   lr = 0.1;  // è©¦è©¦ 0.01 æˆ– 0.5
   ```

2. **åˆå§‹åŒ–å•é¡Œ**
   ```java
   // æœ€å¾Œä¸€å±¤æ‡‰è©²ä¹˜ä»¥å°æ•¸
   lastBatchNorm.getGamma() *= 0.1;  // å¾ˆé‡è¦!
   ```

3. **æ¢¯åº¦æ²’æœ‰æµå‹•**
   ```java
   // æª¢æŸ¥æ¢¯åº¦
   double gradNorm = calculateGradNorm(parameters);
   System.out.println("Grad norm: " + gradNorm);
   // æœŸæœ› > 0
   ```

---

### Q4: æ¿€æ´»å€¼é‚„æ˜¯é£½å’Œ (å³ä½¿ç”¨äº† BatchNorm)

**æª¢æŸ¥æ¸…å–®:**

1. **ç¢ºèª BatchNorm åœ¨è¨“ç·´æ¨¡å¼**
   ```java
   model.setTrainMode();  // å¿…é ˆ!
   ```

2. **ç¢ºèªå±¤çš„é †åº**
   ```java
   // æ­£ç¢º:
   Linear â†’ BatchNorm â†’ Tanh
   
   // éŒ¯èª¤:
   Linear â†’ Tanh â†’ BatchNorm  // BatchNorm æ”¾éŒ¯ä½ç½®!
   ```

3. **æª¢æŸ¥ forward æ˜¯å¦èª¿ç”¨äº† BatchNorm**
   ```java
   for (Layer layer : layers) {
       x = layer.forward(x);  // ç¢ºä¿æ¯å±¤éƒ½è¢«èª¿ç”¨
   }
   ```

---

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
makemore-batchnorm/
â”œâ”€â”€ src/main/java/com/makemore/
â”‚   â”œâ”€â”€ Main.java                    # ä¸»ç¨‹å¼ (é›™é‡å¯¦é©—)
â”‚   â”œâ”€â”€ DeepMLP.java                 # æ·±å±¤ MLP æ¨¡å‹
â”‚   â”œâ”€â”€ DataLoader.java              # æ•¸æ“šåŠ è¼‰å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/                      # å±¤å¯¦ç¾
â”‚   â”‚   â”œâ”€â”€ Layer.java               # å±¤ä»‹é¢
â”‚   â”‚   â”œâ”€â”€ Linear.java              # å…¨é€£æ¥å±¤
â”‚   â”‚   â”œâ”€â”€ BatchNorm1d.java         # â­ BatchNorm å±¤
â”‚   â”‚   â””â”€â”€ TanhLayer.java           # Tanh æ¿€æ´»å±¤
â”‚   â”‚
â”‚   â”œâ”€â”€ mlp/
â”‚   â”‚   â””â”€â”€ Tensor.java              # å¼µé‡ (éœ€è¦æ–°å¢æ“ä½œ)
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ DiagnosticTools.java     # è¨ºæ–·å·¥å…·
â”‚
â”œâ”€â”€ names.txt                        # è¨“ç·´æ•¸æ“š
â”œâ”€â”€ pom.xml                          # Maven é…ç½®
â””â”€â”€ README.md                        # æœ¬æ–‡ä»¶
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### ç·¨è­¯é‹è¡Œ

```bash
# ä½¿ç”¨ Maven
mvn clean compile exec:java

# VM options (æ¨è–¦)
-Xms1g -Xmx2g

# é æœŸé‹è¡Œæ™‚é–“
å¯¦é©— 1 (ç„¡ BatchNorm): ~1 åˆ†é˜
å¯¦é©— 2 (æœ‰ BatchNorm): ~30-60 åˆ†é˜ (200k iterations)
```

### å¿«é€Ÿæ¸¬è©¦ (æ¸›å°‘è¿­ä»£æ¬¡æ•¸)

å¦‚æœæƒ³å¿«é€Ÿçœ‹åˆ°æ•ˆæœ,å¯ä»¥ä¿®æ”¹ `Main.java`:

```java
// å¾ 200000 æ”¹æˆ 50000
trainModel(modelWithBN, dataLoader, 50000, 0.1, 32, true);
```

é æœŸçµæœ:
```
50k æ¬¡:  loss â‰ˆ 2.3-2.4  (5-10 åˆ†é˜)
200k æ¬¡: loss â‰ˆ 2.0-2.1  (30-60 åˆ†é˜)
```

---

## ğŸ“š å­¸ç¿’è·¯å¾‘

```
âœ… Lecture 1: Micrograd (scalar autograd)
âœ… Lecture 2: Bigrams (simple language model)  
âœ… Lecture 3: MLP (embeddings + hidden layers)
âœ… Lecture 4: BatchNorm (deep networks + diagnostics)  â† ä½ åœ¨é€™è£¡
â¬œ Lecture 5: Manual Backprop (gradient ninja)
â¬œ Lecture 6: WaveNet (convolutional architecture)
â¬œ Lecture 7: GPT (transformer)
â¬œ Lecture 8: Tokenizer (BPE)
```

---

## ğŸ“ æ ¸å¿ƒæ”¶ç©«

### 1. æ·±åº¦å­¸ç¿’çš„æ­·å²é›£é¡Œ

**2015 å¹´ä¹‹å‰:**
- è¨“ç·´æ·±å±¤ç¶²è·¯å¹¾ä¹ä¸å¯èƒ½
- éœ€è¦æ¥µå…¶å°å¿ƒçš„åˆå§‹åŒ–
- æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸æ˜¯å¸¸æ…‹
- ç¶²è·¯æ·±åº¦ < 10 å±¤

**BatchNorm (2015) ä¹‹å¾Œ:**
- å¯ä»¥è¨“ç·´ 50-100 å±¤ç¶²è·¯
- å°åˆå§‹åŒ–ä¸é‚£éº¼æ•æ„Ÿ
- è¨“ç·´ç©©å®šå¯é 
- é–‹å•Ÿäº†æ·±åº¦å­¸ç¿’çš„é»ƒé‡‘æ™‚ä»£

### 2. ç¾ä»£æ·±åº¦ç¶²è·¯çš„æ¨™æº–æ¨¡å¼

```
Input
  â†“
[Linear â†’ Normalization â†’ Activation] Ã— N
  â†“
Output

é€™å€‹æ¨¡å¼è²«ç©¿ç¾ä»£æ‰€æœ‰æ¶æ§‹:
- ResNet, VGG (åœ–åƒ)
- Transformer, BERT (èªè¨€)
- WaveNet (éŸ³é »)
```

### 3. è¨ºæ–·æ€ç¶­

**ä¸åªæ˜¯è¨“ç·´æ¨¡å‹,æ›´è¦è¨ºæ–·æ¨¡å‹:**

1. ç›£æ§æ¿€æ´»å€¼çµ±è¨ˆ (mean, std, saturation)
2. ç›£æ§æ¢¯åº¦æµå‹• (grad norm, update ratio)
3. å¯è¦–åŒ–æ¯å±¤çš„è¡Œç‚º
4. ç†è§£æ¨¡å‹ç‚ºä»€éº¼æˆåŠŸ/å¤±æ•—

é€™ç¨®**è¨ºæ–·æ€ç¶­**æ˜¯æˆç‚ºæ·±åº¦å­¸ç¿’å°ˆå®¶çš„é—œéµ!

---
