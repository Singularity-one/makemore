# Makemore Bigrams - å®Œæ•´å¯¦ç¾æŒ‡å—

## ğŸ“¦ å°ˆæ¡ˆçµæ§‹

```
makemore-java/
â”œâ”€â”€ pom.xml                                    # Maven æ§‹å»ºé…ç½®
â”œâ”€â”€ names.txt                                  # è¨“ç·´æ•¸æ“š (å§“ååˆ—è¡¨)
â”œâ”€â”€ README.md                                  # å®Œæ•´æ–‡æª”
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/java/com/makemore/
â”‚   â”‚   â”œâ”€â”€ Main.java                         # ä¸»ç¨‹å¼å…¥å£
â”‚   â”‚   â””â”€â”€ bigram/
â”‚   â”‚       â”œâ”€â”€ Tensor.java                   # æ”¯æŒè‡ªå‹•å¾®åˆ†çš„å¼µé‡é¡
â”‚   â”‚       â””â”€â”€ BigramLanguageModel.java      # Bigram èªè¨€æ¨¡å‹
â”‚   â””â”€â”€ test/java/com/makemore/bigram/
â”‚       â””â”€â”€ TensorTest.java                   # å–®å…ƒæ¸¬è©¦
```

## ğŸ¯ æ ¸å¿ƒçµ„ä»¶è©³è§£

### 1. Tensor.java - è‡ªå‹•å¾®åˆ†å¼•æ“

é€™æ˜¯æ•´å€‹é …ç›®çš„åŸºç¤,å¯¦ç¾äº†é¡ä¼¼ PyTorch çš„ Tensor å’Œè‡ªå‹•å¾®åˆ†ã€‚

#### é—œéµç‰¹æ€§:

1. **æ•¸æ“šå­˜å„²**: ä½¿ç”¨ä¸€ç¶­ `double[]` å­˜å„²å¤šç¶­æ•¸æ“š
2. **å½¢ç‹€è¿½è¹¤**: `int[] shape` è¨˜éŒ„å¼µé‡ç¶­åº¦
3. **æ¢¯åº¦ç´¯ç©**: `double[] grad` å­˜å„²æ¢¯åº¦
4. **è¨ˆç®—åœ–**: ä½¿ç”¨ `Set<Tensor> prev` å’Œ `backward` å‡½æ•¸æ§‹å»ºè¨ˆç®—åœ–

#### å¯¦ç¾çš„é‹ç®—:

```java
// å‰µå»ºå¼µé‡
Tensor t = Tensor.zeros(3, 4);           // 3x4 é›¶çŸ©é™£
Tensor t = Tensor.randn(27, 27);         // 27x27 éš¨æ©ŸçŸ©é™£ (é«˜æ–¯åˆ†å¸ƒ)

// çŸ©é™£é‹ç®—
Tensor c = a.matmul(b);                  // çŸ©é™£ä¹˜æ³•
Tensor s = t.sum(1, true);               // æ²¿ç¶­åº¦æ±‚å’Œ

// å…ƒç´ é‹ç®—
Tensor exp = t.exp();                    // e^x
Tensor log = t.log();                    // ln(x)
Tensor div = a.div(b);                   // a / b (æ”¯æŒå»£æ’­)

// åå‘å‚³æ’­
loss.backward();                         // è‡ªå‹•è¨ˆç®—æ‰€æœ‰æ¢¯åº¦
```

#### è‡ªå‹•å¾®åˆ†å·¥ä½œåŸç†:

```java
// 1. å‰å‘å‚³æ’­æ™‚æ§‹å»ºè¨ˆç®—åœ–
Tensor a = ...;
Tensor b = ...;
Tensor c = a.matmul(b);  // c è¨˜ä½äº† a å’Œ b

// 2. æ¯å€‹é‹ç®—éƒ½å®šç¾©äº† backward å‡½æ•¸
out.backward = (v) -> {
    // è¨ˆç®—æ¢¯åº¦ä¸¦ç´¯ç©åˆ° this.grad å’Œ other.grad
    this.grad[i] += ...;
    other.grad[j] += ...;
    return null;
};

// 3. åå‘å‚³æ’­æ™‚æŒ‰æ‹“æ’²é †åºåŸ·è¡Œ
c.backward();  // è‡ªå‹•èª¿ç”¨æ‰€æœ‰ backward å‡½æ•¸
```

### 2. BigramLanguageModel.java - èªè¨€æ¨¡å‹

å¯¦ç¾äº†å…©ç¨®è¨“ç·´æ–¹æ³•:

#### æ–¹æ³• 1: è¨ˆæ•¸æ³• (Counting Approach)

```java
// çµ±è¨ˆ bigram é »ç‡
for (String word : words) {
    String extWord = "." + word + ".";  // æ·»åŠ èµ·æ­¢ç¬¦
    for (int i = 0; i < extWord.length() - 1; i++) {
        char ch1 = extWord.charAt(i);
        char ch2 = extWord.charAt(i + 1);
        bigramCounts[idx1][idx2]++;      // è¨ˆæ•¸
    }
}

// æ­¸ä¸€åŒ–ç‚ºæ¦‚ç‡
P(char2 | char1) = (count + 1) / (rowSum + vocabSize)
```

**å„ªé»**:
- ç°¡å–®ç›´è§€
- ç²¾ç¢º(çµ¦å®šæ•¸æ“š)
- å¿«é€Ÿè¨“ç·´

**ç¼ºé»**:
- éœ€è¦å¹³æ»‘è™•ç†æœªè¦‹éçš„ bigram
- ç„¡æ³•æ¨å»£åˆ°æ›´è¤‡é›œçš„æ¨¡å‹
- åƒæ•¸æ•¸é‡éš¨è©å½™é‡å¹³æ–¹å¢é•·

#### æ–¹æ³• 2: ç¥ç¶“ç¶²è·¯æ³• (Neural Network Approach)

```java
// åˆå§‹åŒ–æ¬Šé‡ W (27x27)
W = Tensor.randn(rng, 27, 27).requiresGrad(true);

// è¨“ç·´å¾ªç’°
for (int iter = 0; iter < numIterations; iter++) {
    // 1. One-hot ç·¨ç¢¼è¼¸å…¥
    Tensor xenc = oneHotEncode(xs, 27);
    
    // 2. å‰å‘å‚³æ’­
    Tensor logits = xenc.matmul(W);           // ç·šæ€§è®Šæ›
    Tensor counts = logits.exp();             // æŒ‡æ•¸åŒ–
    Tensor sumCounts = counts.sum(1, true);   // æ±‚å’Œ
    Tensor probs = counts.div(sumCounts);     // Softmax
    
    // 3. è¨ˆç®—æå¤±
    loss = -log(probs[correct_indices]).mean() + Î»||W||Â²
    
    // 4. åå‘å‚³æ’­
    W.zeroGrad();
    loss.backward();
    
    // 5. æ›´æ–°åƒæ•¸
    W.data -= learningRate * W.grad;
}
```

**ç¥ç¶“ç¶²è·¯æ¶æ§‹**:

```
è¼¸å…¥ (one-hot): [0, 0, 1, 0, ..., 0]  (27 ç¶­)
           â†“
    æ¬Šé‡çŸ©é™£ W (27Ã—27)
           â†“
    Logits: [lâ‚, lâ‚‚, ..., lâ‚‚â‚‡]
           â†“
    Softmax: exp(láµ¢) / Î£exp(lâ±¼)
           â†“
    è¼¸å‡ºæ¦‚ç‡: [pâ‚, pâ‚‚, ..., pâ‚‚â‚‡]
```

**æå¤±å‡½æ•¸**:

```
L = -1/N Î£ log P(yáµ¢ | xáµ¢) + Î»||W||Â²
    ^^^^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^
    äº¤å‰ç†µæå¤±                L2 æ­£å‰‡åŒ–
```

**å„ªé»**:
- å¯æ“´å±•åˆ°æ›´è¤‡é›œçš„æ¶æ§‹
- è‡ªå‹•å­¸ç¿’ç‰¹å¾µè¡¨ç¤º
- æ”¯æŒæ¢¯åº¦ä¸‹é™å„ªåŒ–
- æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ

**ç¼ºé»**:
- éœ€è¦èª¿æ•´è¶…åƒæ•¸
- è¨“ç·´æ™‚é–“è¼ƒé•·
- å¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª

### 3. Main.java - ä¸»ç¨‹å¼

å±•ç¤ºå®Œæ•´çš„å·¥ä½œæµç¨‹:

1. **åŠ è¼‰æ•¸æ“š**
2. **è¨“ç·´è¨ˆæ•¸æ¨¡å‹**
3. **è¨“ç·´ç¥ç¶“ç¶²è·¯æ¨¡å‹**
4. **ç”Ÿæˆæ¨£æœ¬**
5. **æ¯”è¼ƒçµæœ**

## ğŸ”¬ æ•¸å­¸åŸç†

### Softmax å‡½æ•¸

å°‡ä»»æ„å¯¦æ•¸å‘é‡è½‰æ›ç‚ºæ¦‚ç‡åˆ†å¸ƒ:

```
softmax(x)áµ¢ = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)
```

**æ€§è³ª**:
- è¼¸å‡ºç¸½å’Œç‚º 1
- ä¿æŒå–®èª¿æ€§
- å¯å¾®åˆ†

### äº¤å‰ç†µæå¤±

è¡¡é‡é æ¸¬åˆ†å¸ƒèˆ‡çœŸå¯¦åˆ†å¸ƒçš„å·®è·:

```
H(p, q) = -Î£ p(x) log q(x)
```

å°æ–¼å–®å€‹æ¨£æœ¬(çœŸå¯¦æ¨™ç±¤ç‚º y):

```
L = -log P(y | x)
```

### æ¢¯åº¦è¨ˆç®—ç¤ºä¾‹

ä»¥ Softmax + äº¤å‰ç†µç‚ºä¾‹:

```java
// å‰å‘å‚³æ’­
probs = softmax(X @ W)
loss = -log(probs[y])

// åå‘å‚³æ’­(ç°¡åŒ–ç‰ˆ)
âˆ‚L/âˆ‚probs[i] = -1/probs[y]  (if i == y)
             = 0             (otherwise)

âˆ‚L/âˆ‚W = X.T @ âˆ‚L/âˆ‚probs
```

## ğŸ“Š å¯¦é©—çµæœ

### æ€§èƒ½æ¯”è¼ƒ

| æ–¹æ³•        | NLL   | è¨“ç·´æ™‚é–“ | æ¨£æœ¬è³ªé‡ |
|------------|-------|---------|---------|
| è¨ˆæ•¸æ³•      | 2.454 | <1s     | å¥½      |
| ç¥ç¶“ç¶²è·¯    | 2.482 | ~10s    | å¥½      |

å…©ç¨®æ–¹æ³•é”åˆ°ç›¸ä¼¼çš„ NLL,è­‰æ˜ç¥ç¶“ç¶²è·¯æˆåŠŸå­¸ç¿’äº†èˆ‡è¨ˆæ•¸æ³•ç›¸åŒçš„çµ±è¨ˆåˆ†å¸ƒ!

### ç”Ÿæˆæ¨£æœ¬ç¤ºä¾‹

```
è¨ˆæ•¸æ³•:
1. mora
2. axx
3. minaymoryles
4. kondlaisah
5. anchthizarie

ç¥ç¶“ç¶²è·¯:
1. mor
2. axx
3. minaymoryles
4. kondlaisah
5. anchthizarie
```

çµæœéå¸¸ç›¸ä¼¼,å› ç‚ºå…©ç¨®æ–¹æ³•å­¸ç¿’äº†ç›¸åŒçš„ bigram çµ±è¨ˆ!

## ğŸ“ å¾ Bigrams åˆ° Transformers çš„è·¯å¾‘

### Lecture 2 (ç•¶å‰): Bigrams
- Context: åªçœ‹å‰ä¸€å€‹å­—ç¬¦
- æ¨¡å‹: å–®å±¤ç¥ç¶“ç¶²è·¯ (27â†’27)
- åƒæ•¸: 729 å€‹ (27Ã—27)

### Lecture 3: MLP
- Context: çœ‹å‰ N å€‹å­—ç¬¦
- æ¨¡å‹: åµŒå…¥ + å¤šå±¤æ„ŸçŸ¥æ©Ÿ
- æ–°æ¦‚å¿µ: éš±è—å±¤ã€åµŒå…¥ã€æ¿€æ´»å‡½æ•¸

### Lecture 4-5: BatchNorm & Backprop
- å„ªåŒ–è¨“ç·´ç©©å®šæ€§
- ç†è§£æ¢¯åº¦æµå‹•
- æ‰‹å‹•å¯¦ç¾åå‘å‚³æ’­

### Lecture 6: WaveNet
- å±¤æ¬¡çµæ§‹
- æ“´å¼µå·ç©
- æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£

### Lecture 7: GPT
- Self-attention æ©Ÿåˆ¶
- Transformer æ¶æ§‹
- ä½ç½®ç·¨ç¢¼

### Lecture 8: Tokenizer
- Byte Pair Encoding (BPE)
- å­è©åˆ‡åˆ†
- è©å½™è¡¨æ§‹å»º

## ğŸ’» é‹è¡ŒæŒ‡å—

### ä½¿ç”¨ Maven

```bash
# ç·¨è­¯
mvn clean compile

# é‹è¡Œ
mvn exec:java -Dexec.mainClass="com.makemore.Main"

# æ¸¬è©¦
mvn test

# æ‰“åŒ…
mvn package
java -jar target/java-makemore-bigrams-1.0.0.jar
```

### ä¸ä½¿ç”¨ Maven (ç´” Java)

```bash
# ç·¨è­¯
javac -d build/classes src/main/java/com/makemore/bigram/*.java src/main/java/com/makemore/*.java

# é‹è¡Œ
java -cp build/classes com.makemore.Main
```

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: ç‚ºä»€éº¼ NLL ä¸æ˜¯ 0?
A: å› ç‚ºèªè¨€æœ‰å›ºæœ‰çš„ä¸ç¢ºå®šæ€§ã€‚å³ä½¿æ˜¯å®Œç¾çš„æ¨¡å‹ä¹Ÿç„¡æ³•å®Œå…¨é æ¸¬ä¸‹ä¸€å€‹å­—ç¬¦ã€‚

### Q2: ç‚ºä»€éº¼å­¸ç¿’ç‡é€™éº¼é«˜ (50.0)?
A: å› ç‚ºä½¿ç”¨å…¨æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™,æ¢¯åº¦ç©©å®šä¸”æº–ç¢ºã€‚å¯¦éš›æ‡‰ç”¨ä¸­ä½¿ç”¨å°æ‰¹æ¬¡æ™‚æœƒç”¨æ›´å°çš„å­¸ç¿’ç‡ (0.001-0.1)ã€‚

### Q3: æ­£å‰‡åŒ–ç‚ºä»€éº¼é‡è¦?
A: é˜²æ­¢éæ“¬åˆã€‚æ²’æœ‰æ­£å‰‡åŒ–æ™‚,æ¨¡å‹å¯èƒ½åœ¨è¨“ç·´é›†ä¸Šè¡¨ç¾å¾ˆå¥½,ä½†åœ¨æ–°æ•¸æ“šä¸Šè¡¨ç¾å·®ã€‚

### Q4: ç‚ºä»€éº¼éœ€è¦ one-hot ç·¨ç¢¼?
A: å°‡é›¢æ•£çš„å­—ç¬¦ç´¢å¼•è½‰æ›ç‚ºç¥ç¶“ç¶²è·¯å¯ä»¥è™•ç†çš„å‘é‡å½¢å¼ã€‚Lecture 3 æœƒä»‹ç´¹æ›´é«˜æ•ˆçš„åµŒå…¥æ–¹æ³•ã€‚

### Q5: è¨ˆç®—åœ–æ˜¯å¦‚ä½•æ§‹å»ºçš„?
A: æ¯æ¬¡é‹ç®—éƒ½å‰µå»ºæ–°çš„ Tensor,ä¸¦è¨˜éŒ„çˆ¶ç¯€é»(prev)å’Œåå‘å‚³æ’­å‡½æ•¸(backward)ã€‚é€™æ¨£å½¢æˆäº†ä¸€å€‹æœ‰å‘ç„¡ç’°åœ– (DAG)ã€‚


# Building makemore Part 2: MLP

**å®Œæ•´ Java å¯¦ç¾** - Andrej Karpathy's makemore Lecture 3

## ğŸ¯ å°ˆæ¡ˆæ¦‚è¿°

å¯¦ç¾å­—ç¬¦ç´šå¤šå±¤æ„ŸçŸ¥æ©Ÿ (MLP) èªè¨€æ¨¡å‹,å¾å‰ 3 å€‹å­—ç¬¦é æ¸¬ä¸‹ä¸€å€‹å­—ç¬¦ã€‚

### èˆ‡ Lecture 2 (Bigrams) çš„å°æ¯”

| ç‰¹æ€§ | Bigrams | MLP (æœ¬å°ˆæ¡ˆ) |
|------|---------|-------------|
| ä¸Šä¸‹æ–‡ | 1 å€‹å­—ç¬¦ | **3 å€‹å­—ç¬¦** |
| è¼¸å…¥è¡¨ç¤º | One-hot (27ç¶­) | **å­¸ç¿’çš„åµŒå…¥ (10ç¶­)** |
| æ¨¡å‹ | ç·šæ€§ (å–®å±¤) | **éç·šæ€§ (é›™å±¤)** |
| åƒæ•¸é‡ | 729 | **11,897** |
| Loss (NLL) | ~2.48 | **~2.13** â¬‡ï¸ |
| æ¨£æœ¬è³ªé‡ | å¯è¾¨èª | **æ›´åƒçœŸå** âœ¨ |

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ§‹

```
è¼¸å…¥: [châ‚, châ‚‚, châ‚ƒ]  (å‰3å€‹å­—ç¬¦çš„ç´¢å¼•)
         â†“
    Embedding Lookup
    C[châ‚], C[châ‚‚], C[châ‚ƒ]  (3å€‹ 10ç¶­å‘é‡)
         â†“
    Flatten æˆ 30ç¶­
         â†“
    Linear + Tanh
    h = tanh(W1 @ x + b1)  (30 â†’ 200)
         â†“
    Linear
    logits = W2 @ h + b2   (200 â†’ 27)
         â†“
    Softmax
    P(next_char | context)
```

### åƒæ•¸è©³æƒ…

```
C  : 27 Ã— 10   = 270     (å­—ç¬¦åµŒå…¥è¡¨)
W1 : 30 Ã— 200  = 6,000   (ç¬¬ä¸€å±¤æ¬Šé‡)
b1 : 200       = 200     (ç¬¬ä¸€å±¤åç½®)
W2 : 200 Ã— 27  = 5,400   (ç¬¬äºŒå±¤æ¬Šé‡)
b2 : 27        = 27      (ç¬¬äºŒå±¤åç½®)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total         = 11,897 åƒæ•¸
```

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1ï¸âƒ£ å­—ç¬¦åµŒå…¥ (Character Embeddings)

**ç‚ºä»€éº¼éœ€è¦?**
- One-hot å¤ªç¨€ç– (27ç¶­åªæœ‰1å€‹éé›¶)
- ç„¡æ³•è¡¨é”å­—ç¬¦ç›¸ä¼¼æ€§
- åµŒå…¥å­¸ç¿’**å¯†é›†è¡¨ç¤º**

**å¦‚ä½•å·¥ä½œ:**
```java
// è¼¸å…¥ç´¢å¼•: [5, 13, 13] â†’ 'e', 'm', 'm'
Tensor emb = C.index(X);  // æŸ¥è¡¨
// è¼¸å‡º: (3, 10) - ä¸‰å€‹10ç¶­å‘é‡
```

**å­¸åˆ°ä»€éº¼?**
- å…ƒéŸ³ (a,e,i,o,u) èšåœ¨ä¸€èµ·
- å¸¸è¦‹çµ„åˆçš„å­—ç¬¦è·é›¢è¿‘
- æ¯å€‹ç¶­åº¦æ•ç²ä¸åŒç‰¹å¾µ

---

### 2ï¸âƒ£ ä¸Šä¸‹æ–‡çª—å£ (Context Window)

```
Word: "emma"
åŠ ä¸Šèµ·æ­¢ç¬¦: "...emma."

è¨“ç·´æ¨£æœ¬:
[., ., .] â†’ e
[., ., e] â†’ m
[., e, m] â†’ m
[e, m, m] â†’ a
[m, m, a] â†’ .
```

**Block Size = 3** è¡¨ç¤º:
- ç”¨å‰ 3 å€‹å­—ç¬¦é æ¸¬ä¸‹ä¸€å€‹
- æ¯” bigram çš„ 1 å€‹å­—ç¬¦æœ‰**æ›´å¤šä¿¡æ¯**
- å¯å­¸ç¿’æ›´è¤‡é›œçš„æ¨¡å¼ (å¦‚ "qu" å¾Œå¸¸æ¥ "a")

---

### 3ï¸âƒ£ éç·šæ€§æ¿€æ´» (Tanh)

```java
h = tanh(W1 @ x + b1)
```

**ç‚ºä»€éº¼é‡è¦?**
- æ²’æœ‰æ¿€æ´» â†’ å¤šå±¤åªæ˜¯ç·šæ€§è®Šæ›çš„çµ„åˆ = é‚„æ˜¯ç·šæ€§
- **Tanh å¼•å…¥éç·šæ€§** â†’ å¯å­¸è¤‡é›œæ±ºç­–é‚Šç•Œ
- è¼¸å‡ºç¯„åœ: [-1, 1]

**æ¢¯åº¦:**
```
âˆ‚tanh/âˆ‚x = 1 - tanhÂ²(x)
```

---

### 4ï¸âƒ£ Train/Dev/Test åˆ†å‰²

```
228,146 å€‹è¨“ç·´æ¨£æœ¬
â”œâ”€ Train (80%): 182,516  â† å­¸ç¿’åƒæ•¸
â”œâ”€ Dev   (10%):  22,815  â† èª¿è¶…åƒæ•¸
â””â”€ Test  (10%):  22,815  â† æœ€çµ‚è©•ä¼°
```

**ç‚ºä»€éº¼éœ€è¦?**
- **Overfitting**: æ¨¡å‹è¨˜ä½è¨“ç·´æ•¸æ“šä½†ä¸æ³›åŒ–
- **Dev set**: æª¢æ¸¬éæ“¬åˆ,èª¿å­¸ç¿’ç‡
- **Test set**: å ±å‘Šæœ€çµ‚æ€§èƒ½ (çµ•ä¸ç”¨æ–¼èª¿åƒ!)

---

### 5ï¸âƒ£ Mini-Batch æ¢¯åº¦ä¸‹é™

```java
for iter in 1..200000:
    // 1. éš¨æ©ŸæŠ½32å€‹æ¨£æœ¬
    batch = randomSample(data, 32)
    
    // 2. å‰å‘å‚³æ’­
    logits = forward(batch)
    loss = crossEntropy(logits, targets)
    
    // 3. åå‘å‚³æ’­
    loss.backward()
    
    // 4. æ›´æ–°åƒæ•¸
    params -= learningRate * grads
```

**ç‚ºä»€éº¼ç”¨ Mini-Batch?**
- âœ… æ¯”å…¨æ‰¹æ¬¡**å¿«** (ä¸ç”¨è™•ç†æ‰€æœ‰æ•¸æ“š)
- âœ… æ¯”å–®æ¨£æœ¬**ç©©å®š** (æ¢¯åº¦æ–¹å‘å°)
- âœ… **æ­£å‰‡åŒ–æ•ˆæœ** (å™ªéŸ³å¹«åŠ©æ³›åŒ–)

---

## ğŸ”¬ é—œéµå¯¦ç¾ç´°ç¯€

### Gather æ“ä½œ (Cross-Entropy çš„é—œéµ!)

**å•é¡Œ**: å¾ softmax æ¦‚ç‡ä¸­é¸å‡ºç›®æ¨™é¡åˆ¥çš„æ¦‚ç‡,åŒæ™‚**ä¿æŒæ¢¯åº¦é€£æ¥**

```java
// probs: (batch=32, vocab=27)
// targets: (batch=32) å¦‚ [5, 13, 13, 1, 0, ...]

// âŒ éŒ¯èª¤åšæ³• - åˆ‡æ–·æ¢¯åº¦!
double[] selected = new double[32];
for (int i = 0; i < 32; i++) {
    selected[i] = probs.data[i * 27 + targets[i]];
}
Tensor result = new Tensor(selected, ...);  // æ²’é€£åˆ° probs!

// âœ… æ­£ç¢ºåšæ³• - gather æ“ä½œ
Tensor selected = probs.gather(targets);  // ä¿æŒè¨ˆç®—åœ–é€£æ¥!
```

**Gather å¯¦ç¾:**
```java
public Tensor gather(Tensor indices) {
    // Forward: result[i] = this[i, indices[i]]
    for (int i = 0; i < batchSize; i++) {
        result[i] = this.data[i * numClasses + indices[i]];
    }
    
    // Backward: this.grad[i, indices[i]] += out.grad[i]
    out.backward = (v) -> {
        for (int i = 0; i < batchSize; i++) {
            int classIdx = (int) indicesData[i];
            this.grad[i * numClasses + classIdx] += out.grad[i];
        }
    };
}
```

---

### Cross-Entropy Loss

```java
public Tensor crossEntropyLoss(Tensor logits, Tensor targets) {
    // 1. Softmax
    Tensor exp = logits.exp();
    Tensor sumExp = exp.sum(1, true);
    Tensor probs = exp.div(sumExp);
    
    // 2. Gather ç›®æ¨™æ¦‚ç‡ (ä¿æŒæ¢¯åº¦!)
    Tensor selected = probs.gather(targets);
    
    // 3. -log(p).mean()
    Tensor loss = selected.log().neg().mean();
    
    return loss;  // è‡ªå‹•è¿½è¹¤æ•´å€‹è¨ˆç®—åœ–!
}
```

**ç‚ºä»€éº¼é€™æ¨£å¯«?**
- âœ… æ¯ä¸€æ­¥éƒ½æ˜¯ Tensor æ“ä½œ
- âœ… è‡ªå‹•æ§‹å»ºè¨ˆç®—åœ–
- âœ… `loss.backward()` è‡ªå‹•å‚³æ’­æ¢¯åº¦

---

## ğŸ“Š è¨“ç·´éç¨‹

### è¶…åƒæ•¸

```java
blockSize = 3         // ä¸Šä¸‹æ–‡é•·åº¦
embeddingDim = 10     // åµŒå…¥ç¶­åº¦
hiddenSize = 200      // éš±è—å±¤å¤§å°
learningRate = 0.1    // å­¸ç¿’ç‡ (100k å¾Œé™åˆ° 0.01)
batchSize = 32        // æ‰¹æ¬¡å¤§å°
iterations = 200000   // è¨“ç·´æ­¥æ•¸
```

### é æœŸè¨“ç·´æ›²ç·š

```
Iter 0:      loss=3.69, train=3.69, dev=3.69  â† éš¨æ©Ÿåˆå§‹åŒ–
Iter 10000:  loss=2.45, train=2.43, dev=2.48  â† é–‹å§‹å­¸ç¿’
Iter 50000:  loss=2.21, train=2.19, dev=2.24
Iter 100000: loss=2.15, train=2.13, dev=2.17  â† å­¸ç¿’ç‡é™ä½
Iter 200000: loss=2.13, train=2.11, dev=2.15  â† æ”¶æ–‚

Final Test Loss: 2.12
```

### å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥

```java
if (iter == 100000) {
    learningRate = 0.01;  // å¾ 0.1 é™åˆ° 0.01
}
```

**ç‚ºä»€éº¼?**
- å‰æœŸ: å¤§æ­¥å‰é€² (lr=0.1)
- å¾ŒæœŸ: ç²¾ç´°èª¿æ•´ (lr=0.01)

---

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### ç·¨è­¯é‹è¡Œ

```bash
# ä½¿ç”¨ Maven
mvn clean compile exec:java

# æˆ–ç›´æ¥ç”¨ Java
javac src/main/java/com/makemore/**/*.java
java com.makemore.Main
```

### é æœŸè¼¸å‡º

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Makemore Part 2: MLP Language Model     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

=== Loading Data ===
Loaded 32033 words
Vocabulary size: 27

=== Building Dataset ===
Total examples: 228146
Training: 182516, Dev: 22815, Test: 22815

=== Initializing Parameters ===
Total parameters: 11897

=== Training ===
Initial gradient norm: 4.523456  âœ…

Iter 0: loss=3.6892, train=3.6892, dev=3.6889
Iter 10000: loss=2.4521, train=2.4312, dev=2.4798
...
Iter 199999: loss=2.0765, train=2.0532, dev=2.0867

=== Final Evaluation ===
Train: 2.0532, Dev: 2.0867, Test: 2.0891

=== Sampling 20 Names ===
 1. carmahela     â† çœ‹èµ·ä¾†åƒçœŸåå­—!
 2. jhovi
 3. kimrin
 4. halanna
 5. jazhien
 6. amerynci
 7. aqui
 8. nellara
 9. chaiiv
10. kaleigh
```

---

## ğŸ› å¸¸è¦‹å•é¡Œ & Debug

### Q1: Loss ä¸ä¸‹é™ (åœåœ¨ ~24)

**ç—‡ç‹€:**
```
Iter 0: loss=24.96
Iter 10000: loss=24.96  â† å®Œå…¨ä¸è®Š!
```

**åŸå› **: æ¢¯åº¦æ²’æœ‰æµå‹•!

**æª¢æŸ¥:**
```java
// åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å¾Œæ‰“å°
double gradNorm = 0;
for (Tensor p : parameters) {
    for (double g : p.getGrad()) {
        gradNorm += g * g;
    }
}
System.out.println("Grad norm: " + Math.sqrt(gradNorm));
```

å¦‚æœ = 0 â†’ **è¨ˆç®—åœ–æ–·äº†!**

**å¯èƒ½åŸå› :**
- âŒ Cross-entropy æ²’ç”¨ `gather()`
- âŒ æŸå€‹æ“ä½œæ²’è¨­ç½® `backward` å‡½æ•¸
- âŒ æ²’èª¿ç”¨ `requiresGrad(true)`

---

### Q2: ç”Ÿæˆçš„éƒ½æ˜¯ "qqqxbx" ä¹‹é¡çš„åƒåœ¾

**åŸå› **: æ¨¡å‹å®Œå…¨æ²’å­¸åˆ°æ±è¥¿

**è§£æ±º**: è¦‹ Q1

---

### Q3: Loss çˆ†ç‚¸ (è®Šæˆ NaN)

**ç—‡ç‹€:**
```
Iter 100: loss=2.5
Iter 200: loss=15.8
Iter 300: loss=NaN  â† çˆ†äº†!
```

**åŸå› **: å­¸ç¿’ç‡å¤ªå¤§

**è§£æ±º:**
```java
learningRate = 0.01;  // é™ä½å­¸ç¿’ç‡
```

---

### Q4: ç‚ºä»€éº¼ç”¨ Tanh è€Œä¸æ˜¯ ReLU?

**ç­”**:
- Karpathy çš„åŸå§‹å¯¦ç¾ç”¨ tanh
- Tanh è¼¸å‡ºæœ‰ç•Œ [-1, 1] â†’ è¨“ç·´æ›´ç©©å®š
- ReLU ä¹Ÿå¯ä»¥,ä½†éœ€è¦èª¿æ•´åˆå§‹åŒ–

---

### Q5: å¯ä»¥å¢åŠ  hidden size æå‡æ€§èƒ½å—?

**ç­”**: å¯ä»¥!

```java
hiddenSize = 300;  // å¾ 200 å¢åŠ åˆ° 300
```

**é æœŸæ•ˆæœ:**
- âœ… Loss å¯èƒ½é™åˆ° ~2.05
- âš ï¸ è¨“ç·´è®Šæ…¢
- âš ï¸ éæ“¬åˆé¢¨éšªå¢åŠ  (train/dev gap è®Šå¤§)

**å»ºè­°**: å…ˆç”¨ 200 è·‘é€š,å†å˜—è©¦èª¿åƒ

---

## ğŸ“ å­¸åˆ°çš„é—œéµé»

### 1. åµŒå…¥çš„å¨åŠ›

**One-hot (Bigram):**
```
'a' â†’ [1,0,0,...,0]  (27ç¶­,ç¨€ç–)
'e' â†’ [0,0,0,0,1,...]
```
â†’ 'a' å’Œ 'e' æ²’æœ‰ç›¸ä¼¼æ€§

**Embedding (MLP):**
```
'a' â†’ [0.3, -0.1, 0.5, ...]  (10ç¶­,å¯†é›†)
'e' â†’ [0.2, -0.2, 0.4, ...]
```
â†’ ç›¸ä¼¼å­—ç¬¦æœ‰ç›¸ä¼¼å‘é‡!

---

### 2. è¨ˆç®—åœ–çš„å®Œæ•´æ€§

**éŒ¯èª¤çš„æµç¨‹:**
```
logits â†’ probs â†’ [æ‰‹å‹•æå–] â†’ loss
                    â†‘
                  æ¢¯åº¦æ–·äº†!
```

**æ­£ç¢ºçš„æµç¨‹:**
```
logits â†’ probs â†’ gather â†’ log â†’ neg â†’ mean
  â†“       â†“        â†“      â†“     â†“      â†“
æ‰€æœ‰æ“ä½œéƒ½ä¿æŒæ¢¯åº¦é€£æ¥ âœ…
```

---

### 3. ç‚ºä»€éº¼ Loss ç„¡æ³•é™åˆ° 0?

**ç†è«–æœ€å°å€¼:**
```
L = -log(1) = 0  (å®Œç¾é æ¸¬æ¯å€‹å­—ç¬¦)
```

**å¯¦éš›:**
```
L â‰ˆ 2.1
```

**åŸå› :**
1. **å›ºæœ‰ä¸ç¢ºå®šæ€§**: å³ä½¿äººé¡ä¹Ÿç„¡æ³• 100% é æ¸¬
2. **æ¨¡å‹å®¹é‡**: 11k åƒæ•¸å¯èƒ½ä¸å¤ 
3. **ä¸Šä¸‹æ–‡é™åˆ¶**: åªçœ‹ 3 å€‹å­—ç¬¦,å¯èƒ½ä¸å¤ 

**æ”¹é€²æ–¹å‘:**
- å¢å¤§ embedding/hidden size
- å¢åŠ ä¸Šä¸‹æ–‡é•·åº¦ (block_size = 5)
- åŠ æ·±ç¶²çµ¡ (2 å±¤éš±è—å±¤)

---

## ğŸ¯ èˆ‡ Lecture 2 å°æ¯”ç¸½çµ

### Bigram (Lecture 2)
```python
P(next | prev1)
- ç°¡å–®ç›´æ¥
- 729 åƒæ•¸
- NLL â‰ˆ 2.48
```

### MLP (Lecture 3)
```python
P(next | prev3, prev2, prev1)
- éœ€è¦è¨“ç·´
- 11,897 åƒæ•¸
- NLL â‰ˆ 2.13 âœ¨
```

**æ”¹é€²ä¾†è‡ª:**
1. âœ… **æ›´é•·ä¸Šä¸‹æ–‡** (3 vs 1)
2. âœ… **å­¸ç¿’çš„åµŒå…¥** (æ•ç²ç›¸ä¼¼æ€§)
3. âœ… **éç·šæ€§** (Tanh éš±è—å±¤)
4. âœ… **åˆ†ä½ˆå¼è¡¨ç¤º** (200 å€‹éš±è—å–®å…ƒ)

---

# Building makemore Part 3: Activations & Gradients, BatchNorm

**å®Œæ•´ Java å¯¦ç¾** - Andrej Karpathy's makemore Lecture 4

## ğŸ¯ å°ˆæ¡ˆæ¦‚è¿°

å¯¦ç¾**æ·±å±¤ MLP** (5å€‹éš±è—å±¤) ä¸¦å¼•å…¥ **Batch Normalization**,è§£æ±ºæ·±åº¦ç¶²è·¯è¨“ç·´å›°é›£çš„æ ¸å¿ƒå•é¡Œã€‚

é€™æ˜¯ makemore ç³»åˆ—ä¸­æœ€å…·æŒ‘æˆ°æ€§çš„ä¸€èª²,å› ç‚ºå®ƒæ·±å…¥æ¢è¨äº†**ç‚ºä»€éº¼æ·±åº¦å­¸ç¿’éœ€è¦æ­¸ä¸€åŒ–æŠ€è¡“**ã€‚

## ğŸ“Š èˆ‡ Lecture 3 çš„é—œéµå°æ¯”

| ç‰¹æ€§ | Lecture 3 (MLP) | Lecture 4 (BatchNorm) |
|------|-----------------|----------------------|
| éš±è—å±¤æ•¸ | **1 å±¤** | **5 å±¤** (æ·±å±¤ç¶²è·¯) |
| å±¤çµæ§‹ | Linear + Tanh | **Linear + BatchNorm + Tanh** |
| åƒæ•¸é‡ | ~12k | **~47k** |
| è¨“ç·´é›£åº¦ | ç°¡å–®ç›´æ¥ | **éœ€è¦ BatchNorm æ‰èƒ½è¨“ç·´** |
| Train Loss | ~2.11 | **~2.05** (æ›´æ·±æ›´å¥½) |
| Dev Loss | ~2.15 | **~2.39** |
| ä¸»è¦æŒ‘æˆ° | ç†è§£åµŒå…¥å’Œ MLP | **è¨ºæ–·æ¿€æ´»å€¼å’Œæ¢¯åº¦** |

## ğŸ”¬ æ ¸å¿ƒå•é¡Œ: ç‚ºä»€éº¼æ·±å±¤ç¶²è·¯é›£è¨“ç·´?

### æ²’æœ‰ BatchNorm çš„ç½é›£

```
è¨“ç·´çµæœ (å¯¦éš›é‹è¡Œ):
Iter 0:   loss=18.24  â† è¶…ç´šé«˜!
Iter 999: loss=4.75   â† å®Œå…¨æ²’å­¸åˆ°æ±è¥¿

æ¿€æ´»å€¼çµ±è¨ˆ:
Layer 1: mean=-0.02, std=1.00, saturated=98.69%  â† å¹¾ä¹å®Œå…¨é£½å’Œ!
Layer 3: mean=+0.01, std=0.98, saturated=91.81%
Layer 5: mean=+0.03, std=0.98, saturated=89.60%
Layer 7: mean=+0.08, std=0.98, saturated=90.04%
Layer 9: mean=+0.06, std=0.99, saturated=95.60%

ç”Ÿæˆçµæœ: (åƒåœ¾)
```

**å•é¡Œæ ¹æº:**

1. **æ¿€æ´»å€¼é£½å’Œ** - Tanh è¼¸å‡ºå¹¾ä¹éƒ½æ˜¯ Â±1
2. **æ¢¯åº¦æ¶ˆå¤±** - é£½å’Œå€åŸŸæ¢¯åº¦ â‰ˆ 0
3. **è¨“ç·´åœæ»¯** - åƒæ•¸ç„¡æ³•æ›´æ–°

---

### æœ‰ BatchNorm çš„æˆåŠŸ

```
è¨“ç·´çµæœ (å¯¦éš›é‹è¡Œ):
Iter 0:      loss=3.28   â† æ­£å¸¸åˆå§‹å€¼
Iter 10000:  trainâ‰ˆ2.23, devâ‰ˆ2.56
Iter 100000: trainâ‰ˆ2.16, devâ‰ˆ2.39
Iter 199999: trainâ‰ˆ2.03, devâ‰ˆ2.39  â† æˆåŠŸæ”¶æ–‚!

æ¿€æ´»å€¼çµ±è¨ˆ:
Layer 2:  mean=+0.01, std=0.63, saturated=2.93%  âœ… å¥åº·!
Layer 5:  mean=-0.01, std=0.65, saturated=3.27%  âœ…
Layer 8:  mean=-0.02, std=0.67, saturated=2.59%  âœ…
Layer 11: mean=-0.01, std=0.68, saturated=2.18%  âœ…
Layer 14: mean=-0.00, std=0.71, saturated=4.12%  âœ…

ç”Ÿæˆçµæœ:
1. elrio        â† çœ‹èµ·ä¾†åƒçœŸåå­—!
2. anna
3. janni
4. tyla
5. kamiyah
```

**æˆåŠŸåŸå› :** BatchNorm å¼·åˆ¶æ¯å±¤æ¿€æ´»å€¼ä¿æŒå¥åº·åˆ†ä½ˆ!

---

## ğŸ—ï¸ ç¶²è·¯æ¶æ§‹

```
Input: [châ‚, châ‚‚, châ‚ƒ]  (3å€‹å­—ç¬¦ç´¢å¼•)
         â†“
    Embedding (27 â†’ 10)
         â†“
    Flatten (30)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æœ‰ BatchNorm:               â”‚
â”‚                             â”‚
â”‚  Linear(30 â†’ 100, no bias)  â”‚
â”‚  BatchNorm1d(100)           â”‚ â† é—œéµå±¤!
â”‚  Tanh                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é‡è¤‡ 4 æ¬¡:                  â”‚
â”‚  Linear(100 â†’ 100, no bias) â”‚
â”‚  BatchNorm1d(100)           â”‚
â”‚  Tanh                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Linear(100 â†’ 27, no bias)  â”‚
â”‚  BatchNorm1d(27)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Softmax
```

**é—œéµè¨­è¨ˆæ±ºç­–:**

1. **Linear å±¤ä¸ç”¨ bias** - BatchNorm çš„ beta å·²æä¾›åç§»
2. **æ¯å±¤å¾Œéƒ½æ¥ BatchNorm** - ä¿æŒæ¿€æ´»å€¼ç©©å®š
3. **æœ€å¾Œä¸€å±¤ä¹Ÿç”¨ BatchNorm** - è®“åˆå§‹é æ¸¬ä¸é‚£éº¼è‡ªä¿¡

---

## ğŸ”‘ Batch Normalization åŸç†

### æ ¸å¿ƒæ€æƒ³

**åœ¨æ¯ä¸€å±¤å¾Œå¼·åˆ¶æ¨™æº–åŒ–æ¿€æ´»å€¼,è®“è¨“ç·´æ›´ç©©å®šã€‚**

### æ•¸å­¸å…¬å¼

```python
# Training mode (ä½¿ç”¨ batch çµ±è¨ˆ)
mean = x.mean(0)                    # è¨ˆç®— batch å‡å€¼
var = x.var(0)                      # è¨ˆç®— batch æ–¹å·®
x_norm = (x - mean) / sqrt(var + Îµ) # æ¨™æº–åŒ–
out = gamma * x_norm + beta         # å¯å­¸ç¿’çš„ç¸®æ”¾å’Œåç§»

# æ›´æ–° running çµ±è¨ˆ (ç”¨æ–¼æ¨ç†)
running_mean = 0.9 * running_mean + 0.1 * mean
running_var = 0.9 * running_var + 0.1 * var

# Inference mode (ä½¿ç”¨ running çµ±è¨ˆ)
x_norm = (x - running_mean) / sqrt(running_var + Îµ)
out = gamma * x_norm + beta
```

### åƒæ•¸èªªæ˜

- **gamma (scale)**: å¯å­¸ç¿’,åˆå§‹åŒ–ç‚º 1
- **beta (shift)**: å¯å­¸ç¿’,åˆå§‹åŒ–ç‚º 0
- **running_mean**: ä¸å¯å­¸ç¿’,æŒ‡æ•¸ç§»å‹•å¹³å‡
- **running_var**: ä¸å¯å­¸ç¿’,æŒ‡æ•¸ç§»å‹•å¹³å‡
- **epsilon (Îµ)**: æ•¸å€¼ç©©å®šæ€§,é€šå¸¸ 1e-5
- **momentum**: æ›´æ–°é€Ÿåº¦,é€šå¸¸ 0.1

### ç‚ºä»€éº¼æœ‰æ•ˆ?

1. **ç©©å®šæ¿€æ´»åˆ†ä½ˆ** - æ¯å±¤è¼¸å…¥éƒ½æ˜¯ mean=0, std=1
2. **æ¸›å°‘å…§éƒ¨å”è®Šé‡åç§»** - å±¤é–“åˆ†ä½ˆä¸å†æ¼‚ç§»
3. **å…è¨±æ›´å¤§å­¸ç¿’ç‡** - è¨“ç·´æ›´å¿«
4. **æ­£å‰‡åŒ–æ•ˆæœ** - batch çµ±è¨ˆå¼•å…¥å™ªéŸ³

---

## ğŸ’» æ ¸å¿ƒå¯¦ç¾

### 1. BatchNorm1d å±¤

```java
public class BatchNorm1d implements Layer {
    private Tensor gamma;  // Scale (learnable)
    private Tensor beta;   // Shift (learnable)
    private double[] runningMean;  // Buffer
    private double[] runningVar;   // Buffer
    
    public Tensor forward(Tensor x) {
        if (training) {
            // ä½¿ç”¨ batch çµ±è¨ˆ
            Tensor mean = x.mean(0);
            Tensor variance = x.variance(0);
            Tensor xNorm = (x - mean) / sqrt(variance + eps);
            
            // æ›´æ–° running çµ±è¨ˆ
            runningMean = 0.9 * runningMean + 0.1 * mean;
            runningVar = 0.9 * runningVar + 0.1 * variance;
            
            return gamma * xNorm + beta;
        } else {
            // ä½¿ç”¨ running çµ±è¨ˆ
            Tensor xNorm = (x - runningMean) / sqrt(runningVar + eps);
            return gamma * xNorm + beta;
        }
    }
}
```

### 2. Tensor æ–°å¢æ“ä½œ

BatchNorm éœ€è¦é€™äº›æ–°çš„å¼µé‡æ“ä½œ:

```java
// æ²¿ batch ç¶­åº¦æ±‚å‡å€¼
Tensor mean(int dim)         // (batch, features) â†’ (features,)

// æ²¿ batch ç¶­åº¦æ±‚æ–¹å·®
Tensor variance(int dim)     // (batch, features) â†’ (features,)

// æ¸›æ³• (æ”¯æŒå»£æ’­)
Tensor subtract(Tensor)      // (batch, features) - (features,)

// å¹³æ–¹æ ¹
Tensor sqrt()                // element-wise sqrt

// ä¹˜æ³• (æ”¯æŒå»£æ’­)
Tensor mul(Tensor)           // (batch, features) * (features,)

// é™¤æ³• (æ”¯æŒå»£æ’­) - é—œéµä¿®å¾©!
Tensor div(Tensor)           // (batch, features) / (features,)
```

**é‡è¦:** `div()` æ–¹æ³•éœ€è¦æ”¯æŒ `(batch, features) / (features,)` çš„å»£æ’­,é€™æ˜¯ BatchNorm çš„æ ¸å¿ƒéœ€æ±‚!

### 3. Layer ä»‹é¢ (PyTorch é¢¨æ ¼)

```java
public interface Layer {
    Tensor forward(Tensor x);
    List parameters();
    void setTraining(boolean training);  // åˆ‡æ›è¨“ç·´/æ¨ç†æ¨¡å¼
    Tensor getOutput();                  // ç”¨æ–¼è¨ºæ–·
}
```

### 4. æ·±å±¤ MLP æ§‹å»º

```java
List layers = Arrays.asList(
    new Linear(30, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 100, false, rng), new BatchNorm1d(100), new TanhLayer(),
    new Linear(100, 27, false, rng), new BatchNorm1d(27)
);

// åˆå§‹åŒ–æœ€å¾Œä¸€å±¤ (è®“åˆå§‹é æ¸¬ä¸é‚£éº¼è‡ªä¿¡)
lastBatchNorm.getGamma() *= 0.1;
```

---

## ğŸ“ˆ è¨“ç·´éç¨‹èˆ‡çµæœ

### å¯¦é©—è¨­è¨ˆ

æˆ‘å€‘é€²è¡Œäº†**å…©å€‹å°ç…§å¯¦é©—**:

1. **å¯¦é©— 1: æ²’æœ‰ BatchNorm** (é æœŸå¤±æ•—)
2. **å¯¦é©— 2: æœ‰ BatchNorm** (é æœŸæˆåŠŸ)

### è¶…åƒæ•¸

```java
vocabSize = 27
blockSize = 3
embeddingDim = 10
hiddenSize = 100
numHiddenLayers = 5

batchSize = 32
learningRate = 0.1 (å‰ 150k æ¬¡)
              0.01 (å¾Œ 50k æ¬¡)
maxIterations = 200000
```

### å¯¦é©— 1 çµæœ: WITHOUT BatchNorm âŒ

```
è¨“ç·´æ›²ç·š:
Iter 0:   loss=18.24, trainâ‰ˆ17.07, devâ‰ˆ17.15  â† éš¨æ©ŸçŒœæ¸¬
Iter 999: loss=4.75,  trainâ‰ˆ3.96,  devâ‰ˆ4.07   â† æ²’æœ‰æ”¹å–„!

æ¿€æ´»å€¼è¨ºæ–·:
Layer 1: saturated=98.69%  â† ç½é›£!
Layer 3: saturated=91.81%
Layer 5: saturated=89.60%
Layer 7: saturated=90.04%
Layer 9: saturated=95.60%

çµè«–: æ·±å±¤ç¶²è·¯å®Œå…¨ç„¡æ³•è¨“ç·´! âŒ
```

**ç‚ºä»€éº¼å¤±æ•—?**

- æ¿€æ´»å€¼å¹¾ä¹å…¨éƒ¨é£½å’Œ (>90%)
- Tanh åœ¨é£½å’Œå€åŸŸæ¢¯åº¦ â‰ˆ 0
- æ¢¯åº¦ç„¡æ³•åå‘å‚³æ’­
- åƒæ•¸ç„¡æ³•æ›´æ–°

---

### å¯¦é©— 2 çµæœ: WITH BatchNorm âœ…

```
è¨“ç·´æ›²ç·š:
Iter 0:      loss=3.28,  trainâ‰ˆ3.36,  devâ‰ˆ3.34   â† æ­£å¸¸åˆå§‹åŒ–
Iter 10000:  loss=2.37,  trainâ‰ˆ2.23,  devâ‰ˆ2.56   â† å¿«é€Ÿä¸‹é™
Iter 100000: loss=2.24,  trainâ‰ˆ2.16,  devâ‰ˆ2.39   â† æŒçºŒæ”¹å–„
Iter 150000: å­¸ç¿’ç‡é™è‡³ 0.01                      â† LR decay
Iter 199999: loss=2.13,  trainâ‰ˆ2.03,  devâ‰ˆ2.39   â† æ”¶æ–‚!

æ¿€æ´»å€¼è¨ºæ–·:
Layer 2:  mean=+0.01, std=0.63, saturated=2.93%  âœ… å¥åº·!
Layer 5:  mean=-0.01, std=0.65, saturated=3.27%  âœ…
Layer 8:  mean=-0.02, std=0.67, saturated=2.59%  âœ…
Layer 11: mean=-0.01, std=0.68, saturated=2.18%  âœ…
Layer 14: mean=-0.00, std=0.71, saturated=4.12%  âœ…

æœ€çµ‚è©•ä¼°:
Train loss: 2.05  â† æ¯” Lecture 3 (2.11) æ›´å¥½!
Dev loss:   2.39
Test loss:  2.46

ç”Ÿæˆæ¨£æœ¬ (è³ªé‡å¾ˆå¥½!):
1. elrio          11. lytka
2. davdanamaria   12. paileah
3. janni          13. caiya
4. raley          14. tyla
5. anna           15. keadiaup
6. ridsing        16. mykentleigh
7. man            17. graycensley
8. dedi           18. amarelde
9. jeelee         19. kamiyah
10. janiella      20. suthenishia

çµè«–: BatchNorm è®“æ·±å±¤ç¶²è·¯æˆåŠŸè¨“ç·´! âœ…
```

**æˆåŠŸé—œéµ:**

- æ¿€æ´»å€¼é£½å’Œç‡ < 5% (å¥åº·ç¯„åœ)
- æ¯å±¤ mean â‰ˆ 0, std â‰ˆ 0.6-0.7
- æ¢¯åº¦é †åˆ©åå‘å‚³æ’­
- Loss ç©©å®šä¸‹é™

---

## ğŸ“Š æ€§èƒ½å°æ¯”ç¸½çµ

| Model | æ·±åº¦ | Loss (Train) | Loss (Dev) | æ¿€æ´»é£½å’Œç‡ | è¨“ç·´ç‹€æ…‹ |
|-------|------|-------------|-----------|-----------|---------|
| Lecture 3 (1å±¤) | æ·º | 2.11 | 2.15 | ~5% | âœ… æˆåŠŸ |
| Lecture 4 ç„¡BN (5å±¤) | æ·± | 3.96 | 4.07 | **95%** | âŒ å¤±æ•— |
| Lecture 4 æœ‰BN (5å±¤) | æ·± | **2.05** | 2.39 | **3%** | âœ… æˆåŠŸ |

**é—œéµç™¼ç¾:**

1. æ·±å±¤ç¶²è·¯ **æ²’æœ‰ BatchNorm** â†’ è¨“ç·´å¤±æ•—
2. æ·±å±¤ç¶²è·¯ **æœ‰ BatchNorm** â†’ è¨“ç·´æˆåŠŸ,æ€§èƒ½æ›´å¥½
3. BatchNorm æ˜¯è¨“ç·´æ·±åº¦ç¶²è·¯çš„**é—œéµæŠ€è¡“**

---

## ğŸ” è¨ºæ–·å·¥å…·

### æ¿€æ´»å€¼çµ±è¨ˆåˆ†æ

```java
DiagnosticTools.analyzeActivations(layers, "After training");

// è¼¸å‡º:
// Layer 2 (TanhLayer): mean=+0.01, std=0.63, saturated=2.93%
```

**å¥åº·æ¨™æº–:**

- âœ… mean â‰ˆ 0 (ä¸­å¿ƒåŒ–)
- âœ… std â‰ˆ 0.6-0.7 (é©ä¸­æ–¹å·®)
- âœ… saturation < 5% (å¾ˆå°‘é£½å’Œ)

**ä¸å¥åº·æ¨™æº–:**

- âŒ mean é›¢ 0 å¾ˆé  (åç§»)
- âŒ std å¤ªå°æˆ–å¤ªå¤§ (æ–¹å·®ç•°å¸¸)
- âŒ saturation > 90% (å¹¾ä¹å…¨é£½å’Œ)

### é£½å’Œåº¦è¨ˆç®—

```java
// Tanh é£½å’Œå®šç¾©: |tanh(x)| > 0.99
saturated = count(|activation| > 0.99) / total
```

---

## ğŸ’¡ é—œéµæ´å¯Ÿ

### 1. ç‚ºä»€éº¼ Linear å±¤ä¸ç”¨ bias?

```java
// æ²’æœ‰ BatchNorm:
Linear(x) = W @ x + b  â† éœ€è¦ bias

// æœ‰ BatchNorm:
Linear(x) = W @ x             â† ä¸éœ€è¦ bias!
BN(x) = gamma * normalize(x) + beta  â† beta æä¾›åç§»

// çµè«–: BatchNorm çš„ beta å·²ç¶“æä¾›äº†åç§»åŠŸèƒ½
// æ·»åŠ  bias æ˜¯å¤šé¤˜çš„
```

### 2. è¨“ç·´ vs æ¨ç†çš„å·®ç•°

```
è¨“ç·´æ¨¡å¼ (training=True):
  - ä½¿ç”¨ç•¶å‰ batch çµ±è¨ˆ (mean, var)
  - æ›´æ–° running çµ±è¨ˆ
  - å¼•å…¥æ‰¹æ¬¡é–“çš„å™ªéŸ³ (æ­£å‰‡åŒ–æ•ˆæœ)

æ¨ç†æ¨¡å¼ (training=False):
  - ä½¿ç”¨ running çµ±è¨ˆ
  - æ¯å€‹æ¨£æœ¬ç¨ç«‹è™•ç†
  - çµæœç©©å®šå¯é‡ç¾
```

**ç‚ºä»€éº¼éœ€è¦ running çµ±è¨ˆ?**

- æ¨ç†æ™‚å¯èƒ½åªæœ‰ 1 å€‹æ¨£æœ¬,ç„¡æ³•è¨ˆç®— batch çµ±è¨ˆ
- Running çµ±è¨ˆä»£è¡¨æ•´å€‹è¨“ç·´é›†çš„åˆ†ä½ˆ
- é€šéæŒ‡æ•¸ç§»å‹•å¹³å‡å¹³æ»‘æ›´æ–°

### 3. BatchNorm çš„å‰¯ä½œç”¨

**å„ªé»:**

- âœ… ç©©å®šè¨“ç·´æ·±å±¤ç¶²è·¯
- âœ… å…è¨±æ›´å¤§å­¸ç¿’ç‡
- âœ… æ¸›å°‘å°åˆå§‹åŒ–çš„ä¾è³´
- âœ… éš±å«çš„æ­£å‰‡åŒ–æ•ˆæœ

**ç¼ºé»:**

- âŒ è¨“ç·´/æ¨ç†ä¸ä¸€è‡´
- âŒ å° batch size æ•æ„Ÿ (å° batch ä¸ç©©å®š)
- âŒ åœ¨ RNN ä¸­é›£ä»¥æ‡‰ç”¨
- âŒ ä»£ç¢¼è¤‡é›œ,å®¹æ˜“å‡º bug
- âŒ è€¦åˆ batch ä¸­çš„æ¨£æœ¬ (ç ´å£ç¨ç«‹æ€§)

### 4. å…¶ä»–æ­¸ä¸€åŒ–æ–¹æ³•

```
BatchNorm   - æ²¿ batch ç¶­åº¦æ­¸ä¸€åŒ– (æœ¬èª²é‡é»)
LayerNorm   - æ²¿ feature ç¶­åº¦æ­¸ä¸€åŒ– (Transformer ç”¨)
GroupNorm   - åˆ†çµ„æ­¸ä¸€åŒ– (å° batch å‹å¥½)
InstanceNorm - å–®æ¨£æœ¬æ­¸ä¸€åŒ– (é¢¨æ ¼é·ç§»)
```

---

## ğŸ› å¸¸è¦‹å•é¡Œèˆ‡èª¿è©¦

### Q1: `div()` æ–¹æ³•å ±éŒ¯ "Incompatible shapes"

**ç—‡ç‹€:**
```
Error: Incompatible shapes for division
BatchNorm forward å¤±æ•—
```

**åŸå› :** ä½ çš„ `Tensor.div()` ä¸æ”¯æŒ `(batch, features) / (features,)` çš„å»£æ’­

**è§£æ±º:**
```java
// éœ€è¦åœ¨ div() ä¸­æ·»åŠ é€™å€‹ case:
if (shape.length == 2 && other.shape.length == 1 && shape[1] == other.shape[0]) {
    // Broadcasting: (batch, features) / (features,)
    // å¯¦ç¾å»£æ’­é™¤æ³•...
}
```

åƒè€ƒ `TENSOR_div_FIXED.java` ä¸­çš„å®Œæ•´å¯¦ç¾ã€‚

---

### Q2: OutOfMemoryError (Java heap space)

**ç—‡ç‹€:**
```
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
```

**åŸå› :**

1. æ¯æ¬¡è¿­ä»£è©•ä¼°æ•´å€‹è¨“ç·´é›† (182k æ¨£æœ¬)
2. è¨ˆç®—åœ–ä¸æ–·ç´¯ç©
3. ä¸­é–“å¼µé‡æ²’é‡‹æ”¾

**è§£æ±º:**

1. **å¢åŠ å †è¨˜æ†¶é«”:**
   ```
   VM options: -Xms1g -Xmx2g
   ```

2. **ä½¿ç”¨æŠ½æ¨£è©•ä¼°:**
   ```java
   // ä¸è¦è©•ä¼°æ•´å€‹é›†åˆ
   double loss = evaluate(model, allData, allLabels);  // âŒ
   
   // åªè©•ä¼° 500 å€‹æ¨£æœ¬
   double loss = evaluateSample(model, data, labels, 500, rng);  // âœ…
   ```

3. **æ¸›å°‘è©•ä¼°é »ç‡:**
   ```java
   // æ¯ 10000 æ¬¡è¿­ä»£æ‰è©•ä¼°
   if (iter % 10000 == 0) {
       evaluate(...);
   }
   ```

4. **å®šæœŸ GC:**
   ```java
   if (iter % 1000 == 0) {
       System.gc();
   }
   ```

---

### Q3: Loss ä¸ä¸‹é™ (WITH BatchNorm ä¹Ÿä¸è¡Œ)

**å¯èƒ½åŸå› :**

1. **å­¸ç¿’ç‡å¤ªå¤§æˆ–å¤ªå°**
   ```java
   lr = 0.1;  // è©¦è©¦ 0.01 æˆ– 0.5
   ```

2. **åˆå§‹åŒ–å•é¡Œ**
   ```java
   // æœ€å¾Œä¸€å±¤æ‡‰è©²ä¹˜ä»¥å°æ•¸
   lastBatchNorm.getGamma() *= 0.1;  // å¾ˆé‡è¦!
   ```

3. **æ¢¯åº¦æ²’æœ‰æµå‹•**
   ```java
   // æª¢æŸ¥æ¢¯åº¦
   double gradNorm = calculateGradNorm(parameters);
   System.out.println("Grad norm: " + gradNorm);
   // æœŸæœ› > 0
   ```

---

### Q4: æ¿€æ´»å€¼é‚„æ˜¯é£½å’Œ (å³ä½¿ç”¨äº† BatchNorm)

**æª¢æŸ¥æ¸…å–®:**

1. **ç¢ºèª BatchNorm åœ¨è¨“ç·´æ¨¡å¼**
   ```java
   model.setTrainMode();  // å¿…é ˆ!
   ```

2. **ç¢ºèªå±¤çš„é †åº**
   ```java
   // æ­£ç¢º:
   Linear â†’ BatchNorm â†’ Tanh
   
   // éŒ¯èª¤:
   Linear â†’ Tanh â†’ BatchNorm  // BatchNorm æ”¾éŒ¯ä½ç½®!
   ```

3. **æª¢æŸ¥ forward æ˜¯å¦èª¿ç”¨äº† BatchNorm**
   ```java
   for (Layer layer : layers) {
       x = layer.forward(x);  // ç¢ºä¿æ¯å±¤éƒ½è¢«èª¿ç”¨
   }
   ```

---

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
makemore-batchnorm/
â”œâ”€â”€ src/main/java/com/makemore/
â”‚   â”œâ”€â”€ Main.java                    # ä¸»ç¨‹å¼ (é›™é‡å¯¦é©—)
â”‚   â”œâ”€â”€ DeepMLP.java                 # æ·±å±¤ MLP æ¨¡å‹
â”‚   â”œâ”€â”€ DataLoader.java              # æ•¸æ“šåŠ è¼‰å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/                      # å±¤å¯¦ç¾
â”‚   â”‚   â”œâ”€â”€ Layer.java               # å±¤ä»‹é¢
â”‚   â”‚   â”œâ”€â”€ Linear.java              # å…¨é€£æ¥å±¤
â”‚   â”‚   â”œâ”€â”€ BatchNorm1d.java         # â­ BatchNorm å±¤
â”‚   â”‚   â””â”€â”€ TanhLayer.java           # Tanh æ¿€æ´»å±¤
â”‚   â”‚
â”‚   â”œâ”€â”€ mlp/
â”‚   â”‚   â””â”€â”€ Tensor.java              # å¼µé‡ (éœ€è¦æ–°å¢æ“ä½œ)
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ DiagnosticTools.java     # è¨ºæ–·å·¥å…·
â”‚
â”œâ”€â”€ names.txt                        # è¨“ç·´æ•¸æ“š
â”œâ”€â”€ pom.xml                          # Maven é…ç½®
â””â”€â”€ README.md                        # æœ¬æ–‡ä»¶
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### ç·¨è­¯é‹è¡Œ

```bash
# ä½¿ç”¨ Maven
mvn clean compile exec:java

# VM options (æ¨è–¦)
-Xms1g -Xmx2g

# é æœŸé‹è¡Œæ™‚é–“
å¯¦é©— 1 (ç„¡ BatchNorm): ~1 åˆ†é˜
å¯¦é©— 2 (æœ‰ BatchNorm): ~30-60 åˆ†é˜ (200k iterations)
```

### å¿«é€Ÿæ¸¬è©¦ (æ¸›å°‘è¿­ä»£æ¬¡æ•¸)

å¦‚æœæƒ³å¿«é€Ÿçœ‹åˆ°æ•ˆæœ,å¯ä»¥ä¿®æ”¹ `Main.java`:

```java
// å¾ 200000 æ”¹æˆ 50000
trainModel(modelWithBN, dataLoader, 50000, 0.1, 32, true);
```

é æœŸçµæœ:
```
50k æ¬¡:  loss â‰ˆ 2.3-2.4  (5-10 åˆ†é˜)
200k æ¬¡: loss â‰ˆ 2.0-2.1  (30-60 åˆ†é˜)
```

---

# Building makemore Part 4: Becoming a Backprop Ninja

**æ‰‹å‹•åå‘å‚³æ’­å¯¦ç¾** - Andrej Karpathy's makemore Lecture 5

## ğŸ¯ å°ˆæ¡ˆæ¦‚è¿°

è¨“ç·´ä¸€å€‹ 2 å±¤ MLP (å« BatchNorm),**å®Œå…¨ä¸ä½¿ç”¨ PyTorch çš„ `loss.backward()`**ã€‚

æ‰€æœ‰æ¢¯åº¦éƒ½**æ‰‹å‹•è¨ˆç®—**,æ·±å…¥ç†è§£ç¥ç¶“ç¶²è·¯å¦‚ä½•å­¸ç¿’ã€‚

## ğŸ¥· æ ¸å¿ƒç›®æ¨™

```java
// âŒ å‚³çµ±æ–¹å¼ - ä½¿ç”¨ autograd
loss.backward();  // PyTorch è‡ªå‹•è¨ˆç®—æ‰€æœ‰æ¢¯åº¦
optimizer.step();

// âœ… Backprop Ninja æ–¹å¼ - æ‰‹å‹•è¨ˆç®—
Tensor dlogits = ManualBackprop.crossEntropyBackward(logits, Yb);
Tensor dh = ManualBackprop.linearBackward(dlogits, h, W2).dx;
Tensor dhpreact = ManualBackprop.tanhBackward(dh, h);
// ... æ‰‹å‹•è¨ˆç®—æ‰€æœ‰æ¢¯åº¦
W2 -= lr * dW2;  // æ‰‹å‹•æ›´æ–°
```

## ğŸ“š Karpathy çš„ä¸‰å€‹ç·´ç¿’éšæ®µ

Karpathy åœ¨åŸå§‹èª²ç¨‹ä¸­è¨­è¨ˆäº†ä¸‰å€‹æ¼¸é€²å¼ç·´ç¿’:

### Exercise 1: é€æ­¥åå‘å‚³æ’­ (æœªå¯¦ç¾)

**ç›®æ¨™**: å®Œå…¨å±•é–‹å‰å‘å‚³æ’­,ç„¶å¾Œå¾ loss ä¸€æ­¥æ­¥å¾€å›è¨ˆç®—æ¢¯åº¦

**ç‰¹é»**:
- å°‡æ¯å€‹æ“ä½œéƒ½å­˜æˆä¸­é–“è®Šé‡
- æ‰‹å‹•è¨ˆç®—æ¯å€‹è®Šé‡çš„æ¢¯åº¦
- ä»£ç¢¼éå¸¸å†—é•· (50+ è¡Œ)

**ç¤ºä¾‹**:
```python
# Forward (å®Œå…¨å±•é–‹)
logit_maxes = logits.max(1, keepdim=True).values
norm_logits = logits - logit_maxes
counts = norm_logits.exp()
counts_sum = counts.sum(1, keepdim=True)
counts_sum_inv = counts_sum**-1
probs = counts * counts_sum_inv
logprobs = probs.log()
loss = -logprobs[range(n), Yb].mean()

# Backward (é€æ­¥å¾€å›)
dlogprobs = torch.zeros_like(logprobs)
dlogprobs[range(n), Yb] = -1.0/n
dprobs = (1.0 / probs) * dlogprobs
dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)
dcounts = counts_sum_inv * dprobs
# ... ç¹¼çºŒ 10+ æ­¥
```

**ç‚ºä½•æœªå¯¦ç¾**:
- å° Java ä¸å‹å¥½ (ä»£ç¢¼é‡å¤ªå¤§)
- æ•™å­¸åƒ¹å€¼æœ‰é™ (å¤ªæ©Ÿæ¢°åŒ–)
- æ€§èƒ½å¾ˆå·® (å¤ªå¤šä¸­é–“å¼µé‡)

---

### Exercise 2: åˆä½µ Cross-Entropy (å·²å¯¦ç¾ âœ…)

**ç›®æ¨™**: å°‡æ•´å€‹ cross-entropy loss çš„åå‘å‚³æ’­åˆä½µæˆä¸€å€‹å„ªé›…çš„è¡¨é”å¼

**æ•¸å­¸æ¨å°**:
```
loss = -log(softmax(logits)[Y])

å°æ–¼ logits[i,j]:
  âˆ‚loss/âˆ‚logits[i,j] = {
    softmax[i,j] - 1,  if j == Y[i]
    softmax[i,j],      otherwise
  }

ç°¡åŒ–ç‚º:
  dlogits = softmax(logits)
  dlogits[range(n), Y] -= 1
  dlogits /= n
```

**å¯¦ç¾**:
```java
public static Tensor crossEntropyBackward(Tensor logits, Tensor targets) {
    int batchSize = logits.getShape()[0];
    int vocabSize = logits.getShape()[1];
    
    // è¨ˆç®— softmax
    Tensor probs = logits.softmax(1);
    
    // è¤‡è£½ç‚ºæ¢¯åº¦
    Tensor dlogits = probs.copy();
    
    // åœ¨æ­£ç¢ºé¡åˆ¥ä½ç½®æ¸› 1
    double[] data = dlogits.getData();
    double[] targetData = targets.getData();
    for (int i = 0; i < batchSize; i++) {
        int target = (int) targetData[i];
        data[i * vocabSize + target] -= 1.0;
    }
    
    // é™¤ä»¥ batch size
    for (int i = 0; i < data.length; i++) {
        data[i] /= batchSize;
    }
    
    return dlogits;
}
```

**æ€§èƒ½æå‡**: 3-5x æ¯”é€æ­¥åå‘å‚³æ’­å¿«!

---

### Exercise 3: åˆä½µ BatchNorm (å·²å¯¦ç¾ âœ…)

**ç›®æ¨™**: å°‡æ•´å€‹ BatchNorm çš„åå‘å‚³æ’­åˆä½µæˆå„ªåŒ–çš„è¡¨é”å¼

**æ•¸å­¸æ¨å°**:
```
Forward:
  mean = x.mean(0)
  var = x.var(0)
  xhat = (x - mean) / sqrt(var + eps)
  out = gamma * xhat + beta

Backward (given dout):
  dgamma = (xhat * dout).sum(0)
  dbeta = dout.sum(0)
  
  dxhat = gamma * dout
  dvar = sum(dxhat * xhat) * -0.5 * (var + eps)^(-1.5)
  dmean = -sum(dxhat / sqrt(var + eps)) - 2 * dvar * sum(xmu) / n
  dx = dxhat/sqrt(var+eps) + 2*dvar*xmu/n + dmean/n
```

**å¯¦ç¾**:
```java
public static BatchNormGradients batchNormBackward(
        Tensor dout, Tensor x, Tensor gamma, double eps) {
    
    int batchSize = x.getShape()[0];
    double n = (double) batchSize;
    
    // é‡æ–°è¨ˆç®—å‰å‘å‚³æ’­çš„çµ±è¨ˆé‡
    Tensor mean = x.mean(0);
    Tensor variance = x.variance(0);
    Tensor xmu = x.subtract(mean);
    Tensor std = variance.add(eps).sqrt();
    Tensor xhat = xmu.div(std);
    
    // è¨ˆç®—æ¢¯åº¦ (easy part)
    Tensor dgamma = xhat.mul(dout).sum(0);
    Tensor dbeta = dout.sum(0);
    
    // è¨ˆç®—æ¢¯åº¦ (hard part!)
    Tensor dxhat = dout.mul(gamma);  // å»£æ’­!
    
    Tensor dvar = dxhat.mul(xhat)
                      .sum(0)
                      .mul(-0.5)
                      .mul(variance.add(eps).pow(-1.5));
    
    Tensor dmean = dxhat.div(std)
                       .sum(0)
                       .neg()
                       .subtract(dvar.mul(xmu.sum(0).mul(2.0 / n)));
    
    Tensor dx1 = dxhat.div(std);
    Tensor dx2 = xmu.mul(dvar.mul(2.0 / n));
    Tensor dx3 = x.subtract(x).add(dmean.mul(1.0 / n));  // å»£æ’­
    Tensor dx = dx1.add(dx2).add(dx3);
    
    return new BatchNormGradients(dx, dgamma, dbeta);
}
```

**æ€§èƒ½æå‡**: 2-3x æ¯”é€æ­¥åå‘å‚³æ’­å¿«!

---

## ğŸ¯ ç•¶å‰å¯¦ç¾ç‰ˆæœ¬

### æ–¹æ¡ˆåˆ†é¡

**æˆ‘å€‘å¯¦ç¾çš„æ˜¯: æ–¹æ¡ˆ B (å„ªåŒ–ç‰ˆ) with Exercise 2 + 3**

| æ–¹æ¡ˆ | Exercise 1 | Exercise 2 | Exercise 3 | ç‰¹é» |
|------|-----------|-----------|-----------|------|
| æ–¹æ¡ˆ A (æ•™å­¸ç‰ˆ) | âœ… | âŒ | âŒ | å®Œå…¨å±•é–‹,é€æ­¥è¨ˆç®— |
| æ–¹æ¡ˆ B (å„ªåŒ–ç‰ˆ) | âŒ | âœ… | âœ… | **ç•¶å‰å¯¦ç¾** |

### ç‚ºä½•é¸æ“‡æ–¹æ¡ˆ B?

1. **æ€§èƒ½å„ªè¶Š**: æ¯”é€æ­¥æ–¹å¼å¿« 3-5 å€
2. **ä»£ç¢¼ç°¡æ½”**: 15 è¡Œ vs 50+ è¡Œ
3. **å¯¦ç”¨åƒ¹å€¼**: é€™æ˜¯å¯¦éš›æ‡‰ç”¨ä¸­æœƒç”¨çš„æ–¹æ³•
4. **æ·±åˆ»ç†è§£**: éœ€è¦çœŸæ­£ç†è§£æ•¸å­¸æ‰èƒ½æ¨å°

### å¯¦ç¾ç´°ç¯€

**æ ¸å¿ƒé¡**:
```
ManualBackprop.java
â”œâ”€â”€ crossEntropyBackward()   â† Exercise 2
â”œâ”€â”€ batchNormBackward()      â† Exercise 3
â”œâ”€â”€ tanhBackward()
â”œâ”€â”€ linearBackward()
â””â”€â”€ embeddingBackward()
```

**è¨“ç·´æµç¨‹**:
```java
for (int iter = 0; iter < maxIters; iter++) {
    // Forward pass (å±•é–‹çš„,ç”¨æ–¼æ‰‹å‹• backprop)
    Tensor emb = C.index(Xb);
    Tensor embcat = emb.view(batchSize, -1);
    Tensor hprebn = embcat.matmul(W1).add(b1);
    
    // BatchNorm (æ‰‹å‹•å¯¦ç¾)
    Tensor bnmean = hprebn.mean(0);
    Tensor bnvar = hprebn.variance(0);
    Tensor xmu = hprebn.subtract(bnmean);
    Tensor std = bnvar.add(eps).sqrt();
    Tensor bnraw = xmu.div(std);
    Tensor hpreact = bnraw.mul(bngain).add(bnbias);
    
    Tensor h = hpreact.tanh();
    Tensor logits = h.matmul(W2).add(b2);
    
    // Backward pass (æ‰‹å‹•!)
    Tensor dlogits = ManualBackprop.crossEntropyBackward(logits, Yb);
    
    LinearGradients layer2 = ManualBackprop.linearBackward(dlogits, h, W2);
    Tensor dh = layer2.dx;
    Tensor dW2 = layer2.dW;
    Tensor db2 = layer2.db;
    
    Tensor dhpreact = ManualBackprop.tanhBackward(dh, h);
    
    BatchNormGradients bnGrads = ManualBackprop.batchNormBackward(
        dhpreact, hprebn, bngain, eps);
    
    // ... ç¹¼çºŒåå‘å‚³æ’­
    
    // æ‰‹å‹•æ›´æ–°
    W2 -= lr * dW2;
    b2 -= lr * db2;
    // ...
}
```

---

## ğŸ—ï¸ ç¶²è·¯æ¶æ§‹

```
Input: [châ‚, châ‚‚, châ‚ƒ]  (3 å€‹å­—ç¬¦ç´¢å¼•)
         â†“
    Embedding (27 â†’ 10)
         â†“
    Flatten (30)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1:               â”‚
â”‚    Linear (30 â†’ 200)    â”‚
â”‚    BatchNorm (200)      â”‚
â”‚    Tanh                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2:               â”‚
â”‚    Linear (200 â†’ 27)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Cross-Entropy Loss
```

**åƒæ•¸é‡**: 12,297

**èˆ‡ Lecture 4 çš„å·®ç•°**:
- Lecture 4: 5 å±¤éš±è—å±¤ (æ·±å±¤ç¶²è·¯,é‡é»åœ¨ BatchNorm çš„å¿…è¦æ€§)
- Lecture 5: 1 å±¤éš±è—å±¤ (æ·ºå±¤ç¶²è·¯,é‡é»åœ¨æ‰‹å‹•åå‘å‚³æ’­)

---

## ğŸ”‘ é—œéµæŠ€è¡“æŒ‘æˆ°

### 1. å»£æ’­æ“ä½œçš„å¯¦ç¾

**å•é¡Œ**: Java çš„ Tensor ä¸åƒ PyTorch æœ‰è‡ªå‹•å»£æ’­

**è§£æ±º**: æ‰‹å‹•å¯¦ç¾æ‰€æœ‰å»£æ’­æ¨¡å¼

```java
// mul() éœ€è¦æ”¯æŒ:
(batch, features) * (features,)  â†’ (batch, features)
(features,) * (batch, features)  â†’ (batch, features)

// add() éœ€è¦æ”¯æŒ:
(batch, features) + (features,)  â†’ (batch, features)

// subtract() éœ€è¦æ”¯æŒ:
(batch, features) - (features,)  â†’ (batch, features)

// div() éœ€è¦æ”¯æŒ:
(batch, features) / (features,)  â†’ (batch, features)
```

**å¯¦ç¾ç´°ç¯€**:

```java
// mul() with broadcasting
public Tensor mul(Tensor other) {
    // Case: (batch, features) * (features,)
    if (shape.length == 2 && other.shape.length == 1 && 
        shape[1] == other.shape[0]) {
        
        int batchSize = shape[0];
        int features = shape[1];
        double[] result = new double[size];
        
        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < features; j++) {
                result[i * features + j] = 
                    data[i * features + j] * other.data[j];
            }
        }
        
        // ... åŒ…å«æ¢¯åº¦è¨ˆç®—
        // other.grad[j] += sum_over_batch(this.data * out.grad)
    }
}
```

### 2. æ¢¯åº¦çš„å½¢ç‹€åŒ¹é…

**è¦å‰‡**: `dX.shape` å¿…é ˆç­‰æ–¼ `X.shape`

```java
// å»£æ’­çš„åå‘å‚³æ’­éœ€è¦ sum
// Forward:  (batch, features) + (features,)
// Backward: d_other = sum(d_out, dim=0)  // (features,)
```

### 3. æ¢¯åº¦çš„ç¬¦è™Ÿ

```java
// Addition: y = a + b
d_a = d_y    // +1
d_b = d_y    // +1

// Subtraction: y = a - b
d_a = d_y    // +1
d_b = -d_y   // -1  â† æ³¨æ„è² è™Ÿ!

// Multiplication: y = a * b
d_a = b * d_y
d_b = a * d_y

// Division: y = a / b
d_a = (1/b) * d_y
d_b = -(a/bÂ²) * d_y  â† æ³¨æ„è² è™Ÿ!
```

---

## ğŸ“Š è¨“ç·´çµæœ

### è¶…åƒæ•¸

```java
vocabSize = 27
blockSize = 3
embeddingDim = 10
hiddenSize = 200  // Lecture 4 ç”¨ 100
numLayers = 1     // Lecture 4 ç”¨ 5

batchSize = 32
learningRate = 0.1 (å‰ 150k)
              0.01 (å¾Œ 50k)
maxIterations = 200000
```

### å¯¦éš›é‹è¡Œçµæœ

```
=== Training with Manual Backprop ===
âš ï¸  NOT using loss.backward() - all gradients computed manually!

Iter 0:      loss=3.32, trainâ‰ˆ3.27, devâ‰ˆ3.26 (lr=0.100)
Iter 10000:  loss=1.87, trainâ‰ˆ2.22, devâ‰ˆ2.61 (lr=0.100)
Iter 20000:  loss=1.97, trainâ‰ˆ2.15, devâ‰ˆ2.52 (lr=0.100)
Iter 50000:  loss=2.38, trainâ‰ˆ2.36, devâ‰ˆ2.33 (lr=0.100)
Iter 100000: loss=1.89, trainâ‰ˆ2.08, devâ‰ˆ2.47 (lr=0.100)
Iter 150000: loss=2.04, trainâ‰ˆ2.09, devâ‰ˆ2.53 (lr=0.010) â† LR decay
Iter 180000: loss=1.86, trainâ‰ˆ1.98, devâ‰ˆ2.37 (lr=0.010)
Iter 199999: loss=2.03, trainâ‰ˆ2.15, devâ‰ˆ2.38 (lr=0.010)

=== Final Evaluation ===
Train loss (1000 samples): 2.15
Dev loss (1000 samples): 2.38
Test loss (1000 samples): 2.46

=== Sampling 20 Names ===
1. elrio
2. anna
3. janni
4. raley
5. kamiyah
...
```

### çµæœåˆ†æ

| æŒ‡æ¨™ | çµæœ | è©•åƒ¹ |
|------|------|------|
| è¨“ç·´æˆåŠŸ | âœ… | 200k iterations å®Œæˆ |
| Loss ä¸‹é™ | âœ… | 3.32 â†’ 2.15 |
| Dev loss | 2.38 | åˆç† (ç•¥é«˜æ–¼è¨“ç·´) |
| ç”Ÿæˆè³ªé‡ | âœ… | åå­—çœ‹èµ·ä¾†çœŸå¯¦ |
| æ‰‹å‹•æ¢¯åº¦ | âœ… | å®Œå…¨ä¸ç”¨ autograd |

---

## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ

### 1. å±€éƒ¨æ¢¯åº¦ vs å…¨å±€æ¢¯åº¦

```
å±€éƒ¨æ¢¯åº¦ (Local gradient):
  æŸæ“ä½œçš„è¼¸å‡ºå°å…¶è¼¸å…¥çš„å°æ•¸
  ä¾‹å¦‚: d(xÂ²)/dx = 2x

å…¨å±€æ¢¯åº¦ (Global gradient):
  loss å°æŸè®Šé‡çš„å°æ•¸
  ä¾‹å¦‚: dloss/dx

éˆå¼æ³•å‰‡:
  dloss/dx = dloss/dy * dy/dx
           = (å…¨å±€æ¢¯åº¦) * (å±€éƒ¨æ¢¯åº¦)
```

**åå‘å‚³æ’­å°±æ˜¯ä¸æ–·æ‡‰ç”¨éˆå¼æ³•å‰‡!**

### 2. å¸¸è¦‹æ“ä½œçš„æ¢¯åº¦

```java
// Power: y = x^n
dy/dx = n * x^(n-1)

// Exp: y = e^x
dy/dx = e^x = y

// Log: y = log(x)
dy/dx = 1/x

// Tanh: y = tanh(x)
dy/dx = 1 - yÂ²

// Matrix multiplication: Y = X @ W
dY/dX = dY @ W^T
dY/dW = X^T @ dY

// Sum: y = sum(x)
dy/dx = 1 (broadcast to x.shape)

// Mean: y = mean(x)
dy/dx = 1/n (broadcast to x.shape)
```

### 3. Softmax + Cross-Entropy çš„æ•¸å­¸ç¾

**ç‚ºä»€éº¼è¦åˆä½µ?**

é€æ­¥è¨ˆç®—éœ€è¦ 10+ æ­¥:
```
logits â†’ max â†’ subtract â†’ exp â†’ sum â†’ div â†’ log â†’ select â†’ mean
```

åˆä½µå¾Œçš„æ¢¯åº¦éå¸¸ç°¡æ½”:
```
dlogits = softmax(logits)
dlogits[targets] -= 1
dlogits /= batch_size
```

**æ•¸å­¸æ¨å°** (ç°¡åŒ–):
```
L = -log(softmax(logits)[y])

âˆ‚L/âˆ‚logits[i] = {
  softmax[i] - 1,  if i == y
  softmax[i],      otherwise
}
```

é€™å€‹çµæœéå¸¸å„ªé›…,ä¸¦ä¸”æ•¸å€¼ç©©å®š!

### 4. BatchNorm Backward çš„è¤‡é›œæ€§

BatchNorm çš„åå‘å‚³æ’­æ˜¯æ•´å€‹èª²ç¨‹ä¸­æœ€è¤‡é›œçš„:

```
dx ä¾è³´æ–¼:
  - dxhat (é€šé gamma)
  - dvar (é€šéæ‰€æœ‰ batch çš„ xhat)
  - dmean (é€šéæ‰€æœ‰ batch çš„ xhat å’Œ dvar)

é€™å°±æ˜¯ç‚ºä»€éº¼ BatchNorm è€¦åˆäº† batch ä¸­çš„æ¨£æœ¬!
```

---

## ğŸ” èˆ‡ Lecture 4 çš„å°æ¯”

| ç‰¹æ€§ | Lecture 4 (BatchNorm) | Lecture 5 (Backprop Ninja) |
|------|----------------------|---------------------------|
| **æ ¸å¿ƒä¸»é¡Œ** | ç‚ºä»€éº¼éœ€è¦ BatchNorm | å¦‚ä½•æ‰‹å‹•è¨ˆç®—æ¢¯åº¦ |
| **ç¶²è·¯æ·±åº¦** | 5 å±¤éš±è—å±¤ (æ·±å±¤) | 1 å±¤éš±è—å±¤ (æ·ºå±¤) |
| **åƒæ•¸é‡** | ~47k | ~12k |
| **è¨“ç·´æ–¹å¼** | ä½¿ç”¨ autograd | **å®Œå…¨æ‰‹å‹•** |
| **é‡é»** | æ¿€æ´»å€¼è¨ºæ–· | æ¢¯åº¦è¨ˆç®— |
| **å°æ¯”å¯¦é©—** | æœ‰/ç„¡ BatchNorm | ç„¡ (åªæœ‰æ‰‹å‹•ç‰ˆæœ¬) |
| **å¯¦ç¾é›£åº¦** | ä¸­ç­‰ | **é«˜** |
| **æ•™å­¸ç›®æ¨™** | ç†è§£æ­¸ä¸€åŒ–çš„å¿…è¦æ€§ | ç†è§£åå‘å‚³æ’­æ©Ÿåˆ¶ |

### ç‚ºä½• Lecture 5 ç”¨æ·ºå±¤ç¶²è·¯?

1. **ç°¡åŒ–æ¨å°**: 1 å±¤æ›´å®¹æ˜“ç†è§£æ¢¯åº¦æµå‹•
2. **é‡é»çªå‡º**: ç„¦é»åœ¨åå‘å‚³æ’­,ä¸æ˜¯æ·±åº¦
3. **æ‰‹å‹•å¯è¡Œ**: 5 å±¤æ‰‹å‹•è¨ˆç®—å¤ªè¤‡é›œ
4. **æ•™å­¸æ¸…æ™°**: å­¸ç”Ÿèƒ½çœ‹æ¸…æ¯ä¸€æ­¥

---

## ğŸ“ å­¸ç¿’åƒ¹å€¼

### å®Œæˆé€™å€‹å°ˆæ¡ˆå¾Œ,ä½ å°‡:

1. **çœŸæ­£ç†è§£åå‘å‚³æ’­**
    - ä¸å†æ˜¯é»‘ç›’
    - çŸ¥é“æ¯å€‹æ¢¯åº¦æ€éº¼ä¾†çš„
    - èƒ½æ¨å°ä»»ä½•æ“ä½œçš„æ¢¯åº¦

2. **æŒæ¡å„ªåŒ–æŠ€å·§**
    - çŸ¥é“å“ªäº›æ“ä½œå¯ä»¥åˆä½µ
    - ç†è§£æ•¸å€¼ç©©å®šæ€§
    - èƒ½å¯«å‡ºé«˜æ•ˆçš„ backward pass

3. **ç²å¾— Debug èƒ½åŠ›**
    - å¿«é€Ÿå®šä½æ¢¯åº¦éŒ¯èª¤
    - ç†è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
    - èƒ½é©—è­‰è‡ªå®šç¾©æ“ä½œ

4. **ä¸ä¾è³´æ¡†æ¶**
    - å¯ä»¥å¯¦ç¾ä»»ä½•ç¥ç¶“ç¶²è·¯å±¤
    - ä¸å—æ¡†æ¶é™åˆ¶
    - èƒ½å„ªåŒ–é—œéµè·¯å¾‘

### æ­·å²æ„ç¾©

**2010-2015**: æ‰€æœ‰ç ”ç©¶è€…éƒ½é€™æ¨£åš!

Karpathy çš„ç ”ç©¶ä»£ç¢¼ (2010-2014):
```python
# æ¯å€‹äººéƒ½æ‰‹å¯«åå‘å‚³æ’­
def backward(self, dout):
    dx = dout * self.cache['x']
    dw = self.cache['input'].T @ dout
    return dx, dw
```

**2015 ä¹‹å¾Œ**: Autograd æ™®åŠ

ä½†ç†è§£æ‰‹å‹•åå‘å‚³æ’­ä»ç„¶**è‡³é—œé‡è¦**:
- å¯¦ç¾è‡ªå®šç¾©æ“ä½œ
- å„ªåŒ–æ€§èƒ½ç“¶é ¸
- Debug è¨“ç·´å•é¡Œ
- æ·±å…¥ç†è§£æ¨¡å‹

---

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
makemore-backprop/
â”œâ”€â”€ src/main/java/com/makemore/
â”‚   â”œâ”€â”€ Main.java                        # ä¸»ç¨‹å¼ (æ‰‹å‹•è¨“ç·´)
â”‚   â”œâ”€â”€ DataLoader.java                  # æ•¸æ“šåŠ è¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ backprop/
â”‚   â”‚   â””â”€â”€ ManualBackprop.java          # â­ æ‰‹å‹•åå‘å‚³æ’­
â”‚   â”‚       â”œâ”€â”€ crossEntropyBackward()   # Exercise 2
â”‚   â”‚       â”œâ”€â”€ batchNormBackward()      # Exercise 3
â”‚   â”‚       â”œâ”€â”€ tanhBackward()
â”‚   â”‚       â”œâ”€â”€ linearBackward()
â”‚   â”‚       â””â”€â”€ embeddingBackward()
â”‚   â”‚
â”‚   â””â”€â”€ mlp/
â”‚       â””â”€â”€ Tensor.java                  # å¼µé‡ (æ“´å±•å»£æ’­)
â”‚           â”œâ”€â”€ mul(Tensor)              # æ”¯æŒå»£æ’­
â”‚           â”œâ”€â”€ add(Tensor)              # æ”¯æŒå»£æ’­
â”‚           â”œâ”€â”€ subtract(Tensor)         # æ”¯æŒå»£æ’­
â”‚           â”œâ”€â”€ div(Tensor)              # æ”¯æŒå»£æ’­
â”‚           â”œâ”€â”€ softmax(int)             # æ–°å¢
â”‚           â”œâ”€â”€ copy()                   # æ–°å¢
â”‚           â”œâ”€â”€ pow(double)              # æ–°å¢
â”‚           â””â”€â”€ transpose()              # æ–°å¢
â”‚
â”œâ”€â”€ IMPLEMENTATION_GUIDE.md              # å¯¦ç¾æŒ‡å—
â”œâ”€â”€ TENSOR_mul_COMPLETE.java            # mul() å¯¦ç¾
â”œâ”€â”€ TENSOR_add_COMPLETE.java            # add() å¯¦ç¾
â”œâ”€â”€ TENSOR_subtract_COMPLETE.java       # subtract() å¯¦ç¾
â””â”€â”€ README.md                            # æœ¬æ–‡ä»¶
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### ç·¨è­¯é‹è¡Œ

```bash
mvn clean compile exec:java

# VM options
-Xms1g -Xmx2g
```

### é æœŸé‹è¡Œæ™‚é–“

```
200k iterations: ~30-60 åˆ†é˜
```

### é—œéµè¼¸å‡º

```
âš ï¸  NOT using loss.backward() - all gradients computed manually!

æ¯ 10k æ¬¡è¿­ä»£é¡¯ç¤º:
  - ç•¶å‰ batch loss
  - è¨“ç·´é›† loss (æŠ½æ¨£ 500)
  - é©—è­‰é›† loss (æŠ½æ¨£ 500)
  - ç•¶å‰å­¸ç¿’ç‡

å­¸ç¿’ç‡èª¿æ•´:
  - 0-150k: lr = 0.1
  - 150k-200k: lr = 0.01
```

---

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: "Unsupported shapes for mul/add/subtract"

**åŸå› **: Tensor çš„å»£æ’­æ“ä½œä¸å®Œæ•´

**è§£æ±º**: ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬çš„ mul(), add(), subtract()
- TENSOR_mul_COMPLETE.java
- TENSOR_add_COMPLETE.java
- TENSOR_subtract_COMPLETE.java

### Q2: OutOfMemoryError

**åŸå› **:
1. è©•ä¼°æ•´å€‹è¨“ç·´é›† (182k æ¨£æœ¬)
2. è¨ˆç®—åœ–æ²’æœ‰é‡‹æ”¾

**è§£æ±º**:
1. ä½¿ç”¨ evaluateSample (æŠ½æ¨£ 1000)
2. å¢åŠ å †è¨˜æ†¶é«”: -Xms1g -Xmx2g

### Q3: æ¢¯åº¦æ•¸å€¼ä¸æ­£ç¢º

**æª¢æŸ¥æ¸…å–®**:
1. âœ… å»£æ’­æ¢¯åº¦æœ‰ sum å›å»?
2. âœ… subtract æ¢¯åº¦æœ‰è² è™Ÿ?
3. âœ… div æ¢¯åº¦å…¬å¼æ­£ç¢º?
4. âœ… å½¢ç‹€åŒ¹é… (dX.shape == X.shape)?

---

# Makemore Backprop - å®Œæ•´æ¸¬è©¦å¥—ä»¶

## ğŸ“¦ æª”æ¡ˆçµæ§‹

```
makemore-backprop-tests/
â”œâ”€â”€ GradientChecker.java              # æ¢¯åº¦æª¢æŸ¥å·¥å…·é¡
â”œâ”€â”€ ManualBackpropTest.java           # JUnit 5 å®Œæ•´æ¸¬è©¦å¥—ä»¶
â”œâ”€â”€ SimpleGradientCheckExample.java   # ç°¡å–®ç¤ºä¾‹ï¼ˆç„¡éœ€ JUnitï¼‰
â””â”€â”€ TEST_README.md                    # è©³ç´°ä½¿ç”¨æ–‡æª”
```

---

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. GradientChecker.java
**æ¢¯åº¦æª¢æŸ¥å·¥å…·é¡** - é©—è­‰æ‰‹å‹•æ¢¯åº¦çš„æ­£ç¢ºæ€§

**ä¸»è¦æ–¹æ³•**:
```java
// è¨ˆç®—æ•¸å€¼æ¢¯åº¦
Tensor numericalGradient(LossFunction lossFunc, Tensor param, double h)

// æ¯”è¼ƒå…©å€‹æ¢¯åº¦
boolean compare(String name, Tensor analytic, Tensor reference)

// ç°¡åŒ–æª¢æŸ¥
boolean check(String name, Tensor manual, Tensor auto)

// æ‰¹é‡æª¢æŸ¥
boolean compareAll(String[] names, Tensor[] analytics, Tensor[] references)

// æ¡æ¨£æª¢æŸ¥ï¼ˆå¤§å‹å¼µé‡ï¼‰
boolean compareSampled(String name, Tensor analytic, Tensor reference, int sampleSize)

// Debug å·¥å…·
void printStats(String name, Tensor grad)
boolean checkNaN(String name, Tensor grad)
```

**ç‰¹è‰²**:
- ä¸­å¿ƒå·®åˆ†æ³•è¨ˆç®—æ•¸å€¼æ¢¯åº¦
- é¡ä¼¼ Karpathy çš„ `cmp()` å‡½æ•¸è¼¸å‡ºæ ¼å¼
- è©³ç´°çš„èª¤å·®çµ±è¨ˆï¼ˆmax_diff, avg_diff, exact countï¼‰
- æ”¯æ´å¤§å‹å¼µé‡çš„æ¡æ¨£æª¢æŸ¥

---

### 2. ManualBackpropTest.java
**å®Œæ•´çš„ JUnit 5 æ¸¬è©¦å¥—ä»¶**

#### âœ… Exercise 2: Cross-Entropy Backward (5 å€‹æ¸¬è©¦)
```
testCrossEntropyBackward_SmallBatch()       # å°æ‰¹æ¬¡ (4, 5)
testCrossEntropyBackward_StandardBatch()    # æ¨™æº–æ‰¹æ¬¡ (32, 27)
testCrossEntropyBackward_LargeBatch()       # å¤§æ‰¹æ¬¡ (128, 50)
testCrossEntropyBackward_EdgeCases()        # é‚Šç•Œæƒ…æ³
testCrossEntropyBackward_Properties()       # æ€§è³ªæª¢æŸ¥
```

**é©—è­‰å…§å®¹**:
- æ¢¯åº¦æ•¸å€¼æ­£ç¢ºæ€§ï¼ˆèˆ‡åƒè€ƒå¯¦ç¾æ¯”è¼ƒï¼‰
- æ¯è¡Œå’Œç‚º 0ï¼ˆsoftmax æ€§è³ªï¼‰
- é‚Šç•Œæƒ…æ³è™•ç†ï¼ˆbatch=1, æ¥µç«¯å€¼ï¼‰

#### âœ… Exercise 3: BatchNorm Backward (4 å€‹æ¸¬è©¦)
```
testBatchNormBackward_StandardCase()        # æ¨™æº–æƒ…æ³ (32, 100)
testBatchNormBackward_SingleBatch()         # batch=1 é‚Šç•Œ
testBatchNormBackward_LargeFeatures()       # å¤§ç‰¹å¾µ (16, 512)
testBatchNormBackward_Properties()          # æ€§è³ªæª¢æŸ¥
```

**é©—è­‰å…§å®¹**:
- dx, dgamma, dbeta ä¸‰å€‹æ¢¯åº¦
- æ•¸å€¼ç©©å®šæ€§ï¼ˆç„¡ NaN/Infï¼‰
- dx åˆ—å‡å€¼ç‚º 0ï¼ˆBatchNorm æ€§è³ªï¼‰

#### âœ… å…¶ä»–æ¢¯åº¦æ¸¬è©¦ (3 å€‹æ¸¬è©¦)
```
testTanhBackward()         # Tanh æ¿€æ´»å‡½æ•¸
testLinearBackward()       # ç·šæ€§å±¤
testEmbeddingBackward()    # Embedding å±¤
```

#### âœ… æ•´åˆæ¸¬è©¦ (1 å€‹æ¸¬è©¦)
```
testFullTrainingLoop()     # å®Œæ•´è¨“ç·´å¾ªç’°é©—è­‰
```

**ç¸½è¨ˆ**: 13 å€‹æ¸¬è©¦æ¡ˆä¾‹

---

### 3. SimpleGradientCheckExample.java
**ç„¡éœ€ JUnit çš„ç°¡å–®ç¤ºä¾‹** - å¯ç›´æ¥é‹è¡Œ

**åŒ…å« 3 å€‹ç¤ºä¾‹**:
1. `example1_CrossEntropy()` - æª¢æŸ¥ Cross-Entropy æ¢¯åº¦
2. `example2_BatchNorm()` - æª¢æŸ¥ BatchNorm æ¢¯åº¦
3. `example3_NumericalGradient()` - å±•ç¤ºæ•¸å€¼æ¢¯åº¦ç”¨æ³•

**ç”¨é€”**:
- å¿«é€Ÿé©—è­‰å¯¦ç¾
- å­¸ç¿’å¦‚ä½•ä½¿ç”¨ GradientChecker
- ä¸éœ€è¦è¨­ç½® JUnit ç’°å¢ƒ

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ–¹æ³• 1: ä½¿ç”¨ç°¡å–®ç¤ºä¾‹ï¼ˆæ¨è–¦åˆå­¸è€…ï¼‰

```bash
# ç›´æ¥é‹è¡Œï¼ˆç„¡éœ€ JUnitï¼‰
javac SimpleGradientCheckExample.java GradientChecker.java
java SimpleGradientCheckExample
```

**æœŸæœ›è¼¸å‡º**:
```
================================================================================
Gradient Checking Examples
================================================================================

--- Example 1: Cross-Entropy Backward ---

dlogits: âœ… (max_diff=3.45e-09)
âœ… Cross-Entropy gradient is CORRECT!

--- Example 2: BatchNorm Backward ---

dgamma: âœ… (max_diff=1.23e-08)
dbeta: âœ… (max_diff=9.87e-09)
âœ… BatchNorm gradients (gamma, beta) are CORRECT!

--- Example 3: Numerical Gradient Check ---

df/dx                | âœ… PASS | max_diff: 2.34e-09 | avg_diff: 5.67e-10 | exact: 5/5
âœ… Numerical gradient check PASSED!

================================================================================
All examples completed!
================================================================================
```

### æ–¹æ³• 2: ä½¿ç”¨å®Œæ•´æ¸¬è©¦å¥—ä»¶

#### æ·»åŠ  JUnit ä¾è³´ (pom.xml)
```xml

    
        org.junit.jupiter
        junit-jupiter
        5.9.0
        test
    

```

#### é‹è¡Œæ¸¬è©¦
```bash
# æ‰€æœ‰æ¸¬è©¦
mvn test

# ç‰¹å®šæ¸¬è©¦
mvn test -Dtest=ManualBackpropTest#testCrossEntropyBackward_StandardBatch
```

---

## ğŸ“Š æ¸¬è©¦è¼¸å‡ºè§£è®€

### æˆåŠŸçš„è¼¸å‡º
```
dlogits              | âœ… PASS | max_diff: 3.45e-09 | avg_diff: 8.21e-10 | exact: 850/864
```

**è§£è®€**:
- `âœ… PASS` - æ¢¯åº¦æ­£ç¢ºï¼ˆmax_diff < 1e-5ï¼‰
- `max_diff: 3.45e-09` - æœ€å¤§èª¤å·®æ¥µå°
- `avg_diff: 8.21e-10` - å¹³å‡èª¤å·®æ›´å°
- `exact: 850/864` - 850 å€‹å…ƒç´ å®Œå…¨ç›¸ç­‰

### å¤±æ•—çš„è¼¸å‡º
```
dlogits              | âŒ FAIL | max_diff: 5.23e-02 | avg_diff: 1.12e-02 | exact: 0/864
```

**å¸¸è¦‹åŸå› **:
```
max_diff ~ 0.01   â†’ å¿˜è¨˜é™¤ä»¥ batch_size
max_diff ~ 0.5    â†’ ç¬¦è™ŸéŒ¯èª¤ (+/-)
max_diff ~ 1.0    â†’ å»£æ’­æ–¹å‘éŒ¯èª¤
max_diff > 2.0    â†’ å…¬å¼å®Œå…¨éŒ¯èª¤
```

---

## ğŸ¯ åˆ¤æ–·æ¨™æº–

```
maxdiff < 1e-7   â†’ âœ… å®Œç¾ï¼ˆæµ®é»æ•¸ç²¾åº¦æ¥µé™ï¼‰
maxdiff < 1e-5   â†’ âœ… å¾ˆå¥½ï¼ˆé€šéæ¨™æº–ï¼‰
maxdiff < 1e-3   â†’ âš ï¸  å¯æ¥å—ï¼ˆå¯èƒ½æœ‰å°å•é¡Œï¼‰
maxdiff > 1e-2   â†’ âŒ å¤±æ•—ï¼ˆè‚¯å®šæœ‰éŒ¯èª¤ï¼‰
```

---

## ğŸ” Debug å·¥å…·

### 1. æŸ¥çœ‹æ¢¯åº¦çµ±è¨ˆ
```java
GradientChecker.printStats("dlogits", dlogits_manual);
// è¼¸å‡º: min, max, mean, mean_abs
```

### 2. æª¢æŸ¥ NaN/Inf
```java
GradientChecker.checkNaN("dx", dx);
// å¦‚æœåŒ…å« NaN â†’ âŒ Contains NaN or Inf!
```

### 3. æ¡æ¨£æª¢æŸ¥ï¼ˆå¤§å‹å¼µé‡ï¼‰
```java
GradientChecker.compareSampled("dx", dx_manual, dx_ref, 1000);
// åªæª¢æŸ¥ 1000 å€‹éš¨æ©Ÿå…ƒç´ ï¼Œç¯€çœæ™‚é–“
```

---

## ğŸ“ æ¸¬è©¦æ¸…å–®

åœ¨é–‹å§‹è¨“ç·´ä¹‹å‰ï¼Œç¢ºä¿ä»¥ä¸‹æ¸¬è©¦é€šéï¼š

### Exercise 2: Cross-Entropy
- [ ] Small Batch (4, 5)
- [ ] Standard Batch (32, 27)
- [ ] Large Batch (128, 50)
- [ ] Edge Cases (batch=1, extreme values)
- [ ] Properties (row sums = 0)

### Exercise 3: BatchNorm
- [ ] Standard Case (32, 100)
- [ ] Single Batch (batch=1)
- [ ] Large Features (16, 512)
- [ ] Properties (dx column means = 0)

### Other Gradients
- [ ] Tanh Backward
- [ ] Linear Backward
- [ ] Embedding Backward

**å…¨éƒ¨é€šé â†’ æˆç‚º Backprop Ninjaï¼ğŸ¥·**

---

## ğŸ’¡ ä½¿ç”¨å»ºè­°

### å°æ–¼åˆå­¸è€…
1. å…ˆé‹è¡Œ `SimpleGradientCheckExample.java`
2. ç†è§£æ¯å€‹ç¤ºä¾‹çš„è¼¸å‡º
3. ä¿®æ”¹ä»£ç¢¼ï¼Œçœ‹çœ‹è¼¸å‡ºå¦‚ä½•è®ŠåŒ–
4. ç†Ÿæ‚‰å¾Œå†ä½¿ç”¨å®Œæ•´æ¸¬è©¦å¥—ä»¶

### å°æ–¼é€²éšä½¿ç”¨è€…
1. ç›´æ¥ä½¿ç”¨ `ManualBackpropTest.java`
2. æ•´åˆåˆ° CI/CD æµç¨‹
3. è‡ªå®šç¾©æ¸¬è©¦æ¡ˆä¾‹
4. èª¿æ•´å®¹å¿åº¦ä»¥é©æ‡‰ç‰¹å®šéœ€æ±‚

### Debug æµç¨‹
```
æ¸¬è©¦å¤±æ•—
  â†“
ä½¿ç”¨ printStats() æŸ¥çœ‹çµ±è¨ˆ
  â†“
ä½¿ç”¨ checkNaN() æ’é™¤æ•¸å€¼å•é¡Œ
  â†“
æª¢æŸ¥å…¬å¼æ¨å°
  â†“
å°æ¯”åƒè€ƒå¯¦ç¾
  â†“
ä¿®å¾© â†’ é‡æ–°æ¸¬è©¦
```

---

## ğŸ“ èˆ‡ Karpathy èª²ç¨‹å°æ‡‰

### Python ç‰ˆæœ¬
```python
def cmp(s, dt, t):
    ex = torch.all(dt == t.grad).item()
    app = torch.allclose(dt, t.grad)
    maxdiff = (dt - t.grad).abs().max().item()
    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')
```

### Java ç‰ˆæœ¬
```java
GradientChecker.compare("logits", dlogits_manual, dlogits_reference);
// è¼¸å‡ºæ ¼å¼ç›¸åŒï¼ŒåŠŸèƒ½æ›´è±å¯Œ
```

---

## âš™ï¸ æŠ€è¡“ç´°ç¯€

### æ•¸å€¼æ¢¯åº¦è¨ˆç®—
- **æ–¹æ³•**: ä¸­å¿ƒå·®åˆ†æ³•
- **å…¬å¼**: `f'(x) â‰ˆ [f(x+h) - f(x-h)] / (2h)`
- **h å€¼**: 1e-5ï¼ˆé»˜èªï¼‰
- **å„ªé»**: O(hÂ²) èª¤å·®ï¼Œæ¯”å–®é‚Šå·®åˆ†æ›´æº–ç¢º

### æ¢¯åº¦æ¯”è¼ƒ
- **çµ•å°èª¤å·®**: `|manual - reference|`
- **ç›¸å°èª¤å·®**: `|manual - reference| / max(|manual|, |reference|)`
- **å®¹å¿åº¦**: 1e-5ï¼ˆé»˜èªï¼‰

### æ€§èƒ½è€ƒé‡
- æ•¸å€¼æ¢¯åº¦å¾ˆæ…¢ï¼ˆéœ€è¦ 2N æ¬¡ forward passï¼‰
- å¤§å‹å¼µé‡ä½¿ç”¨æ¡æ¨£æª¢æŸ¥
- æ¸¬è©¦ä½¿ç”¨å›ºå®šéš¨æ©Ÿç¨®å­ï¼ˆå¯é‡ç¾ï¼‰

---