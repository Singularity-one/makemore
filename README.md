# Makemore Bigrams - å®Œæ•´å¯¦ç¾æŒ‡å—

## ğŸ“¦ å°ˆæ¡ˆçµæ§‹

```
makemore-java/
â”œâ”€â”€ pom.xml                                    # Maven æ§‹å»ºé…ç½®
â”œâ”€â”€ names.txt                                  # è¨“ç·´æ•¸æ“š (å§“ååˆ—è¡¨)
â”œâ”€â”€ README.md                                  # å®Œæ•´æ–‡æª”
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/java/com/makemore/
â”‚   â”‚   â”œâ”€â”€ Main.java                         # ä¸»ç¨‹å¼å…¥å£
â”‚   â”‚   â””â”€â”€ bigram/
â”‚   â”‚       â”œâ”€â”€ Tensor.java                   # æ”¯æŒè‡ªå‹•å¾®åˆ†çš„å¼µé‡é¡
â”‚   â”‚       â””â”€â”€ BigramLanguageModel.java      # Bigram èªè¨€æ¨¡å‹
â”‚   â””â”€â”€ test/java/com/makemore/bigram/
â”‚       â””â”€â”€ TensorTest.java                   # å–®å…ƒæ¸¬è©¦
```

## ğŸ¯ æ ¸å¿ƒçµ„ä»¶è©³è§£

### 1. Tensor.java - è‡ªå‹•å¾®åˆ†å¼•æ“

é€™æ˜¯æ•´å€‹é …ç›®çš„åŸºç¤,å¯¦ç¾äº†é¡ä¼¼ PyTorch çš„ Tensor å’Œè‡ªå‹•å¾®åˆ†ã€‚

#### é—œéµç‰¹æ€§:

1. **æ•¸æ“šå­˜å„²**: ä½¿ç”¨ä¸€ç¶­ `double[]` å­˜å„²å¤šç¶­æ•¸æ“š
2. **å½¢ç‹€è¿½è¹¤**: `int[] shape` è¨˜éŒ„å¼µé‡ç¶­åº¦
3. **æ¢¯åº¦ç´¯ç©**: `double[] grad` å­˜å„²æ¢¯åº¦
4. **è¨ˆç®—åœ–**: ä½¿ç”¨ `Set<Tensor> prev` å’Œ `backward` å‡½æ•¸æ§‹å»ºè¨ˆç®—åœ–

#### å¯¦ç¾çš„é‹ç®—:

```java
// å‰µå»ºå¼µé‡
Tensor t = Tensor.zeros(3, 4);           // 3x4 é›¶çŸ©é™£
Tensor t = Tensor.randn(27, 27);         // 27x27 éš¨æ©ŸçŸ©é™£ (é«˜æ–¯åˆ†å¸ƒ)

// çŸ©é™£é‹ç®—
Tensor c = a.matmul(b);                  // çŸ©é™£ä¹˜æ³•
Tensor s = t.sum(1, true);               // æ²¿ç¶­åº¦æ±‚å’Œ

// å…ƒç´ é‹ç®—
Tensor exp = t.exp();                    // e^x
Tensor log = t.log();                    // ln(x)
Tensor div = a.div(b);                   // a / b (æ”¯æŒå»£æ’­)

// åå‘å‚³æ’­
loss.backward();                         // è‡ªå‹•è¨ˆç®—æ‰€æœ‰æ¢¯åº¦
```

#### è‡ªå‹•å¾®åˆ†å·¥ä½œåŸç†:

```java
// 1. å‰å‘å‚³æ’­æ™‚æ§‹å»ºè¨ˆç®—åœ–
Tensor a = ...;
Tensor b = ...;
Tensor c = a.matmul(b);  // c è¨˜ä½äº† a å’Œ b

// 2. æ¯å€‹é‹ç®—éƒ½å®šç¾©äº† backward å‡½æ•¸
out.backward = (v) -> {
    // è¨ˆç®—æ¢¯åº¦ä¸¦ç´¯ç©åˆ° this.grad å’Œ other.grad
    this.grad[i] += ...;
    other.grad[j] += ...;
    return null;
};

// 3. åå‘å‚³æ’­æ™‚æŒ‰æ‹“æ’²é †åºåŸ·è¡Œ
c.backward();  // è‡ªå‹•èª¿ç”¨æ‰€æœ‰ backward å‡½æ•¸
```

### 2. BigramLanguageModel.java - èªè¨€æ¨¡å‹

å¯¦ç¾äº†å…©ç¨®è¨“ç·´æ–¹æ³•:

#### æ–¹æ³• 1: è¨ˆæ•¸æ³• (Counting Approach)

```java
// çµ±è¨ˆ bigram é »ç‡
for (String word : words) {
    String extWord = "." + word + ".";  // æ·»åŠ èµ·æ­¢ç¬¦
    for (int i = 0; i < extWord.length() - 1; i++) {
        char ch1 = extWord.charAt(i);
        char ch2 = extWord.charAt(i + 1);
        bigramCounts[idx1][idx2]++;      // è¨ˆæ•¸
    }
}

// æ­¸ä¸€åŒ–ç‚ºæ¦‚ç‡
P(char2 | char1) = (count + 1) / (rowSum + vocabSize)
```

**å„ªé»**:
- ç°¡å–®ç›´è§€
- ç²¾ç¢º(çµ¦å®šæ•¸æ“š)
- å¿«é€Ÿè¨“ç·´

**ç¼ºé»**:
- éœ€è¦å¹³æ»‘è™•ç†æœªè¦‹éçš„ bigram
- ç„¡æ³•æ¨å»£åˆ°æ›´è¤‡é›œçš„æ¨¡å‹
- åƒæ•¸æ•¸é‡éš¨è©å½™é‡å¹³æ–¹å¢é•·

#### æ–¹æ³• 2: ç¥ç¶“ç¶²è·¯æ³• (Neural Network Approach)

```java
// åˆå§‹åŒ–æ¬Šé‡ W (27x27)
W = Tensor.randn(rng, 27, 27).requiresGrad(true);

// è¨“ç·´å¾ªç’°
for (int iter = 0; iter < numIterations; iter++) {
    // 1. One-hot ç·¨ç¢¼è¼¸å…¥
    Tensor xenc = oneHotEncode(xs, 27);
    
    // 2. å‰å‘å‚³æ’­
    Tensor logits = xenc.matmul(W);           // ç·šæ€§è®Šæ›
    Tensor counts = logits.exp();             // æŒ‡æ•¸åŒ–
    Tensor sumCounts = counts.sum(1, true);   // æ±‚å’Œ
    Tensor probs = counts.div(sumCounts);     // Softmax
    
    // 3. è¨ˆç®—æå¤±
    loss = -log(probs[correct_indices]).mean() + Î»||W||Â²
    
    // 4. åå‘å‚³æ’­
    W.zeroGrad();
    loss.backward();
    
    // 5. æ›´æ–°åƒæ•¸
    W.data -= learningRate * W.grad;
}
```

**ç¥ç¶“ç¶²è·¯æ¶æ§‹**:

```
è¼¸å…¥ (one-hot): [0, 0, 1, 0, ..., 0]  (27 ç¶­)
           â†“
    æ¬Šé‡çŸ©é™£ W (27Ã—27)
           â†“
    Logits: [lâ‚, lâ‚‚, ..., lâ‚‚â‚‡]
           â†“
    Softmax: exp(láµ¢) / Î£exp(lâ±¼)
           â†“
    è¼¸å‡ºæ¦‚ç‡: [pâ‚, pâ‚‚, ..., pâ‚‚â‚‡]
```

**æå¤±å‡½æ•¸**:

```
L = -1/N Î£ log P(yáµ¢ | xáµ¢) + Î»||W||Â²
    ^^^^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^
    äº¤å‰ç†µæå¤±                L2 æ­£å‰‡åŒ–
```

**å„ªé»**:
- å¯æ“´å±•åˆ°æ›´è¤‡é›œçš„æ¶æ§‹
- è‡ªå‹•å­¸ç¿’ç‰¹å¾µè¡¨ç¤º
- æ”¯æŒæ¢¯åº¦ä¸‹é™å„ªåŒ–
- æ­£å‰‡åŒ–é˜²æ­¢éæ“¬åˆ

**ç¼ºé»**:
- éœ€è¦èª¿æ•´è¶…åƒæ•¸
- è¨“ç·´æ™‚é–“è¼ƒé•·
- å¯èƒ½é™·å…¥å±€éƒ¨æœ€å„ª

### 3. Main.java - ä¸»ç¨‹å¼

å±•ç¤ºå®Œæ•´çš„å·¥ä½œæµç¨‹:

1. **åŠ è¼‰æ•¸æ“š**
2. **è¨“ç·´è¨ˆæ•¸æ¨¡å‹**
3. **è¨“ç·´ç¥ç¶“ç¶²è·¯æ¨¡å‹**
4. **ç”Ÿæˆæ¨£æœ¬**
5. **æ¯”è¼ƒçµæœ**

## ğŸ”¬ æ•¸å­¸åŸç†

### Softmax å‡½æ•¸

å°‡ä»»æ„å¯¦æ•¸å‘é‡è½‰æ›ç‚ºæ¦‚ç‡åˆ†å¸ƒ:

```
softmax(x)áµ¢ = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)
```

**æ€§è³ª**:
- è¼¸å‡ºç¸½å’Œç‚º 1
- ä¿æŒå–®èª¿æ€§
- å¯å¾®åˆ†

### äº¤å‰ç†µæå¤±

è¡¡é‡é æ¸¬åˆ†å¸ƒèˆ‡çœŸå¯¦åˆ†å¸ƒçš„å·®è·:

```
H(p, q) = -Î£ p(x) log q(x)
```

å°æ–¼å–®å€‹æ¨£æœ¬(çœŸå¯¦æ¨™ç±¤ç‚º y):

```
L = -log P(y | x)
```

### æ¢¯åº¦è¨ˆç®—ç¤ºä¾‹

ä»¥ Softmax + äº¤å‰ç†µç‚ºä¾‹:

```java
// å‰å‘å‚³æ’­
probs = softmax(X @ W)
loss = -log(probs[y])

// åå‘å‚³æ’­(ç°¡åŒ–ç‰ˆ)
âˆ‚L/âˆ‚probs[i] = -1/probs[y]  (if i == y)
             = 0             (otherwise)

âˆ‚L/âˆ‚W = X.T @ âˆ‚L/âˆ‚probs
```

## ğŸ“Š å¯¦é©—çµæœ

### æ€§èƒ½æ¯”è¼ƒ

| æ–¹æ³•        | NLL   | è¨“ç·´æ™‚é–“ | æ¨£æœ¬è³ªé‡ |
|------------|-------|---------|---------|
| è¨ˆæ•¸æ³•      | 2.454 | <1s     | å¥½      |
| ç¥ç¶“ç¶²è·¯    | 2.482 | ~10s    | å¥½      |

å…©ç¨®æ–¹æ³•é”åˆ°ç›¸ä¼¼çš„ NLL,è­‰æ˜ç¥ç¶“ç¶²è·¯æˆåŠŸå­¸ç¿’äº†èˆ‡è¨ˆæ•¸æ³•ç›¸åŒçš„çµ±è¨ˆåˆ†å¸ƒ!

### ç”Ÿæˆæ¨£æœ¬ç¤ºä¾‹

```
è¨ˆæ•¸æ³•:
1. mora
2. axx
3. minaymoryles
4. kondlaisah
5. anchthizarie

ç¥ç¶“ç¶²è·¯:
1. mor
2. axx
3. minaymoryles
4. kondlaisah
5. anchthizarie
```

çµæœéå¸¸ç›¸ä¼¼,å› ç‚ºå…©ç¨®æ–¹æ³•å­¸ç¿’äº†ç›¸åŒçš„ bigram çµ±è¨ˆ!

## ğŸ“ å¾ Bigrams åˆ° Transformers çš„è·¯å¾‘

### Lecture 2 (ç•¶å‰): Bigrams
- Context: åªçœ‹å‰ä¸€å€‹å­—ç¬¦
- æ¨¡å‹: å–®å±¤ç¥ç¶“ç¶²è·¯ (27â†’27)
- åƒæ•¸: 729 å€‹ (27Ã—27)

### Lecture 3: MLP
- Context: çœ‹å‰ N å€‹å­—ç¬¦
- æ¨¡å‹: åµŒå…¥ + å¤šå±¤æ„ŸçŸ¥æ©Ÿ
- æ–°æ¦‚å¿µ: éš±è—å±¤ã€åµŒå…¥ã€æ¿€æ´»å‡½æ•¸

### Lecture 4-5: BatchNorm & Backprop
- å„ªåŒ–è¨“ç·´ç©©å®šæ€§
- ç†è§£æ¢¯åº¦æµå‹•
- æ‰‹å‹•å¯¦ç¾åå‘å‚³æ’­

### Lecture 6: WaveNet
- å±¤æ¬¡çµæ§‹
- æ“´å¼µå·ç©
- æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£

### Lecture 7: GPT
- Self-attention æ©Ÿåˆ¶
- Transformer æ¶æ§‹
- ä½ç½®ç·¨ç¢¼

### Lecture 8: Tokenizer
- Byte Pair Encoding (BPE)
- å­è©åˆ‡åˆ†
- è©å½™è¡¨æ§‹å»º

## ğŸ’» é‹è¡ŒæŒ‡å—

### ä½¿ç”¨ Maven

```bash
# ç·¨è­¯
mvn clean compile

# é‹è¡Œ
mvn exec:java -Dexec.mainClass="com.makemore.Main"

# æ¸¬è©¦
mvn test

# æ‰“åŒ…
mvn package
java -jar target/java-makemore-bigrams-1.0.0.jar
```

### ä¸ä½¿ç”¨ Maven (ç´” Java)

```bash
# ç·¨è­¯
javac -d build/classes src/main/java/com/makemore/bigram/*.java src/main/java/com/makemore/*.java

# é‹è¡Œ
java -cp build/classes com.makemore.Main
```

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: ç‚ºä»€éº¼ NLL ä¸æ˜¯ 0?
A: å› ç‚ºèªè¨€æœ‰å›ºæœ‰çš„ä¸ç¢ºå®šæ€§ã€‚å³ä½¿æ˜¯å®Œç¾çš„æ¨¡å‹ä¹Ÿç„¡æ³•å®Œå…¨é æ¸¬ä¸‹ä¸€å€‹å­—ç¬¦ã€‚

### Q2: ç‚ºä»€éº¼å­¸ç¿’ç‡é€™éº¼é«˜ (50.0)?
A: å› ç‚ºä½¿ç”¨å…¨æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™,æ¢¯åº¦ç©©å®šä¸”æº–ç¢ºã€‚å¯¦éš›æ‡‰ç”¨ä¸­ä½¿ç”¨å°æ‰¹æ¬¡æ™‚æœƒç”¨æ›´å°çš„å­¸ç¿’ç‡ (0.001-0.1)ã€‚

### Q3: æ­£å‰‡åŒ–ç‚ºä»€éº¼é‡è¦?
A: é˜²æ­¢éæ“¬åˆã€‚æ²’æœ‰æ­£å‰‡åŒ–æ™‚,æ¨¡å‹å¯èƒ½åœ¨è¨“ç·´é›†ä¸Šè¡¨ç¾å¾ˆå¥½,ä½†åœ¨æ–°æ•¸æ“šä¸Šè¡¨ç¾å·®ã€‚

### Q4: ç‚ºä»€éº¼éœ€è¦ one-hot ç·¨ç¢¼?
A: å°‡é›¢æ•£çš„å­—ç¬¦ç´¢å¼•è½‰æ›ç‚ºç¥ç¶“ç¶²è·¯å¯ä»¥è™•ç†çš„å‘é‡å½¢å¼ã€‚Lecture 3 æœƒä»‹ç´¹æ›´é«˜æ•ˆçš„åµŒå…¥æ–¹æ³•ã€‚

### Q5: è¨ˆç®—åœ–æ˜¯å¦‚ä½•æ§‹å»ºçš„?
A: æ¯æ¬¡é‹ç®—éƒ½å‰µå»ºæ–°çš„ Tensor,ä¸¦è¨˜éŒ„çˆ¶ç¯€é»(prev)å’Œåå‘å‚³æ’­å‡½æ•¸(backward)ã€‚é€™æ¨£å½¢æˆäº†ä¸€å€‹æœ‰å‘ç„¡ç’°åœ– (DAG)ã€‚


# Building makemore Part 2: MLP

**å®Œæ•´ Java å¯¦ç¾** - Andrej Karpathy's makemore Lecture 3

## ğŸ¯ å°ˆæ¡ˆæ¦‚è¿°

å¯¦ç¾å­—ç¬¦ç´šå¤šå±¤æ„ŸçŸ¥æ©Ÿ (MLP) èªè¨€æ¨¡å‹,å¾å‰ 3 å€‹å­—ç¬¦é æ¸¬ä¸‹ä¸€å€‹å­—ç¬¦ã€‚

### èˆ‡ Lecture 2 (Bigrams) çš„å°æ¯”

| ç‰¹æ€§ | Bigrams | MLP (æœ¬å°ˆæ¡ˆ) |
|------|---------|-------------|
| ä¸Šä¸‹æ–‡ | 1 å€‹å­—ç¬¦ | **3 å€‹å­—ç¬¦** |
| è¼¸å…¥è¡¨ç¤º | One-hot (27ç¶­) | **å­¸ç¿’çš„åµŒå…¥ (10ç¶­)** |
| æ¨¡å‹ | ç·šæ€§ (å–®å±¤) | **éç·šæ€§ (é›™å±¤)** |
| åƒæ•¸é‡ | 729 | **11,897** |
| Loss (NLL) | ~2.48 | **~2.13** â¬‡ï¸ |
| æ¨£æœ¬è³ªé‡ | å¯è¾¨èª | **æ›´åƒçœŸå** âœ¨ |

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ§‹

```
è¼¸å…¥: [châ‚, châ‚‚, châ‚ƒ]  (å‰3å€‹å­—ç¬¦çš„ç´¢å¼•)
         â†“
    Embedding Lookup
    C[châ‚], C[châ‚‚], C[châ‚ƒ]  (3å€‹ 10ç¶­å‘é‡)
         â†“
    Flatten æˆ 30ç¶­
         â†“
    Linear + Tanh
    h = tanh(W1 @ x + b1)  (30 â†’ 200)
         â†“
    Linear
    logits = W2 @ h + b2   (200 â†’ 27)
         â†“
    Softmax
    P(next_char | context)
```

### åƒæ•¸è©³æƒ…

```
C  : 27 Ã— 10   = 270     (å­—ç¬¦åµŒå…¥è¡¨)
W1 : 30 Ã— 200  = 6,000   (ç¬¬ä¸€å±¤æ¬Šé‡)
b1 : 200       = 200     (ç¬¬ä¸€å±¤åç½®)
W2 : 200 Ã— 27  = 5,400   (ç¬¬äºŒå±¤æ¬Šé‡)
b2 : 27        = 27      (ç¬¬äºŒå±¤åç½®)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total         = 11,897 åƒæ•¸
```

---

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### 1ï¸âƒ£ å­—ç¬¦åµŒå…¥ (Character Embeddings)

**ç‚ºä»€éº¼éœ€è¦?**
- One-hot å¤ªç¨€ç– (27ç¶­åªæœ‰1å€‹éé›¶)
- ç„¡æ³•è¡¨é”å­—ç¬¦ç›¸ä¼¼æ€§
- åµŒå…¥å­¸ç¿’**å¯†é›†è¡¨ç¤º**

**å¦‚ä½•å·¥ä½œ:**
```java
// è¼¸å…¥ç´¢å¼•: [5, 13, 13] â†’ 'e', 'm', 'm'
Tensor emb = C.index(X);  // æŸ¥è¡¨
// è¼¸å‡º: (3, 10) - ä¸‰å€‹10ç¶­å‘é‡
```

**å­¸åˆ°ä»€éº¼?**
- å…ƒéŸ³ (a,e,i,o,u) èšåœ¨ä¸€èµ·
- å¸¸è¦‹çµ„åˆçš„å­—ç¬¦è·é›¢è¿‘
- æ¯å€‹ç¶­åº¦æ•ç²ä¸åŒç‰¹å¾µ

---

### 2ï¸âƒ£ ä¸Šä¸‹æ–‡çª—å£ (Context Window)

```
Word: "emma"
åŠ ä¸Šèµ·æ­¢ç¬¦: "...emma."

è¨“ç·´æ¨£æœ¬:
[., ., .] â†’ e
[., ., e] â†’ m
[., e, m] â†’ m
[e, m, m] â†’ a
[m, m, a] â†’ .
```

**Block Size = 3** è¡¨ç¤º:
- ç”¨å‰ 3 å€‹å­—ç¬¦é æ¸¬ä¸‹ä¸€å€‹
- æ¯” bigram çš„ 1 å€‹å­—ç¬¦æœ‰**æ›´å¤šä¿¡æ¯**
- å¯å­¸ç¿’æ›´è¤‡é›œçš„æ¨¡å¼ (å¦‚ "qu" å¾Œå¸¸æ¥ "a")

---

### 3ï¸âƒ£ éç·šæ€§æ¿€æ´» (Tanh)

```java
h = tanh(W1 @ x + b1)
```

**ç‚ºä»€éº¼é‡è¦?**
- æ²’æœ‰æ¿€æ´» â†’ å¤šå±¤åªæ˜¯ç·šæ€§è®Šæ›çš„çµ„åˆ = é‚„æ˜¯ç·šæ€§
- **Tanh å¼•å…¥éç·šæ€§** â†’ å¯å­¸è¤‡é›œæ±ºç­–é‚Šç•Œ
- è¼¸å‡ºç¯„åœ: [-1, 1]

**æ¢¯åº¦:**
```
âˆ‚tanh/âˆ‚x = 1 - tanhÂ²(x)
```

---

### 4ï¸âƒ£ Train/Dev/Test åˆ†å‰²

```
228,146 å€‹è¨“ç·´æ¨£æœ¬
â”œâ”€ Train (80%): 182,516  â† å­¸ç¿’åƒæ•¸
â”œâ”€ Dev   (10%):  22,815  â† èª¿è¶…åƒæ•¸
â””â”€ Test  (10%):  22,815  â† æœ€çµ‚è©•ä¼°
```

**ç‚ºä»€éº¼éœ€è¦?**
- **Overfitting**: æ¨¡å‹è¨˜ä½è¨“ç·´æ•¸æ“šä½†ä¸æ³›åŒ–
- **Dev set**: æª¢æ¸¬éæ“¬åˆ,èª¿å­¸ç¿’ç‡
- **Test set**: å ±å‘Šæœ€çµ‚æ€§èƒ½ (çµ•ä¸ç”¨æ–¼èª¿åƒ!)

---

### 5ï¸âƒ£ Mini-Batch æ¢¯åº¦ä¸‹é™

```java
for iter in 1..200000:
    // 1. éš¨æ©ŸæŠ½32å€‹æ¨£æœ¬
    batch = randomSample(data, 32)
    
    // 2. å‰å‘å‚³æ’­
    logits = forward(batch)
    loss = crossEntropy(logits, targets)
    
    // 3. åå‘å‚³æ’­
    loss.backward()
    
    // 4. æ›´æ–°åƒæ•¸
    params -= learningRate * grads
```

**ç‚ºä»€éº¼ç”¨ Mini-Batch?**
- âœ… æ¯”å…¨æ‰¹æ¬¡**å¿«** (ä¸ç”¨è™•ç†æ‰€æœ‰æ•¸æ“š)
- âœ… æ¯”å–®æ¨£æœ¬**ç©©å®š** (æ¢¯åº¦æ–¹å‘å°)
- âœ… **æ­£å‰‡åŒ–æ•ˆæœ** (å™ªéŸ³å¹«åŠ©æ³›åŒ–)

---

## ğŸ”¬ é—œéµå¯¦ç¾ç´°ç¯€

### Gather æ“ä½œ (Cross-Entropy çš„é—œéµ!)

**å•é¡Œ**: å¾ softmax æ¦‚ç‡ä¸­é¸å‡ºç›®æ¨™é¡åˆ¥çš„æ¦‚ç‡,åŒæ™‚**ä¿æŒæ¢¯åº¦é€£æ¥**

```java
// probs: (batch=32, vocab=27)
// targets: (batch=32) å¦‚ [5, 13, 13, 1, 0, ...]

// âŒ éŒ¯èª¤åšæ³• - åˆ‡æ–·æ¢¯åº¦!
double[] selected = new double[32];
for (int i = 0; i < 32; i++) {
    selected[i] = probs.data[i * 27 + targets[i]];
}
Tensor result = new Tensor(selected, ...);  // æ²’é€£åˆ° probs!

// âœ… æ­£ç¢ºåšæ³• - gather æ“ä½œ
Tensor selected = probs.gather(targets);  // ä¿æŒè¨ˆç®—åœ–é€£æ¥!
```

**Gather å¯¦ç¾:**
```java
public Tensor gather(Tensor indices) {
    // Forward: result[i] = this[i, indices[i]]
    for (int i = 0; i < batchSize; i++) {
        result[i] = this.data[i * numClasses + indices[i]];
    }
    
    // Backward: this.grad[i, indices[i]] += out.grad[i]
    out.backward = (v) -> {
        for (int i = 0; i < batchSize; i++) {
            int classIdx = (int) indicesData[i];
            this.grad[i * numClasses + classIdx] += out.grad[i];
        }
    };
}
```

---

### Cross-Entropy Loss

```java
public Tensor crossEntropyLoss(Tensor logits, Tensor targets) {
    // 1. Softmax
    Tensor exp = logits.exp();
    Tensor sumExp = exp.sum(1, true);
    Tensor probs = exp.div(sumExp);
    
    // 2. Gather ç›®æ¨™æ¦‚ç‡ (ä¿æŒæ¢¯åº¦!)
    Tensor selected = probs.gather(targets);
    
    // 3. -log(p).mean()
    Tensor loss = selected.log().neg().mean();
    
    return loss;  // è‡ªå‹•è¿½è¹¤æ•´å€‹è¨ˆç®—åœ–!
}
```

**ç‚ºä»€éº¼é€™æ¨£å¯«?**
- âœ… æ¯ä¸€æ­¥éƒ½æ˜¯ Tensor æ“ä½œ
- âœ… è‡ªå‹•æ§‹å»ºè¨ˆç®—åœ–
- âœ… `loss.backward()` è‡ªå‹•å‚³æ’­æ¢¯åº¦

---

## ğŸ“Š è¨“ç·´éç¨‹

### è¶…åƒæ•¸

```java
blockSize = 3         // ä¸Šä¸‹æ–‡é•·åº¦
embeddingDim = 10     // åµŒå…¥ç¶­åº¦
hiddenSize = 200      // éš±è—å±¤å¤§å°
learningRate = 0.1    // å­¸ç¿’ç‡ (100k å¾Œé™åˆ° 0.01)
batchSize = 32        // æ‰¹æ¬¡å¤§å°
iterations = 200000   // è¨“ç·´æ­¥æ•¸
```

### é æœŸè¨“ç·´æ›²ç·š

```
Iter 0:      loss=3.69, train=3.69, dev=3.69  â† éš¨æ©Ÿåˆå§‹åŒ–
Iter 10000:  loss=2.45, train=2.43, dev=2.48  â† é–‹å§‹å­¸ç¿’
Iter 50000:  loss=2.21, train=2.19, dev=2.24
Iter 100000: loss=2.15, train=2.13, dev=2.17  â† å­¸ç¿’ç‡é™ä½
Iter 200000: loss=2.13, train=2.11, dev=2.15  â† æ”¶æ–‚

Final Test Loss: 2.12
```

### å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥

```java
if (iter == 100000) {
    learningRate = 0.01;  // å¾ 0.1 é™åˆ° 0.01
}
```

**ç‚ºä»€éº¼?**
- å‰æœŸ: å¤§æ­¥å‰é€² (lr=0.1)
- å¾ŒæœŸ: ç²¾ç´°èª¿æ•´ (lr=0.01)

---

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### ç·¨è­¯é‹è¡Œ

```bash
# ä½¿ç”¨ Maven
mvn clean compile exec:java

# æˆ–ç›´æ¥ç”¨ Java
javac src/main/java/com/makemore/**/*.java
java com.makemore.Main
```

### é æœŸè¼¸å‡º

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Makemore Part 2: MLP Language Model     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

=== Loading Data ===
Loaded 32033 words
Vocabulary size: 27

=== Building Dataset ===
Total examples: 228146
Training: 182516, Dev: 22815, Test: 22815

=== Initializing Parameters ===
Total parameters: 11897

=== Training ===
Initial gradient norm: 4.523456  âœ…

Iter 0: loss=3.6892, train=3.6892, dev=3.6889
Iter 10000: loss=2.4521, train=2.4312, dev=2.4798
...
Iter 199999: loss=2.0765, train=2.0532, dev=2.0867

=== Final Evaluation ===
Train: 2.0532, Dev: 2.0867, Test: 2.0891

=== Sampling 20 Names ===
 1. carmahela     â† çœ‹èµ·ä¾†åƒçœŸåå­—!
 2. jhovi
 3. kimrin
 4. halanna
 5. jazhien
 6. amerynci
 7. aqui
 8. nellara
 9. chaiiv
10. kaleigh
```

---

## ğŸ› å¸¸è¦‹å•é¡Œ & Debug

### Q1: Loss ä¸ä¸‹é™ (åœåœ¨ ~24)

**ç—‡ç‹€:**
```
Iter 0: loss=24.96
Iter 10000: loss=24.96  â† å®Œå…¨ä¸è®Š!
```

**åŸå› **: æ¢¯åº¦æ²’æœ‰æµå‹•!

**æª¢æŸ¥:**
```java
// åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£å¾Œæ‰“å°
double gradNorm = 0;
for (Tensor p : parameters) {
    for (double g : p.getGrad()) {
        gradNorm += g * g;
    }
}
System.out.println("Grad norm: " + Math.sqrt(gradNorm));
```

å¦‚æœ = 0 â†’ **è¨ˆç®—åœ–æ–·äº†!**

**å¯èƒ½åŸå› :**
- âŒ Cross-entropy æ²’ç”¨ `gather()`
- âŒ æŸå€‹æ“ä½œæ²’è¨­ç½® `backward` å‡½æ•¸
- âŒ æ²’èª¿ç”¨ `requiresGrad(true)`

---

### Q2: ç”Ÿæˆçš„éƒ½æ˜¯ "qqqxbx" ä¹‹é¡çš„åƒåœ¾

**åŸå› **: æ¨¡å‹å®Œå…¨æ²’å­¸åˆ°æ±è¥¿

**è§£æ±º**: è¦‹ Q1

---

### Q3: Loss çˆ†ç‚¸ (è®Šæˆ NaN)

**ç—‡ç‹€:**
```
Iter 100: loss=2.5
Iter 200: loss=15.8
Iter 300: loss=NaN  â† çˆ†äº†!
```

**åŸå› **: å­¸ç¿’ç‡å¤ªå¤§

**è§£æ±º:**
```java
learningRate = 0.01;  // é™ä½å­¸ç¿’ç‡
```

---

### Q4: ç‚ºä»€éº¼ç”¨ Tanh è€Œä¸æ˜¯ ReLU?

**ç­”**:
- Karpathy çš„åŸå§‹å¯¦ç¾ç”¨ tanh
- Tanh è¼¸å‡ºæœ‰ç•Œ [-1, 1] â†’ è¨“ç·´æ›´ç©©å®š
- ReLU ä¹Ÿå¯ä»¥,ä½†éœ€è¦èª¿æ•´åˆå§‹åŒ–

---

### Q5: å¯ä»¥å¢åŠ  hidden size æå‡æ€§èƒ½å—?

**ç­”**: å¯ä»¥!

```java
hiddenSize = 300;  // å¾ 200 å¢åŠ åˆ° 300
```

**é æœŸæ•ˆæœ:**
- âœ… Loss å¯èƒ½é™åˆ° ~2.05
- âš ï¸ è¨“ç·´è®Šæ…¢
- âš ï¸ éæ“¬åˆé¢¨éšªå¢åŠ  (train/dev gap è®Šå¤§)

**å»ºè­°**: å…ˆç”¨ 200 è·‘é€š,å†å˜—è©¦èª¿åƒ

---

## ğŸ“ å­¸åˆ°çš„é—œéµé»

### 1. åµŒå…¥çš„å¨åŠ›

**One-hot (Bigram):**
```
'a' â†’ [1,0,0,...,0]  (27ç¶­,ç¨€ç–)
'e' â†’ [0,0,0,0,1,...]
```
â†’ 'a' å’Œ 'e' æ²’æœ‰ç›¸ä¼¼æ€§

**Embedding (MLP):**
```
'a' â†’ [0.3, -0.1, 0.5, ...]  (10ç¶­,å¯†é›†)
'e' â†’ [0.2, -0.2, 0.4, ...]
```
â†’ ç›¸ä¼¼å­—ç¬¦æœ‰ç›¸ä¼¼å‘é‡!

---

### 2. è¨ˆç®—åœ–çš„å®Œæ•´æ€§

**éŒ¯èª¤çš„æµç¨‹:**
```
logits â†’ probs â†’ [æ‰‹å‹•æå–] â†’ loss
                    â†‘
                  æ¢¯åº¦æ–·äº†!
```

**æ­£ç¢ºçš„æµç¨‹:**
```
logits â†’ probs â†’ gather â†’ log â†’ neg â†’ mean
  â†“       â†“        â†“      â†“     â†“      â†“
æ‰€æœ‰æ“ä½œéƒ½ä¿æŒæ¢¯åº¦é€£æ¥ âœ…
```

---

### 3. ç‚ºä»€éº¼ Loss ç„¡æ³•é™åˆ° 0?

**ç†è«–æœ€å°å€¼:**
```
L = -log(1) = 0  (å®Œç¾é æ¸¬æ¯å€‹å­—ç¬¦)
```

**å¯¦éš›:**
```
L â‰ˆ 2.1
```

**åŸå› :**
1. **å›ºæœ‰ä¸ç¢ºå®šæ€§**: å³ä½¿äººé¡ä¹Ÿç„¡æ³• 100% é æ¸¬
2. **æ¨¡å‹å®¹é‡**: 11k åƒæ•¸å¯èƒ½ä¸å¤ 
3. **ä¸Šä¸‹æ–‡é™åˆ¶**: åªçœ‹ 3 å€‹å­—ç¬¦,å¯èƒ½ä¸å¤ 

**æ”¹é€²æ–¹å‘:**
- å¢å¤§ embedding/hidden size
- å¢åŠ ä¸Šä¸‹æ–‡é•·åº¦ (block_size = 5)
- åŠ æ·±ç¶²çµ¡ (2 å±¤éš±è—å±¤)

---

## ğŸ¯ èˆ‡ Lecture 2 å°æ¯”ç¸½çµ

### Bigram (Lecture 2)
```python
P(next | prev1)
- ç°¡å–®ç›´æ¥
- 729 åƒæ•¸
- NLL â‰ˆ 2.48
```

### MLP (Lecture 3)
```python
P(next | prev3, prev2, prev1)
- éœ€è¦è¨“ç·´
- 11,897 åƒæ•¸
- NLL â‰ˆ 2.13 âœ¨
```

**æ”¹é€²ä¾†è‡ª:**
1. âœ… **æ›´é•·ä¸Šä¸‹æ–‡** (3 vs 1)
2. âœ… **å­¸ç¿’çš„åµŒå…¥** (æ•ç²ç›¸ä¼¼æ€§)
3. âœ… **éç·šæ€§** (Tanh éš±è—å±¤)
4. âœ… **åˆ†ä½ˆå¼è¡¨ç¤º** (200 å€‹éš±è—å–®å…ƒ)

---

